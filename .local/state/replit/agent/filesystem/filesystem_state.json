{"file_contents":{"src/services/youtube.ts":{"content":"import { spawn } from 'child_process'\nimport { promisify } from 'util'\nimport { exec } from 'child_process'\nimport { existsSync } from 'fs'\nimport { join } from 'path'\nimport { tmpdir } from 'os'\nimport { getCookieFilePath, cleanupCookieFile } from './cookieGenerator'\n\nconst execPromise = promisify(exec)\n\ntype DownloadInfo = {\n  file: string\n  height: number\n  fps: number\n  tbr: number\n  vcodec: string\n  acodec: string\n}\n\nfunction run(cmd: string, args: string[]): Promise<{ stdout: string, stderr: string }> {\n  return new Promise((resolve, reject) => {\n    const p = spawn(cmd, args, { stdio: ['ignore', 'pipe', 'pipe'] })\n    let out = ''\n    let err = ''\n    p.stdout.on('data', d => out += d.toString())\n    p.stderr.on('data', d => err += d.toString())\n    p.on('close', code => {\n      if (code === 0)\n      {\n        resolve({ stdout: out, stderr: err })\n      }\n      else\n      {\n        reject(new Error(err || `exit ${code}`))\n      }\n    })\n  })\n}\n\nexport async function downloadBest(url: string, cookieFile?: string): Promise<DownloadInfo> {\n  const outDir = tmpdir()\n  const outTpl = join(outDir, '%(id)s.%(ext)s')\n  const ytdlp = await findYtDlp()\n  const args = [\n    '-j',\n    '--no-simulate',\n    '-f', 'bv*[height>=720][height<=2160][ext=mp4]+ba[ext=m4a]/bv*[height>=480][height<=2160][ext=mp4]+ba[ext=m4a]/bv*[height<=2160]+ba/b',\n    '-S', 'res,fps,br,codec:h264:av01:vp9',\n    '--merge-output-format', 'mp4',\n    '--referer', 'https://www.youtube.com/',\n    '--verbose',\n    '-o', outTpl,\n    url\n  ]\n  if (cookieFile && existsSync(cookieFile))\n  {\n    args.splice(1, 0, '--cookies', cookieFile)\n    console.log('Using cookie file for authentication')\n  }\n  else\n  {\n    console.warn('No cookie file available - download may fail for some videos')\n  }\n  console.log('yt-dlp command:', ytdlp, args.join(' '))\n  const { stdout, stderr } = await run(ytdlp, args)\n  if (stderr && stderr.length > 0)\n  {\n    console.log('yt-dlp stderr:', stderr.substring(0, 2000))\n  }\n  const lines = stdout.trim().split('\\n')\n  const last = JSON.parse(lines[lines.length - 1])\n  const file = last['_filename']\n  const rf = (last['requested_formats'] && last['requested_formats'][0]) || last\n  const height = Number(rf['height'] || 0)\n  const fps = Number(rf['fps'] || 0)\n  const tbr = Number(rf['tbr'] || 0)\n  const vcodec = String(rf['vcodec'] || '')\n  const acodec = String(rf['acodec'] || '')\n  console.log(`Downloaded: ${height}p ${fps}fps ${vcodec} @ ${tbr}kbps`)\n  return { file, height, fps, tbr, vcodec, acodec }\n}\n\nexport function cleanupUserCookiesFile(userId: string): void {\n  console.log('Cookie cleanup no longer needed with OAuth authentication')\n}\n\nasync function findYtDlp(): Promise<string> {\n  if (process.env.YT_DLP_PATH && existsSync(process.env.YT_DLP_PATH))\n  {\n    return process.env.YT_DLP_PATH\n  }\n  \n  try {\n    await execPromise('which yt-dlp')\n    return 'yt-dlp'\n  }\n  catch {\n    try {\n      await execPromise('which youtube-dl')\n      return 'youtube-dl'\n    }\n    catch {\n      throw new Error('yt-dlp or youtube-dl not found in PATH')\n    }\n  }\n}\n\nexport interface Chapter {\n  title: string\n  start_time?: number\n  end_time?: number\n  startSec: number\n  endSec: number\n}\n\ninterface VideoMetadata {\n  id: string\n  title: string\n  duration: number\n  chapters: Chapter[]\n}\n\nexport async function getVideoMetadata(url: string, userId: string): Promise<VideoMetadata> {\n  const ytdlp = await findYtDlp()\n  \n  const cookieFile = await getCookieFilePath(userId)\n  \n  try {\n    const args = [\n      '--dump-json',\n      '--no-playlist',\n      url\n    ]\n    \n    if (cookieFile)\n    {\n      args.splice(0, 0, '--cookies', cookieFile)\n    }\n    \n    const { stdout } = await run(ytdlp, args)\n    const metadata = JSON.parse(stdout)\n    \n    const chapters = (metadata.chapters || []).map((ch: any) => ({\n      title: ch.title,\n      start_time: ch.start_time,\n      end_time: ch.end_time,\n      startSec: ch.start_time,\n      endSec: ch.end_time\n    }))\n    \n    return {\n      id: metadata.id || '',\n      title: metadata.title || 'Unknown',\n      duration: metadata.duration || 0,\n      chapters\n    }\n  }\n  finally {\n    if (cookieFile)\n    {\n      await cleanupCookieFile(cookieFile)\n    }\n  }\n}\n\nexport async function downloadVideo(url: string, outputPath: string, userId: string): Promise<DownloadInfo> {\n  let info: DownloadInfo | null = null\n  let videoId: string | null = null\n  let cookieFile: string | null = null\n  \n  try {\n    cookieFile = await getCookieFilePath(userId)\n    \n    const metadata = await getVideoMetadata(url, userId)\n    videoId = metadata.id\n    \n    info = await downloadBest(url, cookieFile || undefined)\n    \n    if (info.file.length === 0)\n    {\n      throw new Error('no file downloaded')\n    }\n    \n    const { rename } = await import('fs/promises')\n    await rename(info.file, outputPath)\n    return info\n  }\n  finally {\n    if (cookieFile)\n    {\n      await cleanupCookieFile(cookieFile)\n    }\n    if (videoId)\n    {\n      cleanupYtDlpTempFiles(videoId)\n    }\n  }\n}\n\nfunction cleanupYtDlpTempFiles(videoId: string): void {\n  const { readdirSync, rmSync } = require('fs')\n  const tmp = tmpdir()\n  \n  try {\n    const files = readdirSync(tmp)\n    let cleaned = 0\n    \n    for (const file of files)\n    {\n      if (file.startsWith(videoId) && file.match(/\\.(webm|mp4|m4a|part)$/))\n      {\n        try {\n          rmSync(join(tmp, file), { force: true })\n          cleaned++\n        }\n        catch (err) {\n          console.error(`Failed to cleanup temp file ${file}:`, err)\n        }\n      }\n    }\n    \n    if (cleaned > 0)\n    {\n      console.log(`Cleaned up ${cleaned} temp files for video ${videoId}`)\n    }\n  }\n  catch (err) {\n    console.error('Failed to cleanup yt-dlp temp files:', err)\n  }\n}\n\nexport interface YouTubeChapter {\n  title: string\n  start_time: number\n  end_time: number\n}\n\nexport async function getIntroEndFromChapters(url: string): Promise<number | null> {\n  try {\n    const ytdlp = await findYtDlp()\n    const { stdout } = await run(ytdlp, ['-J', url])\n    const meta = JSON.parse(stdout)\n    const chapters: YouTubeChapter[] = meta.chapters || []\n    \n    if (!chapters.length)\n    {\n      return null\n    }\n\n    const first = chapters[0]\n    const title = (first.title || '').toLowerCase()\n\n    const looksLikeIntro =\n      first.start_time === 0 &&\n      (title.includes('intro') ||\n        title.includes('introduction') ||\n        title.includes('opening') ||\n        title.includes('trailer'))\n\n    const maxIntroFraction = 0.3\n    const durationSec = Number(meta.duration || meta.duration_string || 0)\n    const isReasonableLength =\n      durationSec > 0\n        ? (first.end_time - first.start_time) <= durationSec * maxIntroFraction\n        : true\n\n    if (looksLikeIntro && isReasonableLength)\n    {\n      return first.end_time\n    }\n\n    return null\n  }\n  catch (err) {\n    console.error('[Chapters] Failed to fetch chapters with yt-dlp:', err)\n    return null\n  }\n}\n","size_bytes":6968},"app/api/auth/login/route.ts":{"content":"import { NextResponse } from 'next/server'\nimport { getSession } from '@/src/lib/session'\nimport { checkYouTubeConnection } from '@/src/lib/youtube-client'\n\nexport async function POST() {\n  try {\n    const youtubeConnected = await checkYouTubeConnection()\n    \n    if (!youtubeConnected)\n    {\n      return NextResponse.json(\n        { error: 'YouTube connection not configured. Please set up the YouTube connector first.' },\n        { status: 401 }\n      )\n    }\n    \n    const session = await getSession()\n    session.isAuthenticated = true\n    session.userId = 'youtube_user'\n    await session.save()\n    \n    return NextResponse.json({ success: true })\n  }\n  catch (error) {\n    return NextResponse.json(\n      { error: 'Failed to authenticate with YouTube' },\n      { status: 500 }\n    )\n  }\n}\n","size_bytes":799},"src/worker.ts":{"content":"import { Worker, Job } from \"bullmq\";\nimport { connection } from \"./lib/queue\";\nimport { prisma } from \"./lib/prisma\";\nimport \"./worker-tiktok\";\nimport {\n  getVideoMetadata,\n  downloadVideo,\n} from \"./services/youtube\";\nimport {\n  extractAudio,\n  compressAudioForTranscription,\n  renderVerticalClip,\n  renderSmartFramedClip,\n  extractThumbnail,\n  createSrtFile,\n  createWordByWordSrtFile,\n  detectScenes,\n  probeBitrate,\n  probeVideo,\n  createAssWordByWordFile,\n} from \"./services/ffmpeg\";\nimport { transcribeAudio } from \"./services/openai\";\nimport { scoreClip } from \"./services/openai\";\nimport { detectSegments } from \"./services/segmentation\";\nimport {\n  detectEnhancedSegments,\n  mineTimestampsFromComments,\n} from \"./services/segmentation-v2\";\nimport {\n  fetchVideoComments,\n  extractVideoIdFromUrl,\n} from \"./services/youtube-comments\";\nimport { uploadFile } from \"./services/s3\";\nimport { join } from \"path\";\nimport { mkdirSync, existsSync, rmSync } from \"fs\";\nimport { tmpdir } from \"os\";\nimport { cleanupTempFiles } from \"./lib/cleanup\";\nimport { finalizeBestClips } from \"./services/selection/finalizeRanking\";\nimport { inferTaxonomy } from \"./services/scoring/taxonomy\";\nimport type { RankInput } from \"./services/scoring/clipRanker\";\nimport type { EnhancedSegment } from \"./services/segmentation-v2\";\nimport {\n  computeCropMapPersonStatic,\n  buildFFmpegFilter,\n  type TranscriptWord as FramingWord,\n  type Constraints,\n  initializeFaceDetection,\n} from \"./services/framingService\";\nimport { ensureModelsDownloaded } from \"./services/modelDownloader\";\n\ninterface VideoJob {\n  videoId: string;\n  userId: string;\n}\n\nasync function checkCancelled(videoId: string) {\n  const video = await prisma.video.findUnique({\n    where: { id: videoId },\n    select: { status: true },\n  });\n\n  if (video?.status === \"cancelled\") {\n    throw new Error(\"Video processing was cancelled by user\");\n  }\n}\n\nfunction buildRankInputsFromEnhanced(\n  segmentsWithIds: Array<EnhancedSegment & { clipId: string }>,\n  videoId: string,\n  aiOverallById: Record<string, number>,\n  hotspots: number[],\n): RankInput[] {\n  return segmentsWithIds.map((e) => {\n    const nearHotspot = hotspots.some((h) => Math.abs(h - e.startSec) < 30);\n    const rankIn: RankInput = {\n      id: e.clipId,\n      videoId,\n      start: e.startSec,\n      end: e.endSec,\n      score: e.score,\n      pillars: {\n        hook: e.features.hookScore,\n        watchability: e.features.retentionScore,\n        visuals: e.features.visualScore,\n        safety: e.features.safetyScore,\n        novelty: e.features.noveltyScore,\n        coherence: e.features.coherenceScore,\n        durationFit: e.features.closureScore,\n      },\n      aiOverall: aiOverallById[e.clipId] || undefined,\n      durationChoice: e.durationChoice,\n      nearHotspot,\n    };\n    return rankIn;\n  });\n}\n\nasync function processVideo(job: Job<VideoJob>) {\n  const { videoId, userId } = job.data;\n\n  if (!userId) {\n    throw new Error(\"User ID is required for video processing\");\n  }\n\n  const existingClips = await prisma.clip.count({\n    where: { videoId },\n  });\n\n  if (existingClips > 0) {\n    console.log(\n      `Deleting ${existingClips} existing clips from previous processing attempt`,\n    );\n    await prisma.clip.deleteMany({\n      where: { videoId },\n    });\n  }\n\n  const video = await prisma.video.findUnique({\n    where: { id: videoId },\n    include: { user: true },\n  });\n\n  if (!video) {\n    throw new Error(`Video ${videoId} not found`);\n  }\n\n  try {\n    await prisma.video.update({\n      where: { id: videoId },\n      data: { status: \"processing\" },\n    });\n  } catch (updateError: any) {\n    if (updateError.code === 'P2025') {\n      console.log(`Video ${videoId} was deleted before processing could start`);\n      throw new Error(`Video ${videoId} not found`);\n    }\n    throw updateError;\n  }\n\n  const workDir = join(tmpdir(), `video_${videoId}`);\n\n  if (!existsSync(workDir)) {\n    mkdirSync(workDir, { recursive: true });\n  }\n\n  try {\n    const videoPath = join(workDir, \"source.mp4\");\n    const audioPath = join(workDir, \"audio.m4a\");\n    const transcriptionAudioPath = join(workDir, \"audio_transcription.mp3\");\n\n    console.log(`Fetching video metadata for chapters`);\n    const metadata = await getVideoMetadata(video.sourceUrl, userId);\n    console.log(`Found ${metadata.chapters.length} chapters`);\n\n    if (metadata.chapters.length > 0) {\n      metadata.chapters.forEach((ch, i) => {\n        console.log(\n          `  Chapter ${i + 1}: ${ch.title} (${ch.startSec.toFixed(1)}s - ${ch.endSec.toFixed(1)}s)`,\n        );\n      });\n    }\n\n    await checkCancelled(videoId);\n\n    console.log(`Downloading video: ${video.sourceUrl}`);\n    const sourceInfo = await downloadVideo(video.sourceUrl, videoPath, userId);\n    console.log(\n      `Source: ${sourceInfo.height}p ${sourceInfo.fps}fps ${sourceInfo.vcodec} @ ${sourceInfo.tbr}kbps`,\n    );\n\n    await checkCancelled(videoId);\n\n    console.log(`Validating source quality`);\n    const br = await probeBitrate(videoPath);\n    console.log(\n      `Measured bitrate: ${br.kbps.toFixed(0)} kbps, size: ${(br.size / 1024 / 1024).toFixed(1)} MB, duration: ${br.seconds.toFixed(1)}s`,\n    );\n\n    const minBitrate =\n      sourceInfo.height >= 1080 ? 3000 : sourceInfo.height >= 720 ? 1500 : 1000;\n\n    if (br.kbps < minBitrate) {\n      console.warn(\n        `⚠️  Quality Gate: Source bitrate ${br.kbps.toFixed(0)} kbps is below recommended ${minBitrate} kbps for ${sourceInfo.height}p`,\n      );\n    } else {\n      console.log(\n        `✓ Quality Gate: Source quality meets ${sourceInfo.height}p standards (${br.kbps.toFixed(0)} >= ${minBitrate} kbps)`,\n      );\n    }\n\n    await checkCancelled(videoId);\n\n    console.log(`Extracting audio`);\n    await extractAudio(videoPath, audioPath);\n\n    console.log(`Compressing audio for transcription`);\n    await compressAudioForTranscription(audioPath, transcriptionAudioPath);\n\n    await checkCancelled(videoId);\n\n    console.log(`Transcribing audio`);\n    const transcript = await transcribeAudio(\n      transcriptionAudioPath,\n      metadata.chapters,\n    );\n\n    await checkCancelled(videoId);\n\n    await prisma.video.update({\n      where: { id: videoId },\n      data: { transcript: transcript as any },\n    });\n\n    console.log(`Detecting scene changes`);\n    const sceneChanges = await detectScenes(videoPath);\n    console.log(`Found ${sceneChanges.length} scene changes`);\n\n    await checkCancelled(videoId);\n\n    let commentHotspots: number[] = [];\n\n    const ytVideoId = extractVideoIdFromUrl(video.sourceUrl);\n\n    if (ytVideoId) {\n      console.log(`Fetching YouTube comments for engagement analysis`);\n      const comments = await fetchVideoComments(ytVideoId, 100);\n\n      if (comments.length > 0) {\n        commentHotspots = mineTimestampsFromComments(comments);\n        console.log(\n          `Found ${commentHotspots.length} comment hotspots at: ${commentHotspots.map((t) => `${Math.floor(t / 60)}:${String(t % 60).padStart(2, \"0\")}`).join(\", \")}`,\n        );\n      } else {\n        console.log(`No comments found, using engagement score defaults`);\n      }\n\n      await prisma.video.update({\n        where: { id: videoId },\n        data: { commentTimestampHotspotsJson: commentHotspots as any },\n      });\n    } else {\n      console.log(`Could not extract video ID, skipping comment fetching`);\n\n      await prisma.video.update({\n        where: { id: videoId },\n        data: { commentTimestampHotspotsJson: [] as any },\n      });\n    }\n\n    console.log(`Detecting segments with enhanced v2 algorithm`);\n    const segments = detectEnhancedSegments(\n      transcript,\n      sceneChanges,\n      metadata.chapters,\n      video.durationSec,\n      commentHotspots,\n    );\n\n    console.log(`Found ${segments.length} candidate segments`);\n\n    await checkCancelled(videoId);\n\n    console.log(`Stage 1: Filtering with rule-based 7-pillar scores`);\n    const sortedByRuleScore = [...segments].sort((a, b) => b.score - a.score);\n    const topCandidates = sortedByRuleScore.slice(0, 5);\n    console.log(\n      `Selected top ${topCandidates.length} candidates for AI scoring (saving ${segments.length - topCandidates.length} GPT-4o calls)`,\n    );\n\n    await checkCancelled(videoId);\n\n    console.log(`Stage 2: Scoring top candidates with GPT-4o`);\n    const stableTimestamp = Date.now();\n    const aiOverallById: Record<string, number> = {};\n    const segmentIdMap: Record<string, EnhancedSegment & { clipId: string }> =\n      {};\n\n    for (let i = 0; i < segments.length; i++) {\n      await checkCancelled(videoId);\n      const seg = segments[i];\n      const clipId = `clip_${stableTimestamp}_${videoId}_${i}`;\n      segmentIdMap[clipId] = { ...seg, clipId };\n\n      const isTopCandidate = topCandidates.includes(seg);\n      if (isTopCandidate) {\n        try {\n          const scores = await scoreClip(video.title, seg.hook, seg.text);\n          aiOverallById[clipId] = scores.scores.overall;\n          console.log(\n            `  Candidate ${topCandidates.indexOf(seg) + 1}/${topCandidates.length}: AI overall score = ${scores.scores.overall}`,\n          );\n        } catch (err) {\n          console.error(\n            `  Failed to score candidate ${topCandidates.indexOf(seg) + 1}:`,\n            err,\n          );\n          aiOverallById[clipId] = 50;\n        }\n      }\n    }\n\n    await checkCancelled(videoId);\n\n    console.log(`Ranking and selecting best clips with diversity`);\n    const segmentsWithIds = Object.values(segmentIdMap);\n    const rankInputs = buildRankInputsFromEnhanced(\n      segmentsWithIds,\n      videoId,\n      aiOverallById,\n      commentHotspots,\n    );\n    const ranked = finalizeBestClips(rankInputs, 5);\n\n    console.log(\n      `Selected ${ranked.length} clips: ${ranked.map((r) => `${r.tier}-tier`).join(\", \")}`,\n    );\n\n    const selectedSegments = ranked.map((r) => ({\n      ...segmentIdMap[r.id],\n      clipId: r.id,\n      tier: r.tier,\n      rankScore: r.rankScore,\n      reasons: r.reasons,\n      aiOverall: aiOverallById[r.id],\n    }));\n\n    console.log(`Probing video dimensions for framing`);\n    const videoProbe = await probeVideo(videoPath);\n    const baseW = videoProbe.width;\n    const baseH = videoProbe.height;\n    console.log(`Video dimensions: ${baseW}x${baseH}`);\n\n    console.log(`Computing GLOBAL crop for entire video (consistent across all clips)`);\n    let globalCrop = null;\n    try {\n      const { computeGlobalStaticCrop } = await import(\"./services/framingService\");\n      globalCrop = await computeGlobalStaticCrop(\n        videoPath,\n        video.durationSec,\n        baseW,\n        baseH\n      );\n      \n      if (globalCrop) {\n        console.log(`✓ Global crop computed: ${globalCrop.cropW}x${globalCrop.cropH} @ (${globalCrop.cropX},${globalCrop.cropY})`);\n        \n        await prisma.video.update({\n          where: { id: videoId },\n          data: { globalCropMapJson: globalCrop },\n        });\n        console.log(`✓ Stored global crop in database`);\n      } else {\n        console.log(`⚠️  No persons detected for global crop, will use per-segment framing`);\n      }\n    } catch (err) {\n      console.error(`Failed to compute global crop:`, err);\n      console.log(`Will fall back to per-segment framing`);\n    }\n\n    const framingConstraints: Constraints = {\n      margin: 0.02,\n      maxPan: 400,\n      easeMs: 600,\n      centerBiasX: 0.75,\n      centerBiasY: 0.15,\n      safeTop: 0.05,\n      safeBottom: 0.1,\n    };\n\n    const processSegment = async (segment: any, i: number) => {\n      await checkCancelled(videoId);\n\n      const clipId = segment.clipId;\n      const clipDir = join(workDir, `clip_${i}`);\n\n      if (!existsSync(clipDir)) {\n        mkdirSync(clipDir, { recursive: true });\n      }\n\n      const clipPath = join(clipDir, \"clip.mp4\");\n      const thumbPath = join(clipDir, \"thumb.jpg\");\n      const assPath = join(clipDir, \"clip.ass\");\n\n      const adjustedWords = segment.words.map((w: any) => ({\n        word: w.word,\n        start: w.start - segment.startSec,\n        end: w.end - segment.startSec,\n      }));\n\n      createAssWordByWordFile(adjustedWords, assPath, 1);\n\n      let cropMap = null;\n      let smartFramed = false;\n\n      const framingWords: FramingWord[] = segment.words.map((w: any) => ({\n        t: w.start,\n        end: w.end,\n        text: w.word,\n        speaker: w.speaker,\n      }));\n\n      console.log(\n        `Computing static person-centered framing for clip ${i + 1}/${selectedSegments.length}`,\n      );\n\n      try {\n        const kf = await computeCropMapPersonStatic(\n          {\n            videoPath,\n            baseW,\n            baseH,\n            segStart: segment.startSec,\n            segEnd: segment.endSec,\n            transcript: framingWords,\n          },\n          framingConstraints,\n          globalCrop,\n        );\n\n        if (kf && kf.length > 0) {\n          smartFramed = true;\n          cropMap = kf;\n          console.log(\n            `✓ Generated ${kf.length} framing keyframes for clip ${i + 1}`,\n          );\n\n          const filterExpr = buildFFmpegFilter(baseW, baseH, kf);\n\n          await renderSmartFramedClip({\n            inputPath: videoPath,\n            outputPath: clipPath,\n            startTime: segment.startSec,\n            duration: segment.durationSec,\n            srtPath: assPath,\n            filterExpr,\n          });\n        } else {\n          console.log(\n            `⚠️  No persons detected in clip ${i + 1}, using center crop`,\n          );\n        }\n      } catch (err) {\n        console.error(\n          `Framing failed for clip ${i + 1}, using center crop:`,\n          err,\n        );\n      }\n\n      if (!smartFramed) {\n        await renderVerticalClip({\n          inputPath: videoPath,\n          outputPath: clipPath,\n          startTime: segment.startSec,\n          duration: segment.durationSec,\n          srtPath: assPath,\n        });\n      }\n\n      const clipBitrate = await probeBitrate(clipPath);\n\n      await checkCancelled(videoId);\n\n      const scores = await scoreClip(video.title, segment.hook, segment.text);\n      const taxonomy = inferTaxonomy(\n        segment.text,\n        segment.hook,\n        scores.category,\n      );\n\n      await Promise.all([extractThumbnail(clipPath, thumbPath, 1)]);\n\n      await checkCancelled(videoId);\n\n      const s3VideoKey = `videos/${videoId}/clips/${clipId}/clip.mp4`;\n      const s3ThumbKey = `videos/${videoId}/clips/${clipId}/thumb.jpg`;\n      const s3SrtKey = `videos/${videoId}/clips/${clipId}/clip.srt`;\n\n      await Promise.all([\n        uploadFile(s3VideoKey, clipPath, \"video/mp4\"),\n        uploadFile(s3ThumbKey, thumbPath, \"image/jpeg\"),\n        uploadFile(s3SrtKey, assPath, \"text/plain\"),\n      ]);\n\n      await checkCancelled(videoId);\n\n      await prisma.clip.create({\n        data: {\n          id: clipId,\n          videoId,\n          startSec: Math.floor(segment.startSec),\n          endSec: Math.floor(segment.endSec),\n          durationSec: Math.floor(segment.durationSec),\n          category: taxonomy.category,\n          tags: scores.tags,\n          scoreHook: scores.scores.hook_strength,\n          scoreRetention: scores.scores.retention_likelihood,\n          scoreClarity: scores.scores.clarity,\n          scoreShare: scores.scores.shareability,\n          scoreOverall: segment.aiOverall || scores.scores.overall,\n          rationale: scores.rationale,\n          rationaleShort: segment.rationaleShort || scores.rationale,\n          featuresJson: segment.features ? (segment.features as any) : null,\n          durationChoice: segment.durationChoice || null,\n          s3VideoKey,\n          s3ThumbKey,\n          s3SrtKey,\n          smartFramed,\n          cropMapJson: cropMap ? (cropMap as any) : null,\n        },\n      });\n    };\n\n    const clipSuccesses: string[] = [];\n    const clipFailures: Array<{ index: number; error: any }> = [];\n\n    for (let i = 0; i < selectedSegments.length; i++) {\n      await checkCancelled(videoId);\n\n      try {\n        await processSegment(selectedSegments[i], i);\n        clipSuccesses.push(selectedSegments[i].clipId);\n      } catch (err: any) {\n        console.error(`Clip ${i + 1} failed:`, err);\n        clipFailures.push({ index: i, error: err });\n\n        if (err?.message?.includes(\"cancelled\")) {\n          throw err;\n        }\n      }\n    }\n\n    const summary = {\n      totalCandidates: segments.length,\n      selectedClips: selectedSegments.length,\n      successfulClips: clipSuccesses.length,\n      failedClips: clipFailures.length,\n      tierBreakdown: selectedSegments.reduce(\n        (acc, seg) => {\n          acc[seg.tier] = (acc[seg.tier] || 0) + 1;\n          return acc;\n        },\n        {} as Record<string, number>,\n      ),\n      failures: clipFailures.map((f) => ({\n        clip: `clip_${f.index}`,\n        error: f.error?.message || String(f.error),\n      })),\n    };\n\n    console.log(\"Processing summary:\", JSON.stringify(summary, null, 2));\n\n    const finalVideo = await prisma.video.findUnique({\n      where: { id: videoId },\n    });\n    if (finalVideo) {\n      try {\n        await prisma.video.update({\n          where: { id: videoId },\n          data: { status: \"completed\" },\n        });\n      } catch (completedUpdateError: any) {\n        if (completedUpdateError.code === 'P2025') {\n          console.log(`Video ${videoId} was deleted during completion`);\n        } else {\n          throw completedUpdateError;\n        }\n      }\n    }\n\n    console.log(\n      `Video ${videoId} processing completed with ${clipSuccesses.length} successful clips and ${clipFailures.length} failures`,\n    );\n  } catch (error: any) {\n    console.error(`Error processing video ${videoId}:`, error);\n\n    if (error?.message?.includes(\"cancelled by user\")) {\n      console.log(\n        `Video ${videoId} was cancelled by user, keeping cancelled status`,\n      );\n    } else {\n      const errorVideo = await prisma.video.findUnique({\n        where: { id: videoId },\n      });\n      if (errorVideo) {\n        try {\n          await prisma.video.update({\n            where: { id: videoId },\n            data: { status: \"failed\" },\n          });\n        } catch (failedUpdateError: any) {\n          if (failedUpdateError.code === 'P2025') {\n            console.log(`Video ${videoId} was deleted during error handling`);\n            return;\n          }\n          throw failedUpdateError;\n        }\n        throw error;\n      } else {\n        console.log(\n          `Video ${videoId} was deleted before processing could complete`,\n        );\n        return;\n      }\n    }\n  } finally {\n    if (existsSync(workDir)) {\n      rmSync(workDir, { recursive: true, force: true });\n    }\n  }\n}\n\nasync function initializeWorker() {\n  console.log(\"Initializing worker...\");\n  \n  try {\n    await ensureModelsDownloaded();\n    await initializeFaceDetection();\n    console.log(\"Face detection models initialized successfully\");\n    const { initializeCanvas } = await import(\"./services/framingService\");\n    await initializeCanvas();\n    console.log(\"Canvas initialized successfully\");\n  }\n  catch (error) {\n    console.error(\"Failed to initialize worker:\", error);\n    throw error;\n  }\n\n  const worker = new Worker<VideoJob>(\"video.process\", processVideo, {\n    connection,\n    concurrency: 1,\n    lockDuration: 1800000,\n    lockRenewTime: 30000,\n  });\n\n  worker.on(\"completed\", (job) => {\n    console.log(`Job ${job.id} completed`);\n  });\n\n  worker.on(\"failed\", (job, err) => {\n    console.error(`Job ${job?.id} failed:`, err);\n  });\n\n  cleanupTempFiles();\n  console.log(\"Worker started and ready to process jobs\");\n}\n\ninitializeWorker().catch((error) => {\n  console.error(\"Failed to initialize worker:\", error);\n  process.exit(1);\n});\n","size_bytes":19657},"app/api/auth/google/start/route.ts":{"content":"import { NextResponse } from \"next/server\";\nimport { OAuth2Client } from \"google-auth-library\";\n\nconst getOAuthClient = () => {\n  const clientId = process.env.GOOGLE_CLIENT_ID;\n  const clientSecret = process.env.GOOGLE_CLIENT_SECRET;\n\n  let redirectUri = process.env.GOOGLE_REDIRECT_URI;\n\n  if (!clientId || !clientSecret) {\n    throw new Error(\"Google OAuth credentials not configured\");\n  }\n  console.log(\"Call Redirect URI:\", redirectUri);\n  console.log(\"Call Client ID:\", clientId);\n  console.log(\"Call Client Secret:\", clientSecret);\n  return new OAuth2Client(clientId, clientSecret, redirectUri);\n};\n\nexport async function GET() {\n  try {\n    const oauth2Client = getOAuthClient();\n\n    const authUrl = oauth2Client.generateAuthUrl({\n      access_type: \"offline\",\n      scope: [\n        \"https://www.googleapis.com/auth/youtube.readonly\",\n        \"https://www.googleapis.com/auth/userinfo.email\",\n        \"https://www.googleapis.com/auth/userinfo.profile\",\n        \"openid\",\n      ],\n      prompt: \"consent\",\n    });\n\n    return NextResponse.json({ authUrl });\n  } catch (error: any) {\n    console.error(\"Error generating OAuth URL:\", error);\n    return NextResponse.json(\n      { error: \"Failed to generate OAuth URL\" },\n      { status: 500 },\n    );\n  }\n}\n","size_bytes":1264},"next-env.d.ts":{"content":"/// <reference types=\"next\" />\n/// <reference types=\"next/image-types/global\" />\n\n// NOTE: This file should not be edited\n// see https://nextjs.org/docs/basic-features/typescript for more information.\n","size_bytes":201},"postcss.config.js":{"content":"module.exports = {\n  plugins: {\n    tailwindcss: {},\n    autoprefixer: {},\n  },\n}\n","size_bytes":82},"src/lib/rate-limit.ts":{"content":"import rateLimit from 'express-rate-limit'\n\nexport const apiLimiter = rateLimit({\n  windowMs: 15 * 60 * 1000,\n  max: 100,\n  standardHeaders: true,\n  legacyHeaders: false,\n  message: 'Too many requests from this IP, please try again later.'\n})\n\nexport const authLimiter = rateLimit({\n  windowMs: 15 * 60 * 1000,\n  max: 5,\n  standardHeaders: true,\n  legacyHeaders: false,\n  message: 'Too many authentication attempts, please try again later.'\n})\n","size_bytes":444},"app/login/page.tsx":{"content":"'use client'\n\nimport { useState, useEffect } from 'react'\nimport { useRouter, useSearchParams } from 'next/navigation'\n\nexport default function LoginPage() {\n  const [loading, setLoading] = useState(false)\n  const [error, setError] = useState('')\n  const router = useRouter()\n  const searchParams = useSearchParams()\n\n  useEffect(() => {\n    checkAuth()\n    \n    const errorParam = searchParams.get('error')\n    \n    if (errorParam)\n    {\n      setError('Authentication failed. Please try again.')\n    }\n  }, [searchParams])\n\n  async function checkAuth() {\n    try {\n      const res = await fetch('/api/auth/status')\n      const data = await res.json()\n      \n      if (data.isAuthenticated)\n      {\n        router.push('/')\n      }\n    }\n    catch (err) {\n      console.error('Failed to check auth status:', err)\n    }\n  }\n\n  async function handleGoogleLogin() {\n    setLoading(true)\n    setError('')\n    \n    try {\n      const res = await fetch('/api/auth/google/start')\n      const data = await res.json()\n      \n      if (!res.ok)\n      {\n        throw new Error(data.error || 'Failed to start OAuth flow')\n      }\n      \n      window.location.href = data.authUrl\n    }\n    catch (err: any) {\n      setError(err.message || 'Failed to initiate Google login')\n      setLoading(false)\n    }\n  }\n\n  return (\n    <div className=\"min-h-screen bg-gray-900 text-white flex items-center justify-center p-4\">\n      <div className=\"max-w-md w-full bg-gray-800 rounded-lg shadow-xl p-8\">\n        <h1 className=\"text-3xl font-bold mb-2\">YT Shortsmith</h1>\n        <p className=\"text-gray-400 mb-8\">Sign in with Google to continue</p>\n        \n        {error && (\n          <div className=\"mb-6 p-4 bg-red-900/50 border border-red-700 rounded-lg\">\n            <p className=\"text-red-200 text-sm\">{error}</p>\n          </div>\n        )}\n        \n        <button\n          onClick={handleGoogleLogin}\n          disabled={loading}\n          className=\"w-full bg-blue-600 hover:bg-blue-700 disabled:bg-gray-600 disabled:cursor-not-allowed text-white font-semibold py-3 px-4 rounded-lg transition-colors flex items-center justify-center gap-2\"\n        >\n          {loading ? (\n            'Redirecting...'\n          ) : (\n            <>\n              <svg className=\"w-5 h-5\" viewBox=\"0 0 24 24\" fill=\"currentColor\">\n                <path d=\"M22.56 12.25c0-.78-.07-1.53-.2-2.25H12v4.26h5.92c-.26 1.37-1.04 2.53-2.21 3.31v2.77h3.57c2.08-1.92 3.28-4.74 3.28-8.09z\" fill=\"#4285F4\"/>\n                <path d=\"M12 23c2.97 0 5.46-.98 7.28-2.66l-3.57-2.77c-.98.66-2.23 1.06-3.71 1.06-2.86 0-5.29-1.93-6.16-4.53H2.18v2.84C3.99 20.53 7.7 23 12 23z\" fill=\"#34A853\"/>\n                <path d=\"M5.84 14.09c-.22-.66-.35-1.36-.35-2.09s.13-1.43.35-2.09V7.07H2.18C1.43 8.55 1 10.22 1 12s.43 3.45 1.18 4.93l2.85-2.22.81-.62z\" fill=\"#FBBC05\"/>\n                <path d=\"M12 5.38c1.62 0 3.06.56 4.21 1.64l3.15-3.15C17.45 2.09 14.97 1 12 1 7.7 1 3.99 3.47 2.18 7.07l3.66 2.84c.87-2.6 3.3-4.53 6.16-4.53z\" fill=\"#EA4335\"/>\n              </svg>\n              Sign in with Google\n            </>\n          )}\n        </button>\n        \n        <div className=\"mt-8 pt-6 border-t border-gray-700\">\n          <h2 className=\"font-semibold mb-2\">What happens next?</h2>\n          <ul className=\"text-sm text-gray-400 space-y-2\">\n            <li className=\"flex items-start gap-2\">\n              <span className=\"text-blue-400 mt-0.5\">1.</span>\n              <span>Sign in with your Google account</span>\n            </li>\n            <li className=\"flex items-start gap-2\">\n              <span className=\"text-blue-400 mt-0.5\">2.</span>\n              <span>Start creating short clips from YouTube videos</span>\n            </li>\n          </ul>\n        </div>\n      </div>\n    </div>\n  )\n}\n","size_bytes":3747},"src/services/openai.ts":{"content":"import OpenAI from \"openai\";\nimport { createReadStream, statSync } from \"fs\";\nimport { getFileSizeBytes, getDurationSeconds } from \"./ffmpeg\";\nimport ffmpeg from \"fluent-ffmpeg\";\nimport ffmpegPath from \"ffmpeg-static\";\nimport { join } from \"path\";\n\nffmpeg.setFfmpegPath(ffmpegPath!);\n\nconst openai = new OpenAI({\n  apiKey: process.env.OPENAI_API_KEY,\n  timeout: 600000,\n  maxRetries: 3,\n});\n\nexport interface TranscriptWord {\n  word: string;\n  start: number;\n  end: number;\n}\n\nexport interface TranscriptSegment {\n  text: string;\n  start: number;\n  end: number;\n  words: TranscriptWord[];\n  language?: string;\n}\n\nconst TARGET_MB = parseFloat(process.env.OPENAI_CHUNK_TARGET_MB || \"24.5\");\nconst TARGET_BYTES = Math.floor(TARGET_MB * 1024 * 1024);\nconst MAX_RETRIES = parseInt(process.env.OPENAI_MAX_RETRIES || \"2\", 10);\n\nexport async function withRetries<T>(fn: () => Promise<T>): Promise<T> {\n  let attempt = 0;\n  let delay = 1000;\n\n  for (;;) {\n    try {\n      const res = await fn();\n      return res;\n    } catch (err: any) {\n      attempt = attempt + 1;\n      const code = err?.status || err?.code || 0;\n      const transient =\n        code === 429 ||\n        code === 408 ||\n        code === 500 ||\n        code === 502 ||\n        code === 503 ||\n        code === 504;\n\n      if (attempt > MAX_RETRIES) {\n        throw err;\n      }\n\n      if (!transient) {\n        throw err;\n      }\n\n      await new Promise((r) => {\n        setTimeout(r, delay);\n      });\n      delay = delay * 2;\n    }\n  }\n}\n\nexport async function planAudioChunksBySize(\n  inputPath: string,\n  durationSec: number,\n): Promise<{ start: number; duration: number }[]> {\n  const sizeBytes = getFileSizeBytes(inputPath);\n  let chunks = Math.ceil(sizeBytes / TARGET_BYTES);\n\n  if (chunks < 1) {\n    chunks = 1;\n  }\n\n  const base = Math.floor(durationSec / chunks);\n  const rem = durationSec - base * chunks;\n  const plan: { start: number; duration: number }[] = [];\n  let cursor = 0;\n\n  for (let i = 0; i < chunks; i++) {\n    let d = base;\n\n    if (i < rem) {\n      d = d + 1;\n    }\n\n    plan.push({ start: cursor, duration: d });\n    cursor = cursor + d;\n  }\n\n  return plan;\n}\n\nasync function extractAudioChunk(\n  inputPath: string,\n  outputPath: string,\n  start: number,\n  duration: number,\n): Promise<void> {\n  return new Promise((resolve, reject) => {\n    ffmpeg(inputPath)\n      .setStartTime(start)\n      .duration(duration)\n      .outputOptions([\"-c:a\", \"copy\"])\n      .output(outputPath)\n      .on(\"end\", () => resolve())\n      .on(\"error\", reject)\n      .run();\n  });\n}\n\nasync function transcribeAudioFile(\n  audioPath: string,\n  timeOffset: number = 0,\n  retries = 5,\n): Promise<{ words: TranscriptWord[]; language: string }> {\n  let lastError: Error | null = null;\n\n  for (let attempt = 1; attempt <= retries; attempt++) {\n    try {\n      const response = await openai.audio.transcriptions.create({\n        file: createReadStream(audioPath),\n        model: \"whisper-1\",\n        response_format: \"verbose_json\",\n        timestamp_granularities: [\"word\"],\n      });\n\n      const words: TranscriptWord[] = [];\n\n      if (response.words) {\n        for (const word of response.words) {\n          words.push({\n            word: word.word,\n            start: word.start + timeOffset,\n            end: word.end + timeOffset,\n          });\n        }\n      }\n\n      const detectedLanguage = response.language || \"en\";\n      console.log(`Detected language: ${detectedLanguage}`);\n\n      return { words, language: detectedLanguage };\n    } catch (error: any) {\n      lastError = error;\n\n      if (\n        error.code === \"ECONNRESET\" ||\n        error.cause?.code === \"ECONNRESET\" ||\n        error.status === 500 ||\n        error.status === 503\n      ) {\n        const delay = Math.min(1000 * Math.pow(2, attempt - 1), 30000);\n        console.log(\n          `Transcription attempt ${attempt}/${retries} failed with network error. Retrying in ${delay}ms...`,\n        );\n        await new Promise((resolve) => setTimeout(resolve, delay));\n        continue;\n      }\n\n      throw error;\n    }\n  }\n\n  throw lastError || new Error(\"Failed to transcribe audio after retries\");\n}\n\nfunction isIntroChapter(title: string, detectedLanguage?: string): boolean {\n  const titleLower = title.toLowerCase().trim();\n\n  const introKeywords: Record<string, string[]> = {\n    en: [\"intro\", \"introduction\", \"opening\", \"welcome\", \"trailer\", \"credits\"],\n    es: [\n      \"intro\",\n      \"introduccion\",\n      \"introducci\\u00f3n\",\n      \"apertura\",\n      \"inicio\",\n      \"inicio del video\",\n      \"inicio del v\\u00eddeo\",\n      \"bienvenida\",\n    ],\n    pt: [\n      \"intro\",\n      \"introducao\",\n      \"introdu\\u00e7ao\",\n      \"introdu\\u00e7\\u00e3o\",\n      \"apresentacao\",\n      \"apresenta\\u00e7ao\",\n      \"apresenta\\u00e7\\u00e3o\",\n      \"abertura\",\n      \"boas-vindas\",\n    ],\n    fr: [\"intro\", \"introduction\", \"ouverture\", \"bienvenue\"],\n    de: [\n      \"intro\",\n      \"einf\\u00fchrung\",\n      \"einleitung\",\n      \"er\\u00f6ffnung\",\n      \"willkommen\",\n    ],\n    it: [\"intro\", \"introduzione\", \"apertura\", \"benvenuto\"],\n    ja: [\"イントロ\", \"紹介\", \"オープニング\"],\n    ko: [\"인트로\", \"소개\", \"오프닝\"],\n    zh: [\"介绍\", \"简介\", \"开场\"],\n    ru: [\"вступление\", \"введение\", \"открытие\"],\n  };\n\n  let keywordsToCheck = introKeywords[\"en\"] || [];\n\n  if (detectedLanguage && introKeywords[detectedLanguage]) {\n    keywordsToCheck = [\n      ...introKeywords[detectedLanguage],\n      ...introKeywords[\"en\"],\n    ];\n  } else {\n    keywordsToCheck = Object.values(introKeywords).flat();\n  }\n\n  for (const keyword of keywordsToCheck) {\n    if (titleLower.includes(keyword)) {\n      return true;\n    }\n  }\n\n  if (\n    titleLower.length < 20 &&\n    titleLower.match(\n      /^(chapter|cap[íi]tulo|part|parte|section|se[cç][aã]o|episode|epis[óo]dio)\\s*[0-9]+/,\n    )\n  ) {\n    return false;\n  }\n\n  return false;\n}\n\nfunction calculateIntroSkip(chapters: any[], duration: number): number {\n  const defaultSkip = parseInt(process.env.INTRO_SKIP_SECONDS || \"180\", 10);\n\n  if (!chapters || chapters.length === 0) {\n    console.log(`No chapters found, using default skip: ${defaultSkip}s`);\n    return defaultSkip;\n  }\n\n  const firstChapter = chapters[0];\n\n  if (isIntroChapter(firstChapter.title)) {\n    const chapterEnd = firstChapter.endSec || firstChapter.end_time || 0;\n    console.log(\n      `First chapter \"${firstChapter.title}\" detected as intro, skipping to ${chapterEnd}s`,\n    );\n    return chapterEnd;\n  }\n\n  console.log(\n    `First chapter \"${firstChapter.title}\" not detected as intro, processing from start`,\n  );\n  return 0;\n}\n\nexport async function transcribeAudio(\n  audioPath: string,\n  chapters: any[] = [],\n): Promise<TranscriptSegment[]> {\n  const fileSize = getFileSizeBytes(audioPath);\n  const duration = await getDurationSeconds(audioPath);\n\n  const introSkip = calculateIntroSkip(chapters, duration);\n\n  let effectiveStart = 0;\n\n  if (duration > introSkip) {\n    effectiveStart = introSkip;\n    console.log(`Skipping first ${introSkip}s of audio (intro skip)`);\n  }\n\n  const effectiveDuration = duration - effectiveStart;\n  let allWords: TranscriptWord[] = [];\n  let detectedLanguage = \"en\";\n\n  if (fileSize > TARGET_BYTES) {\n    console.log(\n      `Audio file is ${(fileSize / 1024 / 1024).toFixed(1)}MB, splitting into chunks...`,\n    );\n    const chunkPlan = await planAudioChunksBySize(audioPath, effectiveDuration);\n\n    console.log(\n      `Transcribing ${chunkPlan.length} chunks in parallel (max 3 concurrent)...`,\n    );\n\n    const audioDir = audioPath.substring(0, audioPath.lastIndexOf(\"/\"));\n    const audioExt = audioPath.substring(audioPath.lastIndexOf(\".\"));\n\n    const transcribeChunk = async (\n      plan: { start: number; duration: number },\n      index: number,\n    ) => {\n      console.log(`Transcribing chunk ${index + 1}/${chunkPlan.length}`);\n      const chunkPath = join(audioDir, `chunk_${index}${audioExt}`);\n      const absoluteStart = effectiveStart + plan.start;\n      await extractAudioChunk(\n        audioPath,\n        chunkPath,\n        absoluteStart,\n        plan.duration,\n      );\n      return await transcribeAudioFile(chunkPath, absoluteStart);\n    };\n\n    const chunkResults: { words: TranscriptWord[]; language: string }[] = [];\n    const chunkErrors: Array<{ index: number; error: any }> = [];\n\n    for (let i = 0; i < chunkPlan.length; i += 3) {\n      const batch = chunkPlan.slice(i, i + 3);\n      const batchResults = await Promise.allSettled(\n        batch.map((plan, idx) => transcribeChunk(plan, i + idx)),\n      );\n\n      for (let j = 0; j < batchResults.length; j++) {\n        const result = batchResults[j];\n\n        if (result.status === \"fulfilled\") {\n          chunkResults.push(result.value);\n        } else {\n          const chunkIndex = i + j;\n          console.error(\n            `Chunk ${chunkIndex + 1} failed to transcribe:`,\n            result.reason,\n          );\n          chunkErrors.push({ index: chunkIndex, error: result.reason });\n        }\n      }\n    }\n\n    if (chunkErrors.length > 0 && chunkResults.length === 0) {\n      throw new Error(\n        `All chunks failed to transcribe. Errors: ${JSON.stringify(chunkErrors)}`,\n      );\n    }\n\n    if (chunkResults.length > 0) {\n      detectedLanguage = chunkResults[0].language;\n    }\n\n    allWords = chunkResults.flatMap((r) => r.words);\n  } else {\n    let result: { words: TranscriptWord[]; language: string };\n\n    if (effectiveStart > 0) {\n      const audioDir = audioPath.substring(0, audioPath.lastIndexOf(\"/\"));\n      const audioExt = audioPath.substring(audioPath.lastIndexOf(\".\"));\n      const skippedPath = join(audioDir, `skipped${audioExt}`);\n      await extractAudioChunk(\n        audioPath,\n        skippedPath,\n        effectiveStart,\n        effectiveDuration,\n      );\n      result = await transcribeAudioFile(skippedPath, effectiveStart);\n    } else {\n      result = await transcribeAudioFile(audioPath);\n    }\n\n    allWords = result.words;\n    detectedLanguage = result.language;\n  }\n\n  const segments: TranscriptSegment[] = [];\n  let currentSegment: TranscriptWord[] = [];\n  let segmentStart = 0;\n  let segmentText = \"\";\n\n  for (let i = 0; i < allWords.length; i++) {\n    const word = allWords[i];\n\n    if (currentSegment.length === 0) {\n      segmentStart = word.start;\n    }\n\n    currentSegment.push(word);\n    segmentText += word.word + \" \";\n\n    if (i < allWords.length - 1) {\n      const gap = allWords[i + 1].start - word.end;\n\n      if (gap > 0.9 || currentSegment.length >= 50) {\n        segments.push({\n          text: segmentText.trim(),\n          start: segmentStart,\n          end: word.end,\n          words: currentSegment,\n          language: detectedLanguage,\n        });\n\n        currentSegment = [];\n        segmentText = \"\";\n      }\n    }\n  }\n\n  if (currentSegment.length > 0) {\n    segments.push({\n      text: segmentText.trim(),\n      start: segmentStart,\n      end: currentSegment[currentSegment.length - 1].end,\n      words: currentSegment,\n      language: detectedLanguage,\n    });\n  }\n\n  console.log(\n    `Transcription complete. Language: ${detectedLanguage}, Segments: ${segments.length}`,\n  );\n\n  return segments;\n}\n\nexport interface ScoreResult {\n  category: string;\n  tags: string[];\n  scores: {\n    hook_strength: number;\n    retention_likelihood: number;\n    clarity: number;\n    shareability: number;\n    overall: number;\n  };\n  rationale: string;\n}\n\nexport function isSentenceBoundaryToken(token: string): boolean {\n  const t = token?.trim?.() || ''\n  if (!t) { return false }\n  if (/[.!?…]$/.test(t)) { return true }\n  if (/--$/.test(t)) { return true }\n  return false\n}\n\nexport async function scoreClip(\n  title: string,\n  hook: string,\n  transcript: string,\n): Promise<ScoreResult> {\n  return withRetries(async () => {\n    const response = await openai.chat.completions.create({\n      model: \"gpt-4o\",\n      messages: [\n        {\n          role: \"system\",\n          content:\n            \"You are an expert at scoring short-form video clips for TikTok, Instagram Reels, and YouTube Shorts viral potential.\",\n        },\n        {\n          role: \"user\",\n          content: `Analyze this short video clip for viral potential on TikTok/Reels/Shorts.\n\nCRITICAL HOOK CRITERIA (first 1.5-2.0 seconds):\n- Must grab attention immediately with question, bold claim, or intrigue\n- Avoid slow cold opens - needs energy from frame 1\n- Strong hooks: \"How to...\", \"X vs Y\", questions, shocking facts, controversy, numbers\n\nRETENTION SIGNALS:\n- Fast pacing, low pause density, rising energy\n- Clear Q→A arc or story structure\n- Visual variety without being chaotic (2-4 scene changes ideal)\n- Payoff delivered by end, no trailing off\n\nTIKTOK-SPECIFIC:\n- First 2 seconds determine everything\n- Must work with sound OFF (assume subtitles carry meaning)\n- Clarity over complexity\n- Shareability: meme-able, relatable, or teaches something\n\nSCORING RUBRIC:\n- hook_strength (0-10): How compelling are the first 2 seconds for a cold audience?\n- retention_likelihood (0-10): Will viewers watch to the end? Pacing, payoff, engagement?\n- clarity (0-10): Is the message crystal clear? Minimal filler, coherent flow?\n- shareability (0-10): Would someone share this or save it? Relatable, useful, or entertaining?\n- overall (0-100): Weighted viral potential score\n\nReturn JSON with:\n{\n  \"category\": \"[Education|Motivation|Humor|Commentary|Tech|Lifestyle|News|Finance|Health|Sports|Gaming|Other]\",\n  \"tags\": [\"tag1\", \"tag2\", \"tag3\"],\n  \"scores\": {\n    \"hook_strength\": <0-10>,\n    \"retention_likelihood\": <0-10>,\n    \"clarity\": <0-10>,\n    \"shareability\": <0-10>,\n    \"overall\": <0-100>\n  },\n  \"rationale\": \"<one sentence explaining overall score>\"\n}\n\nINPUT:\nTitle: ${title}\nHook (first 2-3s): ${hook}\nFull transcript: ${transcript}\n\nJSON only, no markdown.`,\n        },\n      ],\n      response_format: { type: \"json_object\" },\n      temperature: 0.7,\n    });\n\n    const content = response.choices[0].message.content || \"{}\";\n    return JSON.parse(content) as ScoreResult;\n  });\n}\n","size_bytes":14029},"src/services/ffmpeg.ts":{"content":"import ffmpegStatic from \"ffmpeg-static\";\nimport ffprobeStatic from \"ffprobe-static\";\nimport { spawn } from \"child_process\";\nimport { writeFileSync } from \"fs\";\nimport * as fs from \"fs/promises\";\nimport path from \"path\";\n\n//gpt\nimport os from \"os\";\n\nconst ffmpegPath = ffmpegStatic;\nconst ffprobePath = ffprobeStatic.path;\n\ninterface Probe {\n  width: number;\n  height: number;\n  fps: number;\n}\n\nfunction run(\n  bin: string,\n  args: string[],\n): Promise<{ stdout: string; stderr: string }> {\n  return new Promise((resolve, reject) => {\n    const p = spawn(bin, args, { stdio: [\"ignore\", \"pipe\", \"pipe\"] });\n    let out = \"\";\n    let err = \"\";\n    p.stdout.on(\"data\", (d) => (out += d.toString()));\n    p.stderr.on(\"data\", (d) => (err += d.toString()));\n    p.on(\"close\", (code) => {\n      if (code === 0) {\n        resolve({ stdout: out, stderr: err });\n      }\n      if (code !== 0) {\n        reject(new Error(err || out));\n      }\n    });\n  });\n}\n\nexport async function probeVideo(file: string): Promise<Probe> {\n  const args = [\n    \"-v\",\n    \"error\",\n    \"-select_streams\",\n    \"v:0\",\n    \"-show_entries\",\n    \"stream=width,height,avg_frame_rate\",\n    \"-of\",\n    \"json\",\n    file,\n  ];\n  const { stdout } = await run(ffprobePath, args);\n  const j = JSON.parse(stdout);\n  const s = j.streams[0];\n  const fpsParts = String(s.avg_frame_rate || \"0/1\").split(\"/\");\n  const fps = Number(\n    fpsParts[1] === \"0\" ? 0 : Number(fpsParts[0]) / Number(fpsParts[1]),\n  );\n  return { width: Number(s.width), height: Number(s.height), fps };\n}\n\nfunction even(n: number): number {\n  if (n % 2 === 0) {\n    return n;\n  }\n  return n - 1;\n}\n\nfunction escapeDrawtext(text: string): string {\n  return text\n    .replace(/\\\\/g, \"\\\\\\\\\")\n    .replace(/'/g, \"\\\\'\")\n    .replace(/:/g, \"\\\\:\")\n    .replace(/\\[/g, \"\\\\[\")\n    .replace(/\\]/g, \"\\\\]\");\n}\n\nfunction escapeSubtitlesPath(p: string): string {\n  return p\n    .replace(/\\\\/g, \"\\\\\\\\\")\n    .replace(/'/g, \"\\\\'\")\n    .replace(/:/g, \"\\\\:\")\n    .replace(/\\[/g, \"\\\\[\")\n    .replace(/\\]/g, \"\\\\]\");\n}\n\nfunction chooseTargetSize(\n  srcW: number,\n  srcH: number,\n): { w: number; h: number } {\n  let h = srcH;\n  if (h > 1920) {\n    h = 1920;\n  }\n  const w = even(Math.round((h * 9) / 16));\n  return { w, h: even(h) };\n}\n\nfunction buildFilters(targetW: number, targetH: number): string {\n  const scaleW = \"'if(gt(iw/ih,0.5625),-2,\" + targetW + \")'\";\n  const scaleH = \"'if(gt(iw/ih,0.5625),\" + targetH + \",-2)'\";\n  const chain = [\n    \"scale=\" + scaleW + \":\" + scaleH,\n    \"crop=\" + targetW + \":\" + targetH + \":(in_w-out_w)/2:(in_h-out_h)/2\",\n    \"format=yuv420p\",\n  ];\n  return chain.join(\",\");\n}\n\n//gpt\nasync function writeFilterScript(content: string): Promise<string> {\n  const dir = await fs.mkdtemp(path.join(os.tmpdir(), \"ffvf-\"));\n  const p = path.join(dir, \"filtergraph.txt\");\n  await fs.writeFile(p, content, \"utf8\");\n  return p;\n}\n// Keep coords in-bounds and even (ffmpeg requires even chroma sizes)\nfunction clampExpr(expr: string, limitExpr: string): string {\n  // floor(.../2)*2 snaps to even; max/min clamp to [0, limit]\n  return `floor(max(0,min(${expr},${limitExpr}))/2)*2`;\n}\n\nfunction subtitlesFilter(srtOrAssPath: string): string {\n  const ext = path.extname(srtOrAssPath).toLowerCase();\n  const escaped = escapeSubtitlesPath(srtOrAssPath);\n  const fontsDir = path.resolve(process.cwd(), \"assets/fonts\");\n  const escapedFontsDir = fontsDir.replace(/\\\\/g, \"\\\\\\\\\").replace(/:/g, \"\\\\:\");\n  if (ext === \".ass\" || ext === \".ssa\") {\n    return \"subtitles='\" + escaped + \"':fontsdir='\" + escapedFontsDir + \"'\";\n  }\n  const force =\n    \"Alignment=5,FontName=Forever Freedom Regular Font Regular,FontSize=160,Bold=1,\" +\n    \"PrimaryColour=&H00FFFFFF,OutlineColour=&H00000000,BackColour=&H00000000,\" +\n    \"Outline=0,Shadow=2,Spacing=3,MarginV=-20,MarginL=40,MarginR=40\";\n  return (\n    \"subtitles='\" +\n    escaped +\n    \"':fontsdir='\" +\n    escapedFontsDir +\n    \"':force_style='\" +\n    force +\n    \"'\"\n  );\n}\n\nexport async function renderClip(\n  input: string,\n  startSec: number,\n  endSec: number,\n  srtPath: string | null,\n  outFile: string,\n): Promise<void> {\n  const p = await probeVideo(input);\n  const tgt = chooseTargetSize(p.width, p.height);\n  const filters = buildFilters(tgt.w, tgt.h);\n  const vf =\n    srtPath && srtPath.length > 0\n      ? filters + \",\" + subtitlesFilter(srtPath)\n      : filters;\n  const dur = Math.max(0, endSec - startSec);\n  const args = [\n    \"-y\",\n    \"-ss\",\n    String(startSec),\n    \"-t\",\n    String(dur),\n    \"-i\",\n    input,\n    \"-vf\",\n    vf,\n    \"-c:v\",\n    \"libx264\",\n    \"-profile:v\",\n    \"high\",\n    \"-preset\",\n    \"slow\",\n    \"-crf\",\n    \"16\",\n    \"-pix_fmt\",\n    \"yuv420p\",\n    \"-c:a\",\n    \"aac\",\n    \"-b:a\",\n    \"192k\",\n    \"-movflags\",\n    \"+faststart\",\n    outFile,\n  ];\n  await run(ffmpegPath!, args);\n  await fs.stat(outFile);\n}\n\nexport async function probeBitrate(\n  file: string,\n): Promise<{ size: number; seconds: number; kbps: number }> {\n  const { stdout } = await run(ffprobePath as string, [\n    \"-v\",\n    \"error\",\n    \"-show_entries\",\n    \"format=duration,size\",\n    \"-of\",\n    \"json\",\n    file,\n  ]);\n  const j = JSON.parse(stdout);\n  const size = Number(j.format.size || 0);\n  const seconds = Number(j.format.duration || 0);\n  const kbps = seconds > 0 ? (size * 8) / seconds / 1000 : 0;\n  return { size, seconds, kbps };\n}\n\nexport function getFileSizeBytes(pathStr: string): number {\n  const stat = require(\"fs\").statSync(pathStr);\n  return stat.size;\n}\n\nexport async function getDurationSeconds(inputPath: string): Promise<number> {\n  const { stdout } = await run(ffprobePath as string, [\n    \"-v\",\n    \"error\",\n    \"-show_entries\",\n    \"format=duration\",\n    \"-of\",\n    \"json\",\n    inputPath,\n  ]);\n  const j = JSON.parse(stdout);\n  return Number(j.format.duration || 0);\n}\n\nexport interface SceneChange {\n  timeSec: number;\n}\n\nexport async function detectScenes(\n  inputPath: string,\n  threshold = 0.3,\n): Promise<SceneChange[]> {\n  const args = [\n    \"-i\",\n    inputPath,\n    \"-vf\",\n    `select='gt(scene,${threshold})',showinfo`,\n    \"-f\",\n    \"null\",\n    \"-\",\n  ];\n  const { stderr } = await run(ffmpegPath!, args);\n  const lines = stderr.split(\"\\n\");\n  const changes: SceneChange[] = [];\n  for (const line of lines) {\n    const match = line.match(/pts_time:([\\d.]+)/);\n    if (match) {\n      changes.push({ timeSec: parseFloat(match[1]) });\n    }\n  }\n  return changes;\n}\n\nexport async function extractAudio(\n  inputPath: string,\n  outputPath: string,\n): Promise<void> {\n  await run(ffmpegPath!, [\n    \"-y\",\n    \"-i\",\n    inputPath,\n    \"-vn\",\n    \"-acodec\",\n    \"copy\",\n    outputPath,\n  ]);\n}\n\nexport async function compressAudioForTranscription(\n  inputPath: string,\n  outputPath: string,\n): Promise<void> {\n  await run(ffmpegPath!, [\n    \"-y\",\n    \"-i\",\n    inputPath,\n    \"-ar\",\n    \"16000\",\n    \"-ac\",\n    \"1\",\n    \"-c:a\",\n    \"libmp3lame\",\n    \"-b:a\",\n    \"64k\",\n    outputPath,\n  ]);\n}\n\nexport async function extractThumbnail(\n  videoPath: string,\n  outputPath: string,\n  timeSec: number,\n): Promise<void> {\n  await run(ffmpegPath!, [\n    \"-y\",\n    \"-ss\",\n    String(timeSec),\n    \"-i\",\n    videoPath,\n    \"-vframes\",\n    \"1\",\n    \"-q:v\",\n    \"2\",\n    outputPath,\n  ]);\n}\n\nconst MAX_CUE_DURATION = 4.5;\nconst MAX_CUE_GAP = 0.8;\nconst MAX_TOTAL_CHARACTERS = 84;\nconst MAX_LINE_LENGTH = 42;\nconst MAX_LINES_PER_CUE = 2;\nconst IDEAL_CHAR_BREAK = 28;\nconst SENTENCE_ENDING = /[.!?…]/;\nconst CLAUSE_ENDING = /[,;:\\u2014\\u2013]/;\n\ninterface TimedWord {\n  word: string;\n  start: number;\n  end: number;\n}\n\nexport function createWordByWordSrtFile(\n  words: Array<TimedWord>,\n  outputPath: string,\n): void {\n  const sanitized = words\n    .map((w) => ({ ...w, word: sanitizeWord(w.word) }))\n    .filter((w) => w.word.length > 0);\n  if (sanitized.length === 0) {\n    writeFileSync(outputPath, \"\");\n    return;\n  }\n  let idx = 1;\n  const cues: string[] = [];\n  for (const word of sanitized) {\n    const start = formatSrtTime(word.start);\n    const end = formatSrtTime(word.end);\n    cues.push([String(idx++), `${start} --> ${end}`, word.word].join(\"\\n\"));\n  }\n  writeFileSync(outputPath, cues.join(\"\\n\\n\") + \"\\n\");\n}\n\nexport function createSrtFile(\n  words: Array<TimedWord>,\n  outputPath: string,\n): void {\n  const sanitizedWords = words\n    .map((w) => ({ ...w, word: sanitizeWord(w.word) }))\n    .filter((w) => w.word.length > 0);\n  if (sanitizedWords.length === 0) {\n    writeFileSync(outputPath, \"\");\n    return;\n  }\n  const cues: string[] = [];\n  let currentCue: TimedWord[] = [];\n  let cueStart = sanitizedWords[0].start;\n\n  const flushCue = () => {\n    if (currentCue.length === 0) {\n      return;\n    }\n    const startTime = formatSrtTime(cueStart);\n    const endTime = formatSrtTime(currentCue[currentCue.length - 1].end);\n    const lines = formatCueLines(currentCue);\n    cues.push(\n      [String(cues.length + 1), `${startTime} --> ${endTime}`, ...lines].join(\n        \"\\n\",\n      ),\n    );\n    currentCue = [];\n  };\n\n  sanitizedWords.forEach((word, index) => {\n    const previousWord = currentCue[currentCue.length - 1];\n    if (previousWord) {\n      const gap = word.start - previousWord.end;\n      if (gap >= MAX_CUE_GAP) {\n        flushCue();\n      }\n    }\n    if (currentCue.length === 0) {\n      cueStart = word.start;\n    }\n    const candidateCue = [...currentCue, word];\n    const candidateDuration = word.end - cueStart;\n    const candidateChars = measureCueCharacters(candidateCue);\n    if (\n      currentCue.length > 0 &&\n      (candidateDuration > MAX_CUE_DURATION ||\n        candidateChars > MAX_TOTAL_CHARACTERS)\n    ) {\n      flushCue();\n      cueStart = word.start;\n    }\n    currentCue.push(word);\n    const currentDuration = currentCue[currentCue.length - 1].end - cueStart;\n    const currentChars = measureCueCharacters(currentCue);\n    const nextWord = sanitizedWords[index + 1];\n    const endsSentence = endsWithRegex(word.word, SENTENCE_ENDING);\n    const endsClause = endsWithRegex(word.word, CLAUSE_ENDING);\n    if (endsSentence) {\n      flushCue();\n      return;\n    }\n    if (\n      endsClause &&\n      (currentChars >= IDEAL_CHAR_BREAK ||\n        currentDuration >= MAX_CUE_DURATION / 2)\n    ) {\n      flushCue();\n      return;\n    }\n    if (!nextWord) {\n      flushCue();\n      return;\n    }\n    const gapToNext = nextWord.start - word.end;\n    if (gapToNext >= MAX_CUE_GAP) {\n      flushCue();\n      return;\n    }\n    if (\n      currentDuration >= MAX_CUE_DURATION ||\n      currentChars >= MAX_TOTAL_CHARACTERS\n    ) {\n      flushCue();\n    }\n  });\n\n  flushCue();\n  const srtContent = cues.join(\"\\n\\n\") + \"\\n\";\n  writeFileSync(outputPath, srtContent);\n}\n\nexport function createAssWordByWordFile(\n  words: Array<TimedWord>,\n  outputPath: string,\n  wordsPerSubtitle: number = 1,\n): void {\n  const sanitized = words\n    .map((w) => ({ ...w, word: sanitizeWord(w.word) }))\n    .filter((w) => w.word.length > 0);\n  const header = [\n    \"[Script Info]\",\n    \"ScriptType: v4.00+\",\n    \"PlayResX: 1080\",\n    \"PlayResY: 1920\",\n    \"[V4+ Styles]\",\n    \"Format: Name,Fontname,Fontsize,PrimaryColour,SecondaryColour,OutlineColour,BackColour,Bold,Italic,Underline,StrikeOut,ScaleX,ScaleY,Spacing,Angle,BorderStyle,Outline,Shadow,Alignment,MarginL,MarginR,MarginV,Encoding\",\n    \"Style: Default,Forever Freedom Regular Font Regular,160,&H00FFFFFF,&H00FFFFFF,&H00000000,&H00000000,1,0,0,0,100,100,3,0,1,0,2,5,40,40,0,0\",\n    \"[Events]\",\n    \"Format: Layer,Start,End,Style,Name,MarginL,MarginR,MarginV,Effect,Text\",\n  ];\n  const lines: string[] = [];\n  for (let i = 0; i < sanitized.length; i += wordsPerSubtitle) {\n    const group = sanitized.slice(i, i + wordsPerSubtitle);\n    if (group.length === 0) {\n      continue;\n    }\n    const start = formatAssTime(group[0].start);\n    const end = formatAssTime(group[group.length - 1].end);\n    const text = group\n      .map((w) => w.word)\n      .join(\" \")\n      .replace(/{/g, \"｛\")\n      .replace(/}/g, \"｝\");\n    lines.push(`Dialogue: 0,${start},${end},Default,,0,0,0,,${text}`);\n  }\n  const content = header.concat(lines).join(\"\\n\") + \"\\n\";\n  writeFileSync(outputPath, content);\n}\n\nfunction sanitizeWord(word: string): string {\n  return word.replace(/\\s+/g, \" \").trim();\n}\n\nfunction measureCueCharacters(words: TimedWord[]): number {\n  return buildCueText(words).length;\n}\n\nfunction buildCueText(words: TimedWord[]): string {\n  const joined = words.map((w) => w.word).join(\" \");\n  return joined\n    .replace(/\\s+([,.;!?…:\\u2014\\u2013])/g, \"$1\")\n    .replace(/\\s+/g, \" \")\n    .trim();\n}\n\nfunction formatCueLines(words: TimedWord[]): string[] {\n  const text = buildCueText(words);\n  if (text.length === 0) {\n    return [\"\"];\n  }\n  let lines = wrapText(text, MAX_LINE_LENGTH);\n  if (lines.length > MAX_LINES_PER_CUE) {\n    lines = rebalanceLines(words);\n  }\n  return lines;\n}\n\nfunction wrapText(text: string, maxLen: number): string[] {\n  const tokens = text.split(/\\s+/).filter(Boolean);\n  const lines: string[] = [];\n  let current = \"\";\n  for (const token of tokens) {\n    const candidate = current.length > 0 ? `${current} ${token}` : token;\n    if (candidate.length <= maxLen || current.length === 0) {\n      current = candidate;\n    } else {\n      lines.push(current);\n      current = token;\n    }\n  }\n  if (current.length > 0) {\n    lines.push(current);\n  }\n  return lines;\n}\n\nfunction rebalanceLines(words: TimedWord[]): string[] {\n  const tokens = buildCueText(words).split(/\\s+/).filter(Boolean);\n  if (tokens.length === 0) {\n    return [\"\"];\n  }\n  let bestSplit = Math.ceil(tokens.length / 2);\n  let bestLines = [\n    tokens.slice(0, bestSplit).join(\" \"),\n    tokens.slice(bestSplit).join(\" \"),\n  ];\n  let bestScore = Math.max(...bestLines.map((l) => l.length));\n  for (let i = 1; i < tokens.length; i++) {\n    const left = tokens.slice(0, i).join(\" \");\n    const right = tokens.slice(i).join(\" \");\n    const leftLength = left.length;\n    const rightLength = right.length;\n    const score = Math.max(leftLength, rightLength);\n    if (\n      leftLength <= MAX_LINE_LENGTH &&\n      rightLength <= MAX_LINE_LENGTH &&\n      score < bestScore\n    ) {\n      bestLines = [left, right];\n      bestScore = score;\n    }\n  }\n  if (\n    bestLines[0].length <= MAX_LINE_LENGTH &&\n    bestLines[1].length <= MAX_LINE_LENGTH\n  ) {\n    return bestLines;\n  }\n  const wrapped = wrapText(tokens.join(\" \"), MAX_LINE_LENGTH);\n  if (wrapped.length <= MAX_LINES_PER_CUE) {\n    return wrapped;\n  }\n  const firstLines = wrapped.slice(0, MAX_LINES_PER_CUE - 1);\n  const remaining = wrapped.slice(MAX_LINES_PER_CUE - 1).join(\" \");\n  return [...firstLines, remaining.trim()].filter((line) => line.length > 0);\n}\n\nfunction endsWithRegex(text: string, regex: RegExp): boolean {\n  return regex.test(text.slice(-1));\n}\n\nfunction formatSrtTime(seconds: number): string {\n  const hours = Math.floor(seconds / 3600);\n  const minutes = Math.floor((seconds % 3600) / 60);\n  const secs = Math.floor(seconds % 60);\n  const ms = Math.floor((seconds % 1) * 1000);\n  return `${pad(hours, 2)}:${pad(minutes, 2)}:${pad(secs, 2)},${pad(ms, 3)}`;\n}\n\nfunction formatAssTime(seconds: number): string {\n  const hours = Math.floor(seconds / 3600);\n  const minutes = Math.floor((seconds % 3600) / 60);\n  const secs = Math.floor(seconds % 60);\n  const cs = Math.floor(((seconds % 1) * 1000) / 10);\n  return `${pad(hours, 1)}:${pad(minutes, 2)}:${pad(secs, 2)}.${pad(cs, 2)}`;\n}\n\nfunction pad(num: number, size: number): string {\n  let s = num.toString();\n  while (s.length < size) {\n    s = \"0\" + s;\n  }\n  return s;\n}\n\ninterface RenderVerticalClipOptions {\n  inputPath: string;\n  outputPath: string;\n  startTime: number;\n  duration: number;\n  srtPath?: string;\n  hookText?: string;\n}\n\nexport async function renderVerticalClip(\n  options: RenderVerticalClipOptions,\n): Promise<void> {\n  const p = await probeVideo(options.inputPath);\n  const tgt = chooseTargetSize(p.width, p.height);\n  const filters = buildFilters(tgt.w, tgt.h);\n  let vf = filters;\n  if (options.srtPath && options.srtPath.length > 0) {\n    vf = vf + \",\" + subtitlesFilter(options.srtPath);\n  }\n  if (options.hookText && options.hookText.length > 0) {\n    const hookEscaped = escapeDrawtext(options.hookText);\n    const fontPath = path.resolve(\n      process.cwd(),\n      \"assets/fonts/Forever-Freedom-Regular.ttf\",\n    );\n    const escapedFontPath = fontPath\n      .replace(/\\\\/g, \"\\\\\\\\\")\n      .replace(/:/g, \"\\\\:\");\n    const hookFilter =\n      \"drawtext=text='\" +\n      hookEscaped +\n      \"':fontfile='\" +\n      escapedFontPath +\n      \"':fontsize=30:fontcolor=white:borderw=0:bordercolor=black:x=(w-text_w)/2:y=120\";\n    vf = vf + \",\" + hookFilter;\n  }\n  const dur = Math.max(0, options.duration);\n  const args = [\n    \"-y\",\n    \"-ss\",\n    String(options.startTime),\n    \"-t\",\n    String(dur),\n    \"-i\",\n    options.inputPath,\n    \"-vf\",\n    vf,\n    \"-c:v\",\n    \"libx264\",\n    \"-profile:v\",\n    \"high\",\n    \"-preset\",\n    \"slow\",\n    \"-crf\",\n    \"16\",\n    \"-pix_fmt\",\n    \"yuv420p\",\n    \"-c:a\",\n    \"aac\",\n    \"-b:a\",\n    \"192k\",\n    \"-movflags\",\n    \"+faststart\",\n    options.outputPath,\n  ];\n  await run(ffmpegPath!, args);\n  await fs.stat(options.outputPath);\n}\n\ninterface RenderSmartFramedClipOptions {\n  inputPath: string;\n  outputPath: string;\n  startTime: number;\n  duration: number;\n  srtPath: string;\n  hookText?: string;\n  filterExpr?: string;\n  cropMapExprX?: string;\n  cropMapExprY?: string;\n  cropW?: number;\n  cropH?: number;\n}\n\n//gpt\nexport async function renderSmartFramedClip(\n  options: RenderSmartFramedClipOptions,\n): Promise<void> {\n  let vfString: string;\n\n  if (options.filterExpr) {\n    vfString = options.filterExpr;\n    if (options.srtPath && options.srtPath.length > 0) {\n      vfString = vfString + \",\" + subtitlesFilter(options.srtPath);\n    }\n    if (options.hookText && options.hookText.length > 0) {\n      const hookEscaped = escapeDrawtext(options.hookText);\n      const fontPath = path.resolve(\n        process.cwd(),\n        \"assets/fonts/Forever-Freedom-Regular.ttf\",\n      );\n      const escapedFontPath = fontPath\n        .replace(/\\\\/g, \"\\\\\\\\\")\n        .replace(/:/g, \"\\\\:\");\n      const hookFilter =\n        \"drawtext=text='\" +\n        hookEscaped +\n        \"':fontfile='\" +\n        escapedFontPath +\n        \"':fontsize=30:fontcolor=white:borderw=0:bordercolor=black:x=(w-text_w)/2:y=120\";\n      vfString = vfString + \",\" + hookFilter;\n    }\n  } else {\n    // 👇 add these three lines at the START of the else-branch\n    const p = await probeVideo(options.inputPath);\n    const CROP_W = even(Math.min(options.cropW ?? 1080, p.width));\n    const CROP_H = even(Math.min(options.cropH ?? 1080, p.height));\n\n    // user provides expressions for x(t) / y(t); keep them unquoted inside clamp\n    const xExprRaw = options.cropMapExprX ?? \"0\";\n    const yExprRaw = options.cropMapExprY ?? \"0\";\n\n    // clamp to video bounds and snap to even pixels\n    const xExpr = clampExpr(xExprRaw, `(iw-${CROP_W})`);\n    const yExpr = clampExpr(yExprRaw, `(ih-${CROP_H})`);\n\n    // IMPORTANT: animate ONLY x/y; w/h are constants\n    const cropFilter = `crop=${CROP_W}:${CROP_H}:${xExpr}:${yExpr}`;\n\n    vfString = `${cropFilter},scale=1080:1920,format=yuv420p`;\n\n    if (options.srtPath && options.srtPath.length > 0) {\n      vfString = vfString + \",\" + subtitlesFilter(options.srtPath);\n    }\n    if (options.hookText && options.hookText.length > 0) {\n      const hookEscaped = escapeDrawtext(options.hookText);\n      const fontPath = path.resolve(process.cwd(), \"assets/fonts/Forever-Freedom-Regular.ttf\");\n      const escapedFontPath = fontPath.replace(/\\\\/g, \"\\\\\\\\\").replace(/:/g, \"\\\\:\");\n      const hookFilter =\n        \"drawtext=text='\" + hookEscaped +\n        \"':fontfile='\" + escapedFontPath +\n        \"':fontsize=30:fontcolor=white:borderw=0:bordercolor=black:x=(w-text_w)/2:y=120\";\n      vfString = vfString + \",\" + hookFilter;\n    }\n  }\n\n\n  const dur = Math.max(0, options.duration);\n  const useScript = vfString.length > 60000;\n  const vfArgs = useScript\n    ? [\"-filter_script:v\", await writeFilterScript(vfString)]\n    : [\"-vf\", vfString];\n\n  const args = [\n    \"-y\",\n    \"-ss\",\n    String(options.startTime),\n    \"-t\",\n    String(dur),\n    \"-i\",\n    options.inputPath,\n    ...vfArgs,\n    \"-c:v\",\n    \"libx264\",\n    \"-profile:v\",\n    \"high\",\n    \"-preset\",\n    \"slow\",\n    \"-crf\",\n    \"16\",\n    \"-pix_fmt\",\n    \"yuv420p\",\n    \"-c:a\",\n    \"aac\",\n    \"-b:a\",\n    \"192k\",\n    \"-movflags\",\n    \"+faststart\",\n    options.outputPath,\n  ];\n\n  await run(ffmpegPath!, args);\n  await fs.stat(options.outputPath);\n}\n\n/*export async function renderSmartFramedClip(\n  options: RenderSmartFramedClipOptions,\n): Promise<void> {\n  let vf: string;\n  if (options.filterExpr) {\n    vf = options.filterExpr;\n    if (options.srtPath && options.srtPath.length > 0) {\n      vf = vf + \",\" + subtitlesFilter(options.srtPath);\n    }\n    if (options.hookText && options.hookText.length > 0) {\n      const hookEscaped = escapeDrawtext(options.hookText);\n      const fontPath = path.resolve(\n        process.cwd(),\n        \"assets/fonts/Forever-Freedom-Regular.ttf\",\n      );\n      const escapedFontPath = fontPath\n        .replace(/\\\\/g, \"\\\\\\\\\")\n        .replace(/:/g, \"\\\\:\");\n      const hookFilter =\n        \"drawtext=text='\" +\n        hookEscaped +\n        \"':fontfile='\" +\n        escapedFontPath +\n        \"':fontsize=30:fontcolor=white:borderw=0:bordercolor=black:x=(w-text_w)/2:y=120\";\n      vf = vf + \",\" + hookFilter;\n    }\n  } else {\n    const cropFilter = `crop=${options.cropW}:${options.cropH}:'${options.cropMapExprX}':'${options.cropMapExprY}'`;\n    vf = cropFilter + \",scale=1080:1920,format=yuv420p\";\n    if (options.srtPath && options.srtPath.length > 0) {\n      vf = vf + \",\" + subtitlesFilter(options.srtPath);\n    }\n    if (options.hookText && options.hookText.length > 0) {\n      const hookEscaped = escapeDrawtext(options.hookText);\n      const fontPath = path.resolve(\n        process.cwd(),\n        \"assets/fonts/Forever-Freedom-Regular.ttf\",\n      );\n      const escapedFontPath = fontPath\n        .replace(/\\\\/g, \"\\\\\\\\\")\n        .replace(/:/g, \"\\\\:\");\n      const hookFilter =\n        \"drawtext=text='\" +\n        hookEscaped +\n        \"':fontfile='\" +\n        escapedFontPath +\n        \"':fontsize=30:fontcolor=white:borderw=0:bordercolor=black:x=(w-text_w)/2:y=120\";\n      vf = vf + \",\" + hookFilter;\n    }\n  }\n  const dur = Math.max(0, options.duration);\n  const args = [\n    \"-y\",\n    \"-ss\",\n    String(options.startTime),\n    \"-t\",\n    String(dur),\n    \"-i\",\n    options.inputPath,\n    \"-vf\",\n    vf,\n    \"-c:v\",\n    \"libx264\",\n    \"-profile:v\",\n    \"high\",\n    \"-preset\",\n    \"slow\",\n    \"-crf\",\n    \"16\",\n    \"-pix_fmt\",\n    \"yuv420p\",\n    \"-c:a\",\n    \"aac\",\n    \"-b:a\",\n    \"192k\",\n    \"-movflags\",\n    \"+faststart\",\n    options.outputPath,\n  ];\n  await run(ffmpegPath!, args);\n  await fs.stat(options.outputPath);\n}*/\n","size_bytes":22768},"prisma/migrations/migration_lock.toml":{"content":"# Please do not edit this file manually\n# It should be added in your version-control system (i.e. Git)\nprovider = \"postgresql\"","size_bytes":126},"app/api/auth/status/route.ts":{"content":"import { NextResponse } from 'next/server'\nimport { getSession } from '@/src/lib/session'\nimport { checkYouTubeConnection } from '@/src/lib/youtube-client'\n\nexport async function GET() {\n  try {\n    const session = await getSession()\n    const youtubeConnected = await checkYouTubeConnection()\n    \n    return NextResponse.json({\n      isAuthenticated: session.isAuthenticated || false,\n      youtubeConnected,\n      email: session.email || null\n    })\n  }\n  catch (error) {\n    return NextResponse.json({\n      isAuthenticated: false,\n      youtubeConnected: false,\n      email: null\n    })\n  }\n}\n","size_bytes":598},"types/ffprobe-static.d.ts":{"content":"declare module 'ffprobe-static' {\n  const ffprobeStatic: { path: string }\n  export default ffprobeStatic\n}\n","size_bytes":107},"app/api/auth/google/callback/route.ts":{"content":"import { NextRequest, NextResponse } from \"next/server\";\nimport { OAuth2Client } from \"google-auth-library\";\nimport { prisma } from \"@/src/lib/prisma\";\nimport { getSession } from \"@/src/lib/session\";\nimport { encrypt } from \"@/src/lib/encryption\";\n\nconst getOAuthClient = () => {\n  const clientId = process.env.GOOGLE_CLIENT_ID;\n  const clientSecret = process.env.GOOGLE_CLIENT_SECRET;\n\n  let redirectUri = process.env.GOOGLE_REDIRECT_URI;\n\n  if (!clientId || !clientSecret) {\n    throw new Error(\"Google OAuth credentials not configured\");\n  }\n\n  console.log(\"Redirect URI:\", redirectUri);\n  console.log(\"Client ID:\", clientId);\n  console.log(\"Client Secret:\", clientSecret);\n  return new OAuth2Client(clientId, clientSecret, redirectUri);\n};\n\nexport async function GET(request: NextRequest) {\n  try {\n    const searchParams = request.nextUrl.searchParams;\n    const code = searchParams.get(\"code\");\n    const error = searchParams.get(\"error\");\n\n    if (error) {\n      return NextResponse.redirect(\n        new URL(`/login?error=${error}`, request.url),\n      );\n    }\n\n    if (!code) {\n      return NextResponse.redirect(\n        new URL(\"/login?error=no_code\", request.url),\n      );\n    }\n\n    const oauth2Client = getOAuthClient();\n    const { tokens } = await oauth2Client.getToken(code);\n\n    if (!tokens.access_token) {\n      return NextResponse.redirect(\n        new URL(\"/login?error=no_token\", request.url),\n      );\n    }\n\n    oauth2Client.setCredentials(tokens);\n\n    const oauth2 = await oauth2Client.request({\n      url: \"https://www.googleapis.com/oauth2/v2/userinfo\",\n    });\n\n    const userInfo = oauth2.data as any;\n\n    if (!userInfo.email || !userInfo.id) {\n      return NextResponse.redirect(\n        new URL(\"/login?error=no_user_info\", request.url),\n      );\n    }\n\n    const encryptedAccessToken = tokens.access_token\n      ? encrypt(tokens.access_token)\n      : null;\n    const encryptedRefreshToken = tokens.refresh_token\n      ? encrypt(tokens.refresh_token)\n      : null;\n    const expiresAt = tokens.expiry_date ? new Date(tokens.expiry_date) : null;\n\n    let user = await prisma.user.findUnique({\n      where: { googleAccountId: userInfo.id },\n    });\n\n    if (user) {\n      user = await prisma.user.update({\n        where: { id: user.id },\n        data: {\n          email: userInfo.email,\n          googleAccessToken: encryptedAccessToken,\n          googleRefreshToken: encryptedRefreshToken,\n          googleTokenExpiresAt: expiresAt,\n        },\n      });\n    } else {\n      user = await prisma.user.create({\n        data: {\n          email: userInfo.email,\n          googleAccountId: userInfo.id,\n          googleAccessToken: encryptedAccessToken,\n          googleRefreshToken: encryptedRefreshToken,\n          googleTokenExpiresAt: expiresAt,\n        },\n      });\n    }\n\n    const session = await getSession();\n    session.isAuthenticated = true;\n    session.userId = user.id;\n    session.email = user.email;\n    await session.save();\n\n    const replitDomain = process.env.REPLIT_DOMAINS?.split(\",\")[0];\n    const baseUrl = replitDomain ? `https://${replitDomain}` : request.url;\n\n    return NextResponse.redirect(new URL(\"/\", baseUrl));\n  } catch (error: any) {\n    console.error(\"Error in Google OAuth callback:\", error);\n\n    const replitDomain = process.env.REPLIT_DOMAINS?.split(\",\")[0];\n    const baseUrl = replitDomain ? `https://${replitDomain}` : request.url;\n\n    return NextResponse.redirect(\n      new URL(\"/login?error=callback_failed\", baseUrl),\n    );\n  }\n}\n","size_bytes":3508},"src/lib/prisma.ts":{"content":"import { PrismaClient } from '@prisma/client'\n\nconst globalForPrisma = globalThis as unknown as {\n  prisma: PrismaClient | undefined\n}\n\nexport const prisma = globalForPrisma.prisma ?? new PrismaClient()\n\nif (process.env.NODE_ENV !== 'production') {\n  globalForPrisma.prisma = prisma\n}\n","size_bytes":285},"app/setup-cookies/page.tsx":{"content":"'use client'\n\nimport { useEffect } from 'react'\nimport { useRouter } from 'next/navigation'\n\nexport default function SetupCookiesPage() {\n  const router = useRouter()\n\n  useEffect(() => {\n    checkAuthAndRedirect()\n  }, [])\n\n  async function checkAuthAndRedirect() {\n    try {\n      const res = await fetch('/api/auth/status')\n      const data = await res.json()\n      \n      if (!data.isAuthenticated)\n      {\n        router.push('/login')\n        return\n      }\n\n      router.push('/')\n    }\n    catch (err) {\n      console.error('Failed to check auth:', err)\n      router.push('/login')\n    }\n  }\n\n  return (\n    <div className=\"min-h-screen bg-gradient-to-br from-gray-900 via-gray-800 to-gray-900 flex items-center justify-center p-4\">\n      <div className=\"max-w-md w-full bg-gray-800 rounded-lg shadow-2xl p-8\">\n        <div className=\"text-center\">\n          <div className=\"inline-flex items-center justify-center w-16 h-16 bg-blue-500 bg-opacity-20 rounded-full mb-4\">\n            <svg className=\"w-8 h-8 text-blue-400 animate-spin\" fill=\"none\" viewBox=\"0 0 24 24\">\n              <circle className=\"opacity-25\" cx=\"12\" cy=\"12\" r=\"10\" stroke=\"currentColor\" strokeWidth=\"4\"></circle>\n              <path className=\"opacity-75\" fill=\"currentColor\" d=\"M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z\"></path>\n            </svg>\n          </div>\n          <h2 className=\"text-2xl font-bold text-white mb-2\">\n            YouTube OAuth Connected\n          </h2>\n          <p className=\"text-gray-400\">\n            Redirecting to dashboard...\n          </p>\n        </div>\n      </div>\n    </div>\n  )\n}\n","size_bytes":1669},"app/layout.tsx":{"content":"import type { Metadata } from \"next\";\nimport \"./globals.css\";\n\nexport const metadata: Metadata = {\n  title: \"YT Shortsmith\",\n  description: \"AI-powered short-form video clip generator\",\n};\n\nimport Providers from \"./providers\";\n\nexport default function RootLayout({\n  children,\n}: {\n  children: React.ReactNode;\n}) {\n  return (\n    <html lang=\"en\">\n      <body>\n        <Providers>{children}</Providers>\n      </body>\n    </html>\n  );\n}\n","size_bytes":436},"src/lib/youtube-client.ts":{"content":"import { google } from 'googleapis'\n\nlet connectionSettings: any\n\nasync function getAccessToken() {\n  if (connectionSettings && connectionSettings.settings.expires_at && new Date(connectionSettings.settings.expires_at).getTime() > Date.now())\n  {\n    return connectionSettings.settings.access_token\n  }\n  \n  const hostname = process.env.REPLIT_CONNECTORS_HOSTNAME\n  const xReplitToken = process.env.REPL_IDENTITY \n    ? 'repl ' + process.env.REPL_IDENTITY \n    : process.env.WEB_REPL_RENEWAL \n    ? 'depl ' + process.env.WEB_REPL_RENEWAL \n    : null\n\n  if (!xReplitToken)\n  {\n    throw new Error('X_REPLIT_TOKEN not found for repl/depl')\n  }\n\n  connectionSettings = await fetch(\n    'https://' + hostname + '/api/v2/connection?include_secrets=true&connector_names=youtube',\n    {\n      headers: {\n        'Accept': 'application/json',\n        'X_REPLIT_TOKEN': xReplitToken\n      }\n    }\n  ).then(res => res.json()).then(data => data.items?.[0])\n\n  const accessToken = connectionSettings?.settings?.access_token || connectionSettings.settings?.oauth?.credentials?.access_token\n\n  if (!connectionSettings || !accessToken)\n  {\n    throw new Error('YouTube not connected')\n  }\n  return accessToken\n}\n\nexport async function getYouTubeClient() {\n  const accessToken = await getAccessToken()\n  return google.youtube({ version: 'v3', auth: accessToken })\n}\n\nexport async function checkYouTubeConnection(): Promise<boolean> {\n  try {\n    await getAccessToken()\n    return true\n  }\n  catch {\n    return false\n  }\n}\n","size_bytes":1506},"app/api/videos/[id]/route.ts":{"content":"import { NextRequest, NextResponse } from \"next/server\";\nimport { prisma } from \"@/src/lib/prisma\";\nimport { getSignedUrlForKey, getS3Url } from \"@/src/services/s3\";\nimport { requireAuth } from \"@/src/lib/session\";\nimport { videoQueue } from \"@/src/lib/queue\";\n\nasync function getUrlForKey(key: string): Promise<string> {\n  try {\n    return await getSignedUrlForKey(key, 7200);\n  } catch (error: any) {\n    if (error.name === \"CredentialsProviderError\") {\n      console.warn(\"S3 credentials not configured, using direct URLs\");\n    } else {\n      console.warn(\n        \"Failed to generate signed URL, falling back to direct URL:\",\n        error,\n      );\n    }\n    return getS3Url(key);\n  }\n}\n\nexport async function GET(\n  request: NextRequest,\n  { params }: { params: { id: string } },\n) {\n  try {\n    await requireAuth();\n\n    const video = await prisma.video.findUnique({\n      where: { id: params.id },\n      include: {\n        clips: {\n          orderBy: { scoreOverall: \"desc\" },\n        },\n      },\n    });\n\n    if (!video) {\n      return NextResponse.json({ error: \"Video not found\" }, { status: 404 });\n    }\n\n    const clipsWithUrls = await Promise.all(\n      video.clips.map(async (clip) => ({\n        ...clip,\n        videoUrl: await getUrlForKey(clip.s3VideoKey),\n        thumbUrl: await getUrlForKey(clip.s3ThumbKey),\n        srtUrl: await getUrlForKey(clip.s3SrtKey),\n      })),\n    );\n\n    return NextResponse.json({\n      ...video,\n      clips: clipsWithUrls,\n    });\n  } catch (error: any) {\n    console.error(\"Error fetching video:\", error);\n\n    if (error.message === \"Authentication required\") {\n      return NextResponse.json(\n        { error: \"Authentication required\" },\n        { status: 401 },\n      );\n    }\n\n    return NextResponse.json(\n      { error: \"Failed to fetch video\" },\n      { status: 500 },\n    );\n  }\n}\n\nexport async function DELETE(\n  request: NextRequest,\n  { params }: { params: { id: string } },\n) {\n  try {\n    const session = await requireAuth();\n    const { id } = params;\n\n    const video = await prisma.video.findUnique({\n      where: { id, userId: session.userId },\n    });\n\n    if (!video) {\n      return NextResponse.json({ error: \"Video not found\" }, { status: 404 });\n    }\n\n    const job = await videoQueue.getJob(id);\n    if (job) {\n      await job.remove();\n    }\n\n    await prisma.$transaction([\n      prisma.clip.deleteMany({ where: { videoId: id } }),\n      prisma.video.delete({ where: { id } }),\n    ]);\n\n    return NextResponse.json({ success: true });\n  } catch (error) {\n    console.error(\"Error deleting video:\", error);\n    return NextResponse.json(\n      { error: \"Failed to delete video\" },\n      { status: 500 },\n    );\n  }\n}\n","size_bytes":2697},"tailwind.config.ts":{"content":"import type { Config } from 'tailwindcss'\n\nconst config: Config = {\n  content: [\n    './pages/**/*.{js,ts,jsx,tsx,mdx}',\n    './components/**/*.{js,ts,jsx,tsx,mdx}',\n    './app/**/*.{js,ts,jsx,tsx,mdx}',\n  ],\n  theme: {\n    extend: {},\n  },\n  plugins: [],\n}\nexport default config\n","size_bytes":280},"app/api/youtube/cookies/route.ts":{"content":"import { NextRequest, NextResponse } from 'next/server'\nimport { requireAuth } from '@/src/lib/session'\nimport { prisma } from '@/src/lib/prisma'\nimport { encrypt } from '@/src/lib/encryption'\n\nfunction validateCookies(cookiesText: string): boolean {\n  const lines = cookiesText.split('\\n').filter(line => line.trim() && !line.startsWith('#'))\n  \n  if (lines.length === 0)\n  {\n    return false\n  }\n  \n  const requiredCookies = ['SAPISID', 'HSID', 'SSID']\n  const foundCookies = new Set<string>()\n  \n  for (const line of lines)\n  {\n    const parts = line.split('\\t')\n    \n    if (parts.length < 7)\n    {\n      continue\n    }\n    \n    const cookieName = parts[5]\n    \n    if (requiredCookies.includes(cookieName))\n    {\n      foundCookies.add(cookieName)\n    }\n  }\n  \n  return requiredCookies.every(cookie => foundCookies.has(cookie))\n}\n\nexport async function POST(request: NextRequest) {\n  try {\n    const session = await requireAuth()\n    \n    if (!session.userId)\n    {\n      return NextResponse.json(\n        { error: 'User ID not found in session' },\n        { status: 401 }\n      )\n    }\n    \n    const contentType = request.headers.get('content-type') || ''\n    let cookiesText: string\n    \n    if (contentType.includes('multipart/form-data'))\n    {\n      const formData = await request.formData()\n      const file = formData.get('file') as File\n      \n      if (!file)\n      {\n        return NextResponse.json(\n          { error: 'No file provided' },\n          { status: 400 }\n        )\n      }\n      \n      cookiesText = await file.text()\n    }\n    else\n    {\n      const body = await request.json()\n      cookiesText = body.cookies\n      \n      if (!cookiesText)\n      {\n        return NextResponse.json(\n          { error: 'No cookies provided' },\n          { status: 400 }\n        )\n      }\n    }\n    \n    if (!validateCookies(cookiesText))\n    {\n      return NextResponse.json(\n        { error: 'Invalid cookies format or missing required cookies (SAPISID, HSID, SSID)' },\n        { status: 400 }\n      )\n    }\n    \n    const user = await prisma.user.findUnique({\n      where: { id: session.userId }\n    })\n    \n    if (!user)\n    {\n      return NextResponse.json(\n        { error: 'User not found. Please sign in again.' },\n        { status: 404 }\n      )\n    }\n    \n    const encryptedCookies = encrypt(cookiesText)\n    \n    await prisma.user.update({\n      where: { id: session.userId },\n      data: {\n        youtubeCookies: encryptedCookies,\n        youtubeCookiesCreatedAt: new Date(),\n        youtubeCookiesLastUsedAt: new Date()\n      }\n    })\n    \n    return NextResponse.json({ success: true })\n  }\n  catch (error: any) {\n    console.error('Error saving cookies:', error)\n    \n    if (error.message === 'Authentication required')\n    {\n      return NextResponse.json(\n        { error: 'Authentication required' },\n        { status: 401 }\n      )\n    }\n    \n    return NextResponse.json(\n      { error: 'Failed to save cookies' },\n      { status: 500 }\n    )\n  }\n}\n","size_bytes":2986},"app/globals.css":{"content":"@tailwind base;\n@tailwind components;\n@tailwind utilities;\n\nbody {\n  @apply bg-gray-900 text-white;\n}\n","size_bytes":102},"app/api/me/connections/route.ts":{"content":"import { NextResponse } from \"next/server\";\nimport { requireAuth } from \"@/src/lib/session\";\nimport { prisma } from \"@/src/lib/prisma\";\nimport { isYouTubeConnected } from \"@/src/lib/youtube-oauth\";\nimport { getCookieAge } from \"@/src/services/cookieGenerator\";\n\nexport async function GET() {\n  try {\n    const session = await requireAuth();\n    if (!session.userId) {\n      return NextResponse.json(\n        { error: \"User ID not found in session\" },\n        { status: 401 },\n      );\n    }\n\n    const user = await prisma.user.findUnique({\n      where: { id: session.userId },\n      select: {\n        googleAccountId: true,\n        googleTokenExpiresAt: true,\n      },\n    });\n    if (!user) {\n      return NextResponse.json({ error: \"User not found\" }, { status: 404 });\n    }\n\n    const tiktok = await prisma.tikTokConnection.findFirst({\n      where: { userId: session.userId },\n      select: { updatedAt: true },\n    });\n\n    const hasYouTube = await isYouTubeConnected();\n    const cookieAge = await getCookieAge(session.userId);\n\n    return NextResponse.json({\n      hasOAuth: !!user.googleAccountId,\n      oauthExpired: user.googleTokenExpiresAt\n        ? user.googleTokenExpiresAt < new Date()\n        : true,\n      hasYouTube,\n      hasCookies: cookieAge !== null,\n      cookieAgeDays: cookieAge !== null ? Math.floor(cookieAge) : null,\n      hasTikTok: !!tiktok,\n      tiktokConnectedAt: tiktok?.updatedAt || null,\n    });\n  } catch (error: any) {\n    if (error.message === \"Authentication required\") {\n      return NextResponse.json(\n        { error: \"Authentication required\" },\n        { status: 401 },\n      );\n    }\n    return NextResponse.json(\n      { error: \"Failed to fetch connections\" },\n      { status: 500 },\n    );\n  }\n}\n","size_bytes":1744},"app/videos/[id]/page.tsx":{"content":"\"use client\";\n\nimport { useEffect, useMemo, useState } from \"react\";\nimport Link from \"next/link\";\nimport { useParams, useRouter } from \"next/navigation\";\nimport {\n  AppBar,\n  Toolbar,\n  Typography,\n  Box,\n  Container,\n  Paper,\n  Stack,\n  Chip,\n  Button,\n  Select,\n  MenuItem,\n  Slider,\n  Alert,\n  Card,\n  CardContent,\n  CardMedia,\n  IconButton,\n  Tooltip,\n  CircularProgress,\n  Menu,\n  ListItemIcon,\n  ListItemText,\n  Divider,\n  LinearProgress,\n  TextField,\n  InputAdornment,\n  ToggleButton,\n  ToggleButtonGroup,\n  Badge,\n  Tabs,\n  Tab,\n  FormControl,\n  InputLabel,\n} from \"@mui/material\";\nimport ArrowBackIcon from \"@mui/icons-material/ArrowBack\";\nimport MoreVertIcon from \"@mui/icons-material/MoreVert\";\nimport SubtitlesIcon from \"@mui/icons-material/Subtitles\";\nimport CenterFocusStrongIcon from \"@mui/icons-material/CenterFocusStrong\";\nimport DeleteOutlineIcon from \"@mui/icons-material/DeleteOutline\";\nimport MovieIcon from \"@mui/icons-material/Movie\";\nimport DescriptionIcon from \"@mui/icons-material/Description\";\nimport SearchIcon from \"@mui/icons-material/Search\";\nimport SpeedIcon from \"@mui/icons-material/Speed\";\nimport QueryStatsIcon from \"@mui/icons-material/QueryStats\";\nimport AccessTimeIcon from \"@mui/icons-material/AccessTime\";\n\ninterface Clip {\n  id: string;\n  startSec: number;\n  endSec: number;\n  durationSec: number;\n  category: string;\n  tags: string[];\n  scoreHook: number;\n  scoreRetention: number;\n  scoreClarity: number;\n  scoreShare: number;\n  scoreOverall: number;\n  rationale: string;\n  videoUrl: string;\n  thumbUrl: string;\n  srtUrl: string;\n  tiktokStatus?: string | null;\n  tiktokPublishId?: string | null;\n}\n\ninterface Video {\n  id: string;\n  title: string;\n  sourceUrl: string;\n  status: string;\n  durationSec: number;\n  clips: Clip[];\n}\n\ntype TikTokState = { state: string; publishId?: string };\ntype SortKey = \"score\" | \"duration\" | \"start\";\ntype ScorePreset = \"any\" | \"60\" | \"70\" | \"80\" | \"90\";\n\nexport default function VideoDetail() {\n  const params = useParams();\n  const router = useRouter();\n  const [video, setVideo] = useState<Video | null>(null);\n  const [loading, setLoading] = useState(true);\n\n  const [selectedCategory, setSelectedCategory] = useState<string>(\"all\");\n  const [scoreRange, setScoreRange] = useState<number[]>([0, 100]);\n  const [scorePreset, setScorePreset] = useState<ScorePreset>(\"any\");\n  const [sending, setSending] = useState<Record<string, boolean>>({});\n  const [deleting, setDeleting] = useState<Record<string, boolean>>({});\n  const [updating, setUpdating] = useState<Record<string, boolean>>({});\n  const [centering, setCentering] = useState<Record<string, boolean>>({});\n  const [ttState, setTtState] = useState<Record<string, TikTokState>>({});\n  const [error, setError] = useState<string | null>(null);\n  const [actionMenu, setActionMenu] = useState<\n    Record<string, HTMLElement | null>\n  >({});\n  const [query, setQuery] = useState(\"\");\n  const [sortKey, setSortKey] = useState<SortKey>(\"score\");\n  const [sortDir, setSortDir] = useState<\"asc\" | \"desc\">(\"desc\");\n  const [tabCategory, setTabCategory] = useState(0);\n  const [wordsPerSubtitle, setWordsPerSubtitle] = useState<\n    Record<string, number>\n  >({});\n\n  const clips = video?.clips ?? [];\n\n  const categories = useMemo(() => {\n    return [\"all\", ...Array.from(new Set(clips.map((c) => c.category)))];\n  }, [clips]);\n\n  const isProcessing = useMemo(() => {\n    const a = Object.values(updating).some(Boolean);\n    const b = Object.values(centering).some(Boolean);\n    if (a || b) {\n      return true;\n    }\n    return false;\n  }, [updating, centering]);\n\n  const processingCount = useMemo(() => {\n    let count = 0;\n    clips.forEach((c) => {\n      if (updating[c.id] || centering[c.id]) {\n        count += 1;\n      }\n    });\n    return count;\n  }, [clips, updating, centering]);\n\n  useEffect(() => {\n    const idParam = (params as any)?.id;\n    const id = Array.isArray(idParam) ? idParam[0] : idParam;\n    if (id) {\n      fetchVideo(id);\n    }\n  }, [params]);\n\n  useEffect(() => {\n    if (!video) {\n      return;\n    }\n    if (video.status !== \"processing\") {\n      return;\n    }\n    const handleBeforeUnload = async (e: BeforeUnloadEvent) => {\n      e.preventDefault();\n      try {\n        await fetch(`/api/videos/${video.id}/cancel`, {\n          method: \"POST\",\n          keepalive: true,\n        });\n      } catch {}\n    };\n    window.addEventListener(\"beforeunload\", handleBeforeUnload);\n    return () => {\n      window.removeEventListener(\"beforeunload\", handleBeforeUnload);\n    };\n  }, [video]);\n\n  useEffect(() => {\n    if (scorePreset === \"any\") {\n      setScoreRange([0, 100]);\n      return;\n    }\n    if (scorePreset === \"60\") {\n      setScoreRange([60, 100]);\n      return;\n    }\n    if (scorePreset === \"70\") {\n      setScoreRange([70, 100]);\n      return;\n    }\n    if (scorePreset === \"80\") {\n      setScoreRange([80, 100]);\n      return;\n    }\n    if (scorePreset === \"90\") {\n      setScoreRange([90, 100]);\n      return;\n    }\n  }, [scorePreset]);\n\n  const filteredClips = useMemo(() => {\n    const [min, max] = scoreRange;\n    let list = clips.filter((clip) => {\n      if (selectedCategory !== \"all\" && clip.category !== selectedCategory) {\n        return false;\n      }\n      if (clip.scoreOverall < min || clip.scoreOverall > max) {\n        return false;\n      }\n      if (query.trim().length > 0) {\n        const q = query.toLowerCase();\n        const inTags = clip.tags.some((t) => t.toLowerCase().includes(q));\n        const inRationale = clip.rationale.toLowerCase().includes(q);\n        const inCategory = clip.category.toLowerCase().includes(q);\n        if (!inTags && !inRationale && !inCategory) {\n          return false;\n        }\n      }\n      return true;\n    });\n    list = list.sort((a, b) => {\n      if (sortKey === \"score\") {\n        if (sortDir === \"asc\") {\n          return a.scoreOverall - b.scoreOverall;\n        }\n        return b.scoreOverall - a.scoreOverall;\n      }\n      if (sortKey === \"duration\") {\n        if (sortDir === \"asc\") {\n          return a.durationSec - b.durationSec;\n        }\n        return b.durationSec - a.durationSec;\n      }\n      if (sortDir === \"asc\") {\n        return a.startSec - b.startSec;\n      }\n      return b.startSec - a.startSec;\n    });\n    return list;\n  }, [clips, selectedCategory, scoreRange, query, sortKey, sortDir]);\n\n  function hexToRgba(hex: string, alpha: number) {\n    let c = hex.replace(\"#\", \"\");\n    if (c.length === 3) {\n      c = c\n        .split(\"\")\n        .map((x) => x + x)\n        .join(\"\");\n    }\n    const r = parseInt(c.substring(0, 2), 16);\n    const g = parseInt(c.substring(2, 4), 16);\n    const b = parseInt(c.substring(4, 6), 16);\n    return `rgba(${r}, ${g}, ${b}, ${alpha})`;\n  }\n\n  function scoreHex(score: number) {\n    if (score < 30) {\n      return \"#ef4444\";\n    }\n    if (score < 45) {\n      return \"#f97316\";\n    }\n    if (score < 60) {\n      return \"#f59e0b\";\n    }\n    return \"#22c55e\";\n  }\n\n  function scoreChipSx(score: number) {\n    const c = scoreHex(score);\n    return {\n      fontWeight: 700,\n      color: hexToRgba(c, 1),\n      bgcolor: hexToRgba(c, 0.12),\n      borderColor: c,\n      borderWidth: 1,\n      borderStyle: \"solid\",\n    };\n  }\n\n  async function fetchVideo(id: string) {\n    setError(null);\n    try {\n      const response = await fetch(`/api/videos/${id}`);\n      const data = await response.json();\n      if (response.ok) {\n        setVideo(data);\n        const initial: Record<string, TikTokState> = {};\n        data.clips?.forEach((clip: Clip) => {\n          if (clip.tiktokStatus) {\n            initial[clip.id] = {\n              state: clip.tiktokStatus,\n              publishId: clip.tiktokPublishId || undefined,\n            };\n          }\n        });\n        setTtState(initial);\n      }\n    } catch {\n      setError(\"Error fetching video\");\n    } finally {\n      setLoading(false);\n    }\n  }\n\n  async function sendToTikTok(clipId: string) {\n    setError(null);\n    setSending((prev) => ({ ...prev, [clipId]: true }));\n    try {\n      const response = await fetch(`/api/tiktok/clip/${clipId}/post`, {\n        method: \"POST\",\n        headers: { \"content-type\": \"application/json\" },\n        body: JSON.stringify({ mode: \"draft\" }),\n      });\n      if (!response.ok) {\n        const data = await response.json().catch(() => null);\n        if (data && data.error) {\n          setError(data.error);\n        } else {\n          setError(\"Failed to send to TikTok\");\n        }\n        setTtState((prev) => ({ ...prev, [clipId]: { state: \"failed\" } }));\n        setSending((prev) => ({ ...prev, [clipId]: false }));\n        return;\n      }\n      const posted = await response.json().catch(() => ({}) as any);\n      if (posted?.publishId) {\n        setTtState((prev) => ({\n          ...prev,\n          [clipId]: { state: \"uploading\", publishId: posted.publishId },\n        }));\n      } else {\n        setTtState((prev) => ({ ...prev, [clipId]: { state: \"uploading\" } }));\n      }\n      const poll = async () => {\n        const statusRes = await fetch(`/api/tiktok/clip/${clipId}/status`);\n        if (statusRes.ok) {\n          const data = await statusRes.json();\n          if (data?.tiktokStatus) {\n            setTtState((prev) => ({\n              ...prev,\n              [clipId]: {\n                state: data.tiktokStatus,\n                publishId: data.publishId || prev[clipId]?.publishId,\n              },\n            }));\n            if (data.tiktokStatus === \"draft\") {\n              setSending((prev) => ({ ...prev, [clipId]: false }));\n              return;\n            }\n            if (data.tiktokStatus === \"failed\") {\n              setSending((prev) => ({ ...prev, [clipId]: false }));\n              return;\n            }\n            if (data.tiktokStatus === \"published\") {\n              setSending((prev) => ({ ...prev, [clipId]: false }));\n              return;\n            }\n          }\n        }\n        setTimeout(poll, 2000);\n      };\n      poll();\n    } catch {\n      setError(\"Failed to send to TikTok\");\n      setTtState((prev) => ({ ...prev, [clipId]: { state: \"failed\" } }));\n      setSending((prev) => ({ ...prev, [clipId]: false }));\n    }\n  }\n\n  async function deleteClip(clipId: string) {\n    setError(null);\n    const ok = window.confirm(\"Delete this clip?\");\n    if (ok) {\n      setDeleting((prev) => ({ ...prev, [clipId]: true }));\n      try {\n        const res = await fetch(`/api/clips/${clipId}`, { method: \"DELETE\" });\n        if (!res.ok) {\n          const data = await res.json().catch(() => null);\n          if (data && data.error) {\n            setError(data.error);\n          } else {\n            setError(\"Failed to delete clip\");\n          }\n          setDeleting((prev) => ({ ...prev, [clipId]: false }));\n          return;\n        }\n        setVideo((prev) => {\n          if (!prev) {\n            return prev;\n          }\n          return {\n            ...prev,\n            clips: prev.clips.filter((c) => {\n              if (c.id !== clipId) {\n                return true;\n              }\n              return false;\n            }),\n          };\n        });\n        setSending((prev) => {\n          const next = { ...prev };\n          delete next[clipId];\n          return next;\n        });\n        setTtState((prev) => {\n          const next = { ...prev };\n          delete next[clipId];\n          return next;\n        });\n        setDeleting((prev) => {\n          const next = { ...prev };\n          delete next[clipId];\n          return next;\n        });\n      } catch {\n        setError(\"Failed to delete clip\");\n        setDeleting((prev) => ({ ...prev, [clipId]: false }));\n      }\n    }\n  }\n\n  async function updateSubtitles(clipId: string) {\n    setError(null);\n    setUpdating((prev) => ({ ...prev, [clipId]: true }));\n    try {\n      const wordCount = wordsPerSubtitle[clipId] || 1;\n      const res = await fetch(`/api/clips/${clipId}/update-subs`, {\n        method: \"POST\",\n        headers: { \"Content-Type\": \"application/json\" },\n        body: JSON.stringify({ wordsPerSubtitle: wordCount }),\n      });\n      if (!res.ok) {\n        const data = await res.json().catch(() => null);\n        if (data && data.error) {\n          setError(data.error);\n        } else {\n          setError(\"Failed to update subtitles\");\n        }\n        setUpdating((prev) => ({ ...prev, [clipId]: false }));\n        return;\n      }\n      const id = video?.id || \"\";\n      if (id) {\n        await fetchVideo(id);\n      }\n      setUpdating((prev) => ({ ...prev, [clipId]: false }));\n    } catch {\n      setError(\"Failed to update subtitles\");\n      setUpdating((prev) => ({ ...prev, [clipId]: false }));\n    }\n  }\n\n  async function centerClip(clipId: string) {\n    setError(null);\n    setCentering((prev) => ({ ...prev, [clipId]: true }));\n    try {\n      const res = await fetch(`/api/clips/${clipId}/center`, {\n        method: \"POST\",\n      });\n      if (!res.ok) {\n        const data = await res.json().catch(() => null);\n        if (data && data.error) {\n          setError(data.error);\n        } else {\n          setError(\"Failed to center the video\");\n        }\n        setCentering((prev) => ({ ...prev, [clipId]: false }));\n        return;\n      }\n      const id = video?.id || \"\";\n      if (id) {\n        await fetchVideo(id);\n      }\n      setCentering((prev) => ({ ...prev, [clipId]: false }));\n    } catch {\n      setError(\"Failed to center the video\");\n      setCentering((prev) => ({ ...prev, [clipId]: false }));\n    }\n  }\n\n  function openMenu(clipId: string, el: HTMLElement) {\n    setActionMenu((prev) => ({ ...prev, [clipId]: el }));\n  }\n\n  function closeMenu(clipId: string) {\n    setActionMenu((prev) => ({ ...prev, [clipId]: null }));\n  }\n\n  if (loading) {\n    return (\n      <Box\n        minHeight=\"100vh\"\n        display=\"grid\"\n        alignItems=\"center\"\n        justifyContent=\"center\"\n        gap={2}\n      >\n        <CircularProgress />\n        <Typography variant=\"body2\" color=\"text.secondary\">\n          Loading video…\n        </Typography>\n      </Box>\n    );\n  }\n\n  if (!video) {\n    return (\n      <Box minHeight=\"100vh\" display=\"grid\" placeItems=\"center\">\n        <Stack spacing={2} alignItems=\"center\">\n          <Typography variant=\"h6\">Video not found</Typography>\n          <Button href=\"/\" component={Link}>\n            Back to all videos\n          </Button>\n        </Stack>\n      </Box>\n    );\n  }\n\n  return (\n    <Box minHeight=\"100vh\" display=\"flex\" flexDirection=\"column\">\n      <AppBar position=\"sticky\">\n        <Toolbar sx={{ gap: 1 }}>\n          <Tooltip title=\"Back\">\n            <IconButton\n              edge=\"start\"\n              color=\"inherit\"\n              onClick={() => router.push(\"/\")}\n            >\n              <ArrowBackIcon />\n            </IconButton>\n          </Tooltip>\n          <Typography\n            variant=\"h6\"\n            sx={{\n              ml: 1,\n              flexGrow: 1,\n              minWidth: 0,\n              overflow: \"hidden\",\n              textOverflow: \"ellipsis\",\n              whiteSpace: \"nowrap\",\n            }}\n          >\n            {video.title}\n          </Typography>\n          <Button href=\"/\" component={Link} color=\"primary\">\n            All Videos\n          </Button>\n        </Toolbar>\n        {isProcessing && <LinearProgress />}\n      </AppBar>\n\n      <Container sx={{ py: 4, flex: 1 }}>\n        {error && (\n          <Alert severity=\"error\" sx={{ mb: 3 }}>\n            {error}\n          </Alert>\n        )}\n\n        <Paper sx={{ p: 2, mb: 2 }}>\n          <Stack spacing={2}>\n            <Stack direction={{ xs: \"column\", md: \"row\" }} spacing={2}>\n              <Paper sx={{ p: 1.25, flex: 1 }}>\n                <Tabs\n                  value={tabCategory}\n                  onChange={(_, v) => {\n                    setTabCategory(v);\n                    const cat = categories[v] || \"all\";\n                    setSelectedCategory(cat);\n                  }}\n                  variant=\"scrollable\"\n                  scrollButtons=\"auto\"\n                >\n                  {categories.map((c) => (\n                    <Tab key={c} label={c === \"all\" ? \"All\" : c} />\n                  ))}\n                </Tabs>\n              </Paper>\n\n              <Paper\n                sx={{\n                  p: 1.25,\n                  display: \"flex\",\n                  alignItems: \"center\",\n                  gap: 1.5,\n                  flexWrap: \"wrap\",\n                }}\n              >\n                <TextField\n                  value={query}\n                  onChange={(e) => setQuery(e.target.value)}\n                  size=\"small\"\n                  placeholder=\"Search tags, rationale, category\"\n                  InputProps={{\n                    startAdornment: (\n                      <InputAdornment position=\"start\">\n                        <SearchIcon fontSize=\"small\" />\n                      </InputAdornment>\n                    ),\n                  }}\n                  sx={{ minWidth: 260, flex: 2 }}\n                />\n                <FormControl size=\"small\" sx={{ minWidth: 160 }}>\n                  <InputLabel id=\"sort-key\">Sort</InputLabel>\n                  <Select\n                    labelId=\"sort-key\"\n                    label=\"Sort\"\n                    value={sortKey}\n                    onChange={(e) => setSortKey(e.target.value as SortKey)}\n                  >\n                    <MenuItem value=\"score\">Score</MenuItem>\n                    <MenuItem value=\"duration\">Duration</MenuItem>\n                    <MenuItem value=\"start\">Start time</MenuItem>\n                  </Select>\n                </FormControl>\n                <ToggleButtonGroup\n                  size=\"small\"\n                  value={sortDir}\n                  exclusive\n                  onChange={(_, v) => {\n                    if (v) {\n                      setSortDir(v);\n                    }\n                  }}\n                >\n                  <ToggleButton value=\"asc\">Asc</ToggleButton>\n                  <ToggleButton value=\"desc\">Desc</ToggleButton>\n                </ToggleButtonGroup>\n              </Paper>\n            </Stack>\n\n            <Paper sx={{ p: 1.5 }}>\n              <Stack spacing={1.25}>\n                <Stack\n                  direction=\"row\"\n                  spacing={1}\n                  alignItems=\"center\"\n                  justifyContent=\"space-between\"\n                  flexWrap=\"wrap\"\n                >\n                  <Stack direction=\"row\" spacing={1} flexWrap=\"wrap\">\n                    <Chip\n                      size=\"small\"\n                      label=\"Any\"\n                      color={scorePreset === \"any\" ? \"primary\" : undefined}\n                      variant={scorePreset === \"any\" ? \"filled\" : \"outlined\"}\n                      onClick={() => setScorePreset(\"any\")}\n                    />\n                    <Chip\n                      size=\"small\"\n                      label=\"60+\"\n                      color={scorePreset === \"60\" ? \"primary\" : undefined}\n                      variant={scorePreset === \"60\" ? \"filled\" : \"outlined\"}\n                      onClick={() => setScorePreset(\"60\")}\n                    />\n                    <Chip\n                      size=\"small\"\n                      label=\"70+\"\n                      color={scorePreset === \"70\" ? \"primary\" : undefined}\n                      variant={scorePreset === \"70\" ? \"filled\" : \"outlined\"}\n                      onClick={() => setScorePreset(\"70\")}\n                    />\n                    <Chip\n                      size=\"small\"\n                      label=\"80+\"\n                      color={scorePreset === \"80\" ? \"primary\" : undefined}\n                      variant={scorePreset === \"80\" ? \"filled\" : \"outlined\"}\n                      onClick={() => setScorePreset(\"80\")}\n                    />\n                    <Chip\n                      size=\"small\"\n                      label=\"90+\"\n                      color={scorePreset === \"90\" ? \"primary\" : undefined}\n                      variant={scorePreset === \"90\" ? \"filled\" : \"outlined\"}\n                      onClick={() => setScorePreset(\"90\")}\n                    />\n                  </Stack>\n\n                  <Stack\n                    direction=\"row\"\n                    spacing={2}\n                    alignItems=\"center\"\n                    sx={{ minWidth: 260, flex: 1 }}\n                  >\n                    <Typography\n                      variant=\"caption\"\n                      sx={{ minWidth: 90, textAlign: \"right\" }}\n                    >\n                      Score range\n                    </Typography>\n                    <Slider\n                      value={scoreRange}\n                      onChange={(_, v) => {\n                        const val = v as number[];\n                        setScoreRange([\n                          Math.min(val[0], val[1]),\n                          Math.max(val[0], val[1]),\n                        ]);\n                        setScorePreset(\"any\");\n                      }}\n                      min={0}\n                      max={100}\n                      step={1}\n                      valueLabelDisplay=\"auto\"\n                      disableSwap\n                      sx={{ flex: 1 }}\n                    />\n                  </Stack>\n                </Stack>\n              </Stack>\n            </Paper>\n\n            <Paper\n              sx={{\n                p: 1.25,\n                display: \"grid\",\n                gridTemplateColumns: { xs: \"1fr\", sm: \"repeat(3,1fr)\" },\n                gap: 1.5,\n              }}\n            >\n              <Stack spacing={0.25} alignItems=\"center\">\n                <Typography variant=\"caption\" color=\"text.secondary\">\n                  Status\n                </Typography>\n                <Chip\n                  size=\"small\"\n                  label={video.status}\n                  color={\n                    video.status === \"completed\"\n                      ? \"success\"\n                      : video.status === \"processing\"\n                        ? \"warning\"\n                        : video.status === \"failed\"\n                          ? \"error\"\n                          : \"default\"\n                  }\n                />\n              </Stack>\n              <Stack spacing={0.25} alignItems=\"center\">\n                <Typography variant=\"caption\" color=\"text.secondary\">\n                  Duration\n                </Typography>\n                <Stack direction=\"row\" spacing={1} alignItems=\"center\">\n                  <AccessTimeIcon fontSize=\"small\" />\n                  <Box>{Math.floor(video.durationSec / 60)}m</Box>\n                </Stack>\n              </Stack>\n              <Stack spacing={0.25} alignItems=\"center\">\n                <Typography variant=\"caption\" color=\"text.secondary\">\n                  Clips\n                </Typography>\n                <Stack direction=\"row\" spacing={3} alignItems=\"center\">\n                  <QueryStatsIcon fontSize=\"small\" />\n                  <Badge color=\"primary\" badgeContent={video.clips.length} />\n                </Stack>\n              </Stack>\n            </Paper>\n          </Stack>\n        </Paper>\n\n        <Box\n          display=\"grid\"\n          gridTemplateColumns={{ xs: \"1fr\", sm: \"1fr 1fr\", lg: \"1fr 1fr 1fr\" }}\n          gap={3}\n        >\n          {filteredClips.map((clip) => {\n            const state = ttState[clip.id]?.state;\n            const publishId = ttState[clip.id]?.publishId;\n            const menuOpen = Boolean(actionMenu[clip.id]);\n            const busy = Boolean(updating[clip.id] || centering[clip.id]);\n\n            return (\n              <Card\n                key={clip.id}\n                sx={{\n                  display: \"flex\",\n                  flexDirection: \"column\",\n                  height: \"100%\",\n                  position: \"relative\",\n                  overflow: \"hidden\",\n                }}\n                elevation={1}\n              >\n                {busy && (\n                  <Box\n                    sx={{\n                      position: \"absolute\",\n                      inset: 0,\n                      bgcolor: \"rgba(37,99,235,0.12)\",\n                      zIndex: 3,\n                      display: \"flex\",\n                      alignItems: \"center\",\n                      justifyContent: \"center\",\n                      flexDirection: \"column\",\n                      backdropFilter: \"blur(2px)\",\n                    }}\n                  >\n                    <CircularProgress size={28} sx={{ mb: 1 }} />\n                    <Typography variant=\"body2\">\n                      {updating[clip.id]\n                        ? \"Updating subtitles…\"\n                        : \"Reframing video…\"}\n                    </Typography>\n                  </Box>\n                )}\n\n                <Box\n                  sx={{\n                    px: 2,\n                    py: 1,\n                    display: \"flex\",\n                    alignItems: \"center\",\n                    justifyContent: \"space-between\",\n                  }}\n                >\n                  <Stack\n                    direction=\"row\"\n                    spacing={1}\n                    alignItems=\"center\"\n                    flexWrap=\"wrap\"\n                  >\n                    <Chip size=\"small\" label={clip.category} />\n                    <Chip\n                      size=\"small\"\n                      icon={<SpeedIcon fontSize=\"small\" />}\n                      label={`${clip.durationSec}s`}\n                      variant=\"outlined\"\n                    />\n                  </Stack>\n                  <Chip\n                    size=\"small\"\n                    sx={scoreChipSx(clip.scoreOverall)}\n                    label={clip.scoreOverall}\n                  />\n                </Box>\n\n                <CardMedia\n                  component=\"video\"\n                  controls\n                  poster={clip.thumbUrl}\n                  src={clip.videoUrl}\n                  sx={{\n                    aspectRatio: \"9/16\",\n                    objectFit: \"cover\",\n                    bgcolor: \"black\",\n                  }}\n                />\n\n                <CardContent\n                  sx={{\n                    display: \"flex\",\n                    flexDirection: \"column\",\n                    gap: 1.25,\n                    flexGrow: 1,\n                  }}\n                >\n                  <Stack direction=\"row\" spacing={1} flexWrap=\"wrap\">\n                    {clip.tags.map((t, i) => (\n                      <Chip\n                        key={`${clip.id}-tag-${i}`}\n                        size=\"small\"\n                        label={t}\n                        variant=\"outlined\"\n                      />\n                    ))}\n                  </Stack>\n\n                  <Stack\n                    direction=\"row\"\n                    spacing={2}\n                    flexWrap=\"wrap\"\n                    sx={{ color: \"text.secondary\" }}\n                  >\n                    <Box>Hook: {clip.scoreHook}/10</Box>\n                    <Box>Retention: {clip.scoreRetention}/10</Box>\n                    <Box>Clarity: {clip.scoreClarity}/10</Box>\n                    <Box>Share: {clip.scoreShare}/10</Box>\n                  </Stack>\n\n                  <Typography\n                    variant=\"body2\"\n                    sx={{\n                      display: \"-webkit-box\",\n                      overflow: \"hidden\",\n                      WebkitLineClamp: 3,\n                      WebkitBoxOrient: \"vertical\",\n                      minHeight: 60,\n                    }}\n                  >\n                    {clip.rationale}\n                  </Typography>\n\n                  <Stack\n                    direction=\"row\"\n                    spacing={1}\n                    alignItems=\"center\"\n                    sx={{ color: \"text.secondary\" }}\n                  >\n                    <AccessTimeIcon fontSize=\"small\" />\n                    <Typography variant=\"caption\">\n                      {Math.floor(clip.startSec)}s – {Math.floor(clip.endSec)}s\n                    </Typography>\n                    <Typography variant=\"caption\" sx={{ ml: 0.5 }}>\n                      ({clip.durationSec}s)\n                    </Typography>\n                  </Stack>\n\n                  <Stack direction=\"row\" spacing={1} alignItems=\"center\">\n                    <Typography variant=\"caption\" color=\"text.secondary\">\n                      TikTok:\n                    </Typography>\n                    <Typography\n                      variant=\"caption\"\n                      color={\n                        state === \"published\"\n                          ? \"success.main\"\n                          : state === \"failed\"\n                            ? \"error.main\"\n                            : state === \"draft\"\n                              ? \"warning.main\"\n                              : state === \"uploading\"\n                                ? \"info.main\"\n                                : \"text.secondary\"\n                      }\n                    >\n                      {state || \"—\"}\n                    </Typography>\n                    {publishId && (\n                      <Typography variant=\"caption\" color=\"text.secondary\">\n                        ({publishId})\n                      </Typography>\n                    )}\n                  </Stack>\n\n                  <Stack\n                    direction=\"row\"\n                    alignItems=\"center\"\n                    justifyContent=\"space-between\"\n                    sx={{ mt: \"auto\" }}\n                  >\n                    <Button\n                      onClick={() => sendToTikTok(clip.id)}\n                      disabled={\n                        Boolean(sending[clip.id]) ||\n                        ttState[clip.id]?.state === \"published\" ||\n                        busy\n                      }\n                      color=\"secondary\"\n                      fullWidth\n                      sx={{ mr: 1 }}\n                    >\n                      {sending[clip.id] ? \"Sending…\" : \"Send to TikTok\"}\n                    </Button>\n\n                    <IconButton\n                      onClick={(e) => openMenu(clip.id, e.currentTarget)}\n                      disabled={\n                        Boolean(sending[clip.id]) || Boolean(deleting[clip.id])\n                      }\n                      sx={{ zIndex: 4 }}\n                    >\n                      <MoreVertIcon />\n                    </IconButton>\n\n                    <Menu\n                      anchorEl={actionMenu[clip.id] || null}\n                      open={menuOpen}\n                      onClose={() => closeMenu(clip.id)}\n                      anchorOrigin={{ vertical: \"bottom\", horizontal: \"right\" }}\n                      transformOrigin={{ vertical: \"top\", horizontal: \"right\" }}\n                    >\n                      <Box sx={{ px: 2, py: 1 }}>\n                        <Typography variant=\"caption\" color=\"text.secondary\">\n                          Words per subtitle\n                        </Typography>\n                        <Select\n                          size=\"small\"\n                          value={wordsPerSubtitle[clip.id] || 1}\n                          onChange={(e) => {\n                            setWordsPerSubtitle((prev) => ({\n                              ...prev,\n                              [clip.id]: Number(e.target.value),\n                            }));\n                          }}\n                          fullWidth\n                          sx={{ mt: 0.5 }}\n                        >\n                          <MenuItem value={1}>1 word</MenuItem>\n                          <MenuItem value={2}>2 words</MenuItem>\n                          <MenuItem value={3}>3 words</MenuItem>\n                          <MenuItem value={4}>4 words</MenuItem>\n                          <MenuItem value={5}>5 words</MenuItem>\n                        </Select>\n                      </Box>\n                      <MenuItem\n                        onClick={() => {\n                          closeMenu(clip.id);\n                          updateSubtitles(clip.id);\n                        }}\n                        disabled={Boolean(updating[clip.id])}\n                      >\n                        <ListItemIcon>\n                          <SubtitlesIcon fontSize=\"small\" />\n                        </ListItemIcon>\n                        <ListItemText\n                          primary={\n                            updating[clip.id] ? \"Updating…\" : \"Update Subtitles\"\n                          }\n                        />\n                      </MenuItem>\n\n                      <MenuItem\n                        onClick={() => {\n                          closeMenu(clip.id);\n                          centerClip(clip.id);\n                        }}\n                        disabled={\n                          Boolean(centering[clip.id]) ||\n                          Boolean(sending[clip.id]) ||\n                          Boolean(deleting[clip.id])\n                        }\n                      >\n                        <ListItemIcon>\n                          <CenterFocusStrongIcon fontSize=\"small\" />\n                        </ListItemIcon>\n                        <ListItemText\n                          primary={\n                            centering[clip.id] ? \"Reframing…\" : \"Reframe\"\n                          }\n                        />\n                      </MenuItem>\n\n                      <Divider />\n\n                      <MenuItem\n                        component=\"a\"\n                        href={clip.videoUrl}\n                        target=\"_blank\"\n                        rel=\"noopener noreferrer\"\n                        onClick={() => closeMenu(clip.id)}\n                      >\n                        <ListItemIcon>\n                          <MovieIcon fontSize=\"small\" />\n                        </ListItemIcon>\n                        <ListItemText primary=\"Open Video\" />\n                      </MenuItem>\n\n                      <MenuItem\n                        component=\"a\"\n                        href={clip.srtUrl}\n                        target=\"_blank\"\n                        rel=\"noopener noreferrer\"\n                        onClick={() => closeMenu(clip.id)}\n                      >\n                        <ListItemIcon>\n                          <DescriptionIcon fontSize=\"small\" />\n                        </ListItemIcon>\n                        <ListItemText primary=\"Open SRT\" />\n                      </MenuItem>\n\n                      <Divider />\n\n                      <MenuItem\n                        onClick={() => {\n                          closeMenu(clip.id);\n                          deleteClip(clip.id);\n                        }}\n                        disabled={Boolean(deleting[clip.id])}\n                        sx={{ color: \"error.main\" }}\n                      >\n                        <ListItemIcon>\n                          <DeleteOutlineIcon fontSize=\"small\" color=\"error\" />\n                        </ListItemIcon>\n                        <ListItemText\n                          primary={deleting[clip.id] ? \"Deleting…\" : \"Delete\"}\n                        />\n                      </MenuItem>\n                    </Menu>\n                  </Stack>\n                </CardContent>\n              </Card>\n            );\n          })}\n        </Box>\n\n        {filteredClips.length === 0 && (\n          <Box py={8} textAlign=\"center\" color=\"text.secondary\">\n            <Stack spacing={2} alignItems=\"center\">\n              <Typography variant=\"h6\">No clips match your filters</Typography>\n              <Typography variant=\"body2\">\n                Try adjusting the score, category, or search query\n              </Typography>\n              <Button\n                variant=\"outlined\"\n                onClick={() => {\n                  setSelectedCategory(\"all\");\n                  setScoreRange([0, 100]);\n                  setScorePreset(\"any\");\n                  setQuery(\"\");\n                  setTabCategory(0);\n                }}\n              >\n                Reset filters\n              </Button>\n            </Stack>\n          </Box>\n        )}\n      </Container>\n    </Box>\n  );\n}\n","size_bytes":36165},"src/lib/encryption.ts":{"content":"import { createCipheriv, createDecipheriv, randomBytes, scryptSync } from 'crypto'\n\nconst ALGORITHM = 'aes-256-gcm'\nconst KEY_LENGTH = 32\nconst IV_LENGTH = 16\nconst SALT_LENGTH = 16\nconst TAG_LENGTH = 16\n\nfunction getKey(salt: Buffer): Buffer {\n  const secret = process.env.SESSION_SECRET || 'complex_password_at_least_32_characters_long_for_session_security'\n  return scryptSync(secret, salt, KEY_LENGTH)\n}\n\nexport function encrypt(text: string): string {\n  const salt = randomBytes(SALT_LENGTH)\n  const iv = randomBytes(IV_LENGTH)\n  const key = getKey(salt)\n  \n  const cipher = createCipheriv(ALGORITHM, key, iv)\n  \n  const encrypted = Buffer.concat([\n    cipher.update(text, 'utf8'),\n    cipher.final()\n  ])\n  \n  const tag = cipher.getAuthTag()\n  \n  const result = Buffer.concat([salt, iv, tag, encrypted])\n  return result.toString('base64')\n}\n\nexport function decrypt(encryptedText: string): string {\n  const buffer = Buffer.from(encryptedText, 'base64')\n  \n  const salt = buffer.subarray(0, SALT_LENGTH)\n  const iv = buffer.subarray(SALT_LENGTH, SALT_LENGTH + IV_LENGTH)\n  const tag = buffer.subarray(SALT_LENGTH + IV_LENGTH, SALT_LENGTH + IV_LENGTH + TAG_LENGTH)\n  const encrypted = buffer.subarray(SALT_LENGTH + IV_LENGTH + TAG_LENGTH)\n  \n  const key = getKey(salt)\n  \n  const decipher = createDecipheriv(ALGORITHM, key, iv)\n  decipher.setAuthTag(tag)\n  \n  const decrypted = Buffer.concat([\n    decipher.update(encrypted),\n    decipher.final()\n  ])\n  \n  return decrypted.toString('utf8')\n}\n","size_bytes":1497},"app/api/auth/logout/route.ts":{"content":"import { NextResponse } from 'next/server'\nimport { getSession } from '@/src/lib/session'\n\nexport async function POST() {\n  try {\n    const session = await getSession()\n    session.destroy()\n    \n    return NextResponse.json({ success: true })\n  }\n  catch (error) {\n    return NextResponse.json(\n      { error: 'Failed to logout' },\n      { status: 500 }\n    )\n  }\n}\n","size_bytes":367},"src/services/segmentation.ts":{"content":"import { TranscriptSegment, TranscriptWord, isSentenceBoundaryToken } from './openai'\nimport { SceneChange } from './ffmpeg'\nimport { Chapter } from './youtube'\n\nexport interface Segment {\n  startSec: number\n  endSec: number\n  durationSec: number\n  words: TranscriptWord[]\n  text: string\n  hook: string\n  score: number\n  chapterTitle?: string\n}\n\nfunction hasOverlap(a: Segment, b: Segment): boolean {\n  if (a.endSec <= b.startSec) { return false }\n  if (b.endSec <= a.startSec) { return false }\n  return true\n}\n\nfunction removeOverlaps(segments: Segment[]): Segment[] {\n  if (segments.length === 0) { return [] }\n  const sorted = [...segments].sort((x, y) => y.score - x.score)\n  const out: Segment[] = []\n  for (const s of sorted) {\n    let keep = true\n    for (const k of out) { if (hasOverlap(s, k)) { keep = false; break } }\n    if (keep) { out.push(s) }\n  }\n  return out.sort((a, b) => a.startSec - b.startSec)\n}\n\nfunction chooseTargets(): number[] { return [60, 120] }\n\nfunction withinTolerance(d: number, target: number): boolean {\n  const low = target - 15\n  const high = target + 15\n  if (d < low) { return false }\n  if (d > high) { return false }\n  return true\n}\n\nfunction snapToBoundary(words: TranscriptWord[], startSec: number, hardEndSec: number, target: number): { endSec: number; slice: TranscriptWord[] } {\n  if (words.length === 0) { return { endSec: startSec, slice: [] } }\n  const desired = startSec + target\n  const maxEnd = Math.min(hardEndSec, startSec + target + 15)\n  const minEnd = Math.max(startSec + target - 15, startSec + 20)\n  let idx = words.findIndex(w => w.end >= desired)\n  if (idx === -1) { idx = words.length - 1 }\n  let pick = idx\n  for (let i = idx; i < words.length; i++) {\n    const w = words[i]\n    if (w.end > maxEnd) { break }\n    const tok = w.word\n    const next = words[i + 1]\n    const gap = next ? next.start - w.end : 0\n    if (isSentenceBoundaryToken(tok)) { pick = i; break }\n    if (gap >= 0.8 && w.end >= minEnd) { pick = i; break }\n  }\n  if (words[pick].end - startSec < 20 && words[words.length - 1].end - startSec >= 20) { while (pick < words.length - 1 && words[pick].end - startSec < 20) { pick = pick + 1 } }\n  const endSec = Math.min(maxEnd, words[pick].end)\n  const slice = words.filter(w => w.end <= endSec + 1e-3)\n  return { endSec, slice }\n}\n\nfunction scoreBasic(words: TranscriptWord[], start: number, end: number, sceneChanges: SceneChange[]): number {\n  const dur = Math.max(0.001, end - start)\n  const speech = words.reduce((s, w) => s + Math.max(0, w.end - w.start), 0)\n  const pause = Math.max(0, dur - speech)\n  const pauseRatio = pause / dur\n  const wps = words.length / dur\n  const hook = words.filter(w => w.start - start < 3).map(w => w.word).join(' ').toLowerCase()\n  const hookHits = ['how','why','what','watch','secret','truth','mistake','biggest','here\\'s','stop'].reduce((n, k) => hook.includes(k) ? n + 1 : n, 0)\n  const scenes = sceneChanges.filter(s => s.timeSec >= start && s.timeSec <= end).length\n  const pacing = Math.max(0, 1 - Math.abs(pauseRatio - 0.18))\n  const variety = scenes >= 1 && scenes <= 6 ? 0.8 : 0.5\n  const hookness = Math.min(1, 0.4 + 0.1 * hookHits + (wps > 2.2 ? 0.1 : 0))\n  const coh = end - start >= 20 ? 0.7 : 0.4\n  const base = 0.3 * hookness + 0.25 * pacing + 0.2 * variety + 0.25 * coh\n  return base\n}\n\nexport function detectSegments(transcript: TranscriptSegment[], sceneChanges: SceneChange[], chapters: Chapter[] = [], videoDuration: number = 0): Segment[] {\n  const all: TranscriptWord[] = []\n  for (const s of transcript) { for (const w of s.words) { all.push(w) } }\n  if (all.length === 0) { return [] }\n  const targets = chooseTargets()\n  const limLow = Math.min(...targets) - 15\n  const limHigh = Math.max(...targets) + 15\n  const windows: Array<{ start: number; end: number; title: string }> = []\n  const stride = 45\n  const winLen = Math.min(limHigh + 10, 135)\n  const endCap = Math.max(videoDuration, all[all.length - 1].end)\n  for (let t = 0; t + limLow < endCap; t += stride) { const e = Math.min(endCap, t + winLen); windows.push({ start: t, end: e, title: 'Window' }) }\n  const candidates: Segment[] = []\n  for (const win of windows) {\n    const words = all.filter(w => w.start >= win.start && w.start < win.end)\n    if (words.length < 12) { continue }\n    const pauses: number[] = [0]\n    for (let i = 0; i < words.length - 1; i++) {\n      const gap = words[i + 1].start - words[i].end\n      if (gap >= 0.35 && gap <= 1.2) { pauses.push(i + 1) }\n    }\n    pauses.push(words.length)\n    for (let i = 0; i < pauses.length - 1; i++) {\n      for (let j = i + 1; j < pauses.length; j++) {\n        const segWords = words.slice(pauses[i], pauses[j])\n        if (segWords.length < 10) { continue }\n        const s = segWords[0].start\n        const hardEnd = segWords[segWords.length - 1].end\n        for (const t of targets) {\n          const { endSec, slice } = snapToBoundary(segWords, s, hardEnd, t)\n          const d = endSec - s\n          if (!withinTolerance(d, t)) { continue }\n          const score = scoreBasic(slice, s, endSec, sceneChanges)\n          const text = slice.map(w => w.word).join(' ')\n          const hook = slice.filter(w => w.start - s < 3).map(w => w.word).join(' ').trim()\n          candidates.push({ startSec: s, endSec, durationSec: d, words: slice, text, hook: hook || text.substring(0, 60), score, chapterTitle: win.title })\n        }\n      }\n    }\n  }\n  const filtered = candidates.filter(c => c.durationSec >= 45 && c.durationSec <= 135)\n  filtered.sort((a, b) => b.score - a.score)\n  const unique = removeOverlaps(filtered)\n  return unique.slice(0, 12)\n}\n","size_bytes":5610},"src/lib/session.ts":{"content":"import { getIronSession } from \"iron-session\";\nimport { cookies } from \"next/headers\";\n\nexport interface SessionData {\n  isAuthenticated: boolean;\n  userId?: string;\n  email?: string;\n}\n\nexport async function getSession() {\n  const sessionCookies = await cookies();\n\n  return getIronSession<SessionData>(sessionCookies, {\n    password:\n      process.env.SESSION_SECRET ||\n      \"complex_password_at_least_32_characters_long_for_session_security\",\n    cookieName: \"yt_shortsmith_session\",\n    cookieOptions: {\n      secure: process.env.NODE_ENV === \"production\",\n      httpOnly: true,\n      sameSite: \"lax\",\n      maxAge: 60 * 60 * 24 * 7,\n      path: \"/\",\n    },\n  });\n}\n\nexport async function requireAuth() {\n  const session = await getSession();\n\n  if (!session.isAuthenticated) {\n    throw new Error(\"Authentication required\");\n  }\n\n  return session;\n}\n\nexport async function getCurrentUserId() {\n  const s = await requireAuth();\n  if (!s.userId) {\n    return \"\";\n  }\n  return s.userId;\n}\n","size_bytes":992},"next.config.js":{"content":"/** @type {import('next').NextConfig} */\nconst nextConfig = {\n  experimental: {\n    serverActions: {\n      bodySizeLimit: \"2mb\",\n    },\n    serverComponentsExternalPackages: [\n      \"ffmpeg-static\",\n      \"ffprobe-static\",\n      \"@tensorflow/tfjs-node\",\n      \"@vladmandic/face-api\",\n    ],\n  },\n  eslint: {\n    ignoreDuringBuilds: true,\n  },\n};\n\nmodule.exports = nextConfig;\n","size_bytes":376},"replit.md":{"content":"# YT Shortsmith\n\n## Overview\nYT Shortsmith is an AI-powered service designed to automate the creation of engaging short-form video clips from YouTube videos. It handles the entire process from downloading and transcribing to intelligently segmenting and formatting videos into 20-60 second vertical clips optimized for social media platforms like TikTok, Instagram Reels, and YouTube Shorts. The project's main purpose is to streamline content repurposing for creators, enabling them to efficiently transform long-form content into high-impact social media assets.\n\n## User Preferences\nPreferred communication style: Simple, everyday language.\n\nCode style constraints:\n- Never add comments to code\n- Always put single if conditions inside brackets with line breaks\n- Focus on clean, self-documenting code\n\n**Cost & Resource Management**:\n- Maximum 12 clips per video\n- Immediate cancellation required to prevent resource waste\n- No tolerance for processing beyond cancellation\n\n## System Architecture\n\n### Frontend\nThe frontend uses Next.js 14 (App Router), TypeScript, and TailwindCSS for a modern, responsive user interface with server-side rendering, featuring a dashboard and video detail pages.\n\n### Backend\nThe backend is built with Node.js 20 and TypeScript, leveraging a Next.js API layer for REST endpoints and a BullMQ-powered worker process (`src/worker.ts`) for asynchronous video processing.\n\n**Core Processing Pipeline:**\n1.  **Job Creation**: User submits a YouTube URL, initiating video record creation and job enqueuing.\n2.  **Video Ingestion**: Downloads video metadata and the video content using `yt-dlp`.\n3.  **Audio Processing**: Extracts, compresses, and transcribes audio using the Whisper API with word-level timestamps and automatic language detection.\n4.  **Comment Mining**: Fetches YouTube comments to detect timestamp hotspots for engagement-aware scoring.\n5.  **Intelligent Segmentation**: Segments videos into clips enforced to 60s or 120s duration (±10-15s tolerance), ending at sentence boundaries (punctuation or pauses ≥0.8s) to prevent mid-sentence cuts. Uses voice activity, pauses, scene changes, and multi-factor feature extraction.\n6.  **Two-Stage Scoring** (Cost-Optimized):\n    - **Stage 1**: All segments scored using rule-based 7-pillar features (hook, watchability, visuals, safety, novelty, coherence, duration fit) - instant and free\n    - **Stage 2**: Only top 25 candidates (by rule-based score) sent to GPT-4o for AI overall scoring\n    - Result: Maximum 25 GPT-4o calls per video, regardless of segment count (saves ~50% on videos with 50+ segments)\n7.  **Diversity Selection**: MMR (Maximal Marginal Relevance) algorithm selects diverse clips to prevent duplicates, ensuring variety in content.\n8.  **Final Ranking**: Clips are ranked with S/A/B tier assignment based on geometric mean of pillars, AI overall score, hotspot proximity, and duration preference. Max 12 clips selected with 33% quota for 120s clips.\n9.  **Taxonomy Classification**: Auto-categorizes clips by hook type (question, bold, number, contrast, statement, story) and tone (educational, motivational, humor, commentary, etc.).\n10. **Clip Generation**: Converts selected segments to a 9:16 vertical format, applies smart cropping (with body-aware framing), adds TikTok-optimized burned-in subtitles (1-5 words per subtitle using Forever Freedom Regular font), hook text overlay, and generates a thumbnail. Assets are uploaded to S3-compatible storage.\n11. **Data Storage**: Stores clip metadata, scores, taxonomy, tier, ranking reasons, S3 keys, and crop maps in the database.\n\n**Data Models (Prisma):**\n-   `User`: Authentication and user-specific data.\n-   `Video`: Source YouTube video metadata, comment hotspot data, and global crop map for consistent framing across all clips.\n-   `Clip`: Generated short video clips and their associated data, including rationale and feature vectors.\n-   `TikTokConnection`: User TikTok OAuth tokens.\n\n**Core Services:**\n-   `framingService.ts`: Global static framing using face-api.js (SSD MobileNet v1) for face detection with pose-detection fallback for full-body tracking. Includes `computeGlobalStaticCrop()` that samples frames across entire video to compute ONE canonical crop position, and `computeCropMapPersonStatic()` that applies this global crop to individual segments. Ensures consistent person-centered framing across all clips from the same video, eliminating black padding and inconsistent positioning issues.\n-   `ffmpeg.ts`: Handles video/audio processing (rendering, audio extraction, thumbnail generation).\n-   `youtube.ts`: Manages `yt-dlp` operations for YouTube video downloads using cookie-based authentication.\n-   `youtube-oauth.ts`: Handles YouTube OAuth token retrieval and management via Replit's YouTube connector for YouTube Data API access.\n-   `cookieGenerator.ts`: Manages YouTube cookie storage, age tracking, and expiration monitoring for yt-dlp authentication.\n-   `youtube-comments.ts`: Fetches YouTube comments and mines timestamp hotspots for engagement scoring.\n-   `segmentation.ts`: Implements 60/120s clip segmentation with sentence boundary detection.\n-   `segmentation-v2.ts`: Enhanced segmentation with multi-factor feature extraction, comment hotspot integration, and adaptive duration selection.\n-   `openai.ts`: Integrates OpenAI APIs for Whisper transcription and GPT-4o clip scoring, includes isSentenceBoundaryToken helper.\n-   `scoring/features.ts`: Computes 7-pillar features (hook, watchability, visuals, safety, novelty, coherence, duration fit).\n-   `scoring/clipScorer.ts`: Scores clips using extracted features and multi-factor algorithms.\n-   `scoring/clipRanker.ts`: Final ranking with geometric mean blending, S/A/B tier assignment, and reason generation.\n-   `scoring/taxonomy.ts`: Classifies clips by hook type and tone based on content analysis.\n-   `selection/clipSelector.ts`: MMR diversity selection to prevent duplicate clips.\n-   `selection/finalizeRanking.ts`: Selects best clips with duration balancing (33% quota for 120s clips).\n-   `s3.ts`: Provides S3 object storage functionalities.\n-   `queue.ts`: Configures BullMQ for job orchestration.\n\n### UI/UX Decisions\nThe application uses Next.js with TailwindCSS to provide a modern, dark-themed interface, ensuring a consistent and visually appealing user experience.\n\n### System Design Choices\n-   **Asynchronous Processing**: Utilizes BullMQ and Redis for robust, scalable background job processing.\n-   **Modular Architecture**: Services are clearly separated for maintainability.\n-   **Prisma ORM**: Simplifies database interactions and schema management.\n-   **Global Static Framing**: Two-tier person detection (face-api.js primary, pose-detection fallback) that samples frames across the ENTIRE video, calculates ONE global median position, and applies the same static crop to ALL clips from that video. This ensures consistent framing across all timestamps (1:00, 2:00, 3:00 look identical). Global crop is computed once per video, stored in database, and reused for every clip. Uses correct median calculation (averages two middle values for even-length arrays) to handle balanced multi-person scenarios. Generates person-centered 9:16 crops with z_min = max(targetW/baseW, 1.0) ensuring valid FFmpeg dimensions. For 16:9 videos, z_min = 1.0 prevents zoom-out attempts.\n-   **Robust Cancellation**: Implements frequent cancellation checks throughout the worker process to prevent resource waste.\n-   **Two-Stage AI Scoring**: Cost-optimized approach that filters segments with rule-based features before GPT-4o scoring, capping AI calls at 25 per video regardless of segment count.\n-   **YouTube Cookie Authentication**: Uses browser cookie export for yt-dlp video downloads (OAuth deprecated by yt-dlp as of 2024). Cookies are uploaded via UI and automatically tracked for expiration (21-day warning threshold).\n-   **YouTube API OAuth**: Uses Replit's YouTube connector with OAuth 2.0 for YouTube Data API access (comment fetching, metadata).\n-   **TikTok Integration**: Supports OAuth-based TikTok publishing with encrypted token management and automatic refresh.\n\n### Static Framing Configuration\nStatic person-centered framing is configured via environment variables (see `.env.example`):\n\n**Face & Pose Detection:**\n- `FACE_CONF=0.5` - Face detection confidence threshold (0-1)\n- `POSE_CONF=0.3` - Pose detection confidence threshold (0-1)\n- `MIN_DET_AREA=100` - Min detection area in pixels\n- `FRAMING_SAMPLE_FPS=12` - Frame extraction rate for analysis\n\n**Global Crop Sampling:**\n- Samples 15-second windows at 30-second intervals across entire video\n- Aggregates all detections from all samples (continues even if some samples fail)\n- Requires minimum 50 total detections across video to succeed\n- Computes median position from all aggregated detections for optimal centering\n\n## External Dependencies\n\n**Database:**\n-   **PostgreSQL**: Primary data store, managed by Prisma ORM.\n\n**Queue System:**\n-   **Redis**: Used by BullMQ for managing asynchronous job queues.\n\n**AI Services:**\n-   **OpenAI Whisper API**: For accurate audio transcription and universal language detection.\n-   **GPT-4o**: Utilized for intelligent clip scoring, categorization, and rationale generation with TikTok-specific criteria.\n\n**Video Processing Libraries:**\n-   **ffmpeg**: The core video and audio manipulation tool.\n-   **yt-dlp**: For downloading YouTube videos and extracting metadata.\n\n**Object Storage:**\n-   **S3-compatible storage**: Used for storing generated video clips, thumbnails, and SRT files (AWS SDK v3).\n\n**Person Tracking & AI Models:**\n-   **@vladmandic/face-api**: Production-ready face detection library using SSD MobileNet v1 for primary person detection.\n-   **@tensorflow-models/pose-detection**: MoveNet-based pose estimation for full-body detection fallback when faces are not visible.\n-   **@tensorflow/tfjs-node**: TensorFlow backend for image preprocessing and tensor operations.\n\n**Social Media Integration:**\n-   **TikTok API**: For OAuth-based video publishing and user authentication.\n\n**YouTube Integration:**\n-   **Replit YouTube Connector**: OAuth 2.0 authentication for yt-dlp video downloads with automatic token management and refresh.\n-   **YouTube Data API v3**: For fetching comments and mining timestamp hotspots for engagement scoring.","size_bytes":10369},"app/api/videos/[id]/cancel/route.ts":{"content":"import { NextRequest, NextResponse } from 'next/server'\nimport { requireAuth } from '@/src/lib/session'\nimport { prisma } from '@/src/lib/prisma'\nimport { videoQueue } from '@/src/lib/queue'\n\nexport async function POST(\n  request: NextRequest,\n  { params }: { params: { id: string } }\n) {\n  try {\n    const session = await requireAuth()\n    const { id } = params\n    \n    const video = await prisma.video.findUnique({\n      where: { id, userId: session.userId }\n    })\n    \n    if (!video)\n    {\n      return NextResponse.json({ error: 'Video not found' }, { status: 404 })\n    }\n    \n    if (video.status !== 'processing')\n    {\n      return NextResponse.json({ error: 'Video is not processing' }, { status: 400 })\n    }\n    \n    const job = await videoQueue.getJob(id)\n    \n    if (job)\n    {\n      await job.remove()\n    }\n    \n    await prisma.video.update({\n      where: { id },\n      data: { status: 'cancelled' }\n    })\n    \n    return NextResponse.json({ success: true })\n  }\n  catch (error) {\n    console.error('Error cancelling video:', error)\n    return NextResponse.json({ error: 'Failed to cancel video' }, { status: 500 })\n  }\n}\n","size_bytes":1143},"src/services/s3.ts":{"content":"import { S3Client, PutObjectCommand, GetObjectCommand } from '@aws-sdk/client-s3'\nimport { getSignedUrl } from '@aws-sdk/s3-request-presigner'\nimport { readFileSync } from 'fs'\n\nconst s3Client = new S3Client({\n  endpoint: process.env.S3_ENDPOINT,\n  region: process.env.S3_REGION || 'auto',\n  credentials: process.env.S3_ACCESS_KEY_ID && process.env.S3_SECRET_ACCESS_KEY ? {\n    accessKeyId: process.env.S3_ACCESS_KEY_ID,\n    secretAccessKey: process.env.S3_SECRET_ACCESS_KEY\n  } : undefined\n})\n\nconst bucket = process.env.S3_BUCKET!\n\nexport async function uploadFile(key: string, filePath: string, contentType: string): Promise<string> {\n  const fileContent = readFileSync(filePath)\n  \n  await s3Client.send(new PutObjectCommand({\n    Bucket: bucket,\n    Key: key,\n    Body: fileContent,\n    ContentType: contentType,\n    ContentDisposition: 'inline'\n  }))\n  \n  if (process.env.S3_ENDPOINT)\n  {\n    return `${process.env.S3_ENDPOINT}/${bucket}/${key}`\n  }\n  return `https://${bucket}.s3.${process.env.S3_REGION}.amazonaws.com/${key}`\n}\n\nexport async function getSignedUrlForKey(key: string, expiresIn: number = 3600): Promise<string> {\n  const command = new GetObjectCommand({\n    Bucket: bucket,\n    Key: key\n  })\n  \n  return await getSignedUrl(s3Client, command, { expiresIn })\n}\n\nexport async function uploadBuffer(key: string, buffer: Buffer, contentType: string): Promise<string> {\n  await s3Client.send(new PutObjectCommand({\n    Bucket: bucket,\n    Key: key,\n    Body: buffer,\n    ContentType: contentType,\n    ContentDisposition: 'inline'\n  }))\n  \n  if (process.env.S3_ENDPOINT)\n  {\n    return `${process.env.S3_ENDPOINT}/${bucket}/${key}`\n  }\n  return `https://${bucket}.s3.${process.env.S3_REGION}.amazonaws.com/${key}`\n}\n\nexport function getS3Url(key: string): string {\n  if (process.env.S3_ENDPOINT)\n  {\n    return `${process.env.S3_ENDPOINT}/${bucket}/${key}`\n  }\n  return `https://${bucket}.s3.${process.env.S3_REGION}.amazonaws.com/${key}`\n}\n","size_bytes":1957},"app/api/submit/route.ts":{"content":"import { NextRequest, NextResponse } from 'next/server'\nimport { prisma } from '@/src/lib/prisma'\nimport { videoQueue } from '@/src/lib/queue'\nimport { getVideoMetadata } from '@/src/services/youtube'\nimport { requireAuth } from '@/src/lib/session'\n\nfunction sanitizeUrl(url: string): string {\n  const trimmed = url.trim()\n  const urlPattern = /^(https?:\\/\\/)?(www\\.)?(youtube\\.com|youtu\\.be)\\/.+$/i\n  \n  if (!urlPattern.test(trimmed))\n  {\n    throw new Error('Invalid YouTube URL')\n  }\n  \n  return trimmed\n}\n\nexport async function POST(request: NextRequest) {\n  try {\n    const session = await requireAuth()\n    \n    if (!session.userId)\n    {\n      return NextResponse.json(\n        { error: 'User ID not found in session' },\n        { status: 401 }\n      )\n    }\n    \n    const body = await request.json()\n    const { url } = body\n    \n    if (!url)\n    {\n      return NextResponse.json(\n        { error: 'URL is required' },\n        { status: 400 }\n      )\n    }\n    \n    const sanitizedUrl = sanitizeUrl(url)\n    \n    const metadata = await getVideoMetadata(sanitizedUrl, session.userId)\n    \n    const video = await prisma.video.create({\n      data: {\n        userId: session.userId,\n        sourceUrl: sanitizedUrl,\n        title: metadata.title,\n        durationSec: metadata.duration,\n        status: 'queued'\n      }\n    })\n    \n    await videoQueue.add('process', {\n      videoId: video.id,\n      userId: session.userId\n    })\n    \n    return NextResponse.json({\n      videoId: video.id,\n      status: 'queued'\n    })\n  }\n  catch (error: any) {\n    console.error('Error submitting video:', error)\n    \n    if (error.message === 'Authentication required')\n    {\n      return NextResponse.json(\n        { error: 'Authentication required' },\n        { status: 401 }\n      )\n    }\n    \n    if (error.message === 'Invalid YouTube URL')\n    {\n      return NextResponse.json(\n        { error: 'Invalid YouTube URL' },\n        { status: 400 }\n      )\n    }\n    \n    return NextResponse.json(\n      { error: 'Failed to submit video' },\n      { status: 500 }\n    )\n  }\n}\n","size_bytes":2072},"src/lib/queue.ts":{"content":"import { Queue, QueueOptions } from 'bullmq'\nimport IORedis from 'ioredis'\n\nconst connection = new IORedis(process.env.REDIS_URL || 'redis://localhost:6379', {\n  maxRetriesPerRequest: null\n})\n\nconst queueOptions: QueueOptions = {\n  connection\n}\n\nexport const videoQueue = new Queue('video.process', queueOptions)\n\nexport { connection }\n","size_bytes":336},"app/page.tsx":{"content":"\"use client\";\n\nimport { useEffect, useMemo, useState } from \"react\";\nimport Link from \"next/link\";\nimport { useRouter } from \"next/navigation\";\nimport {\n  AppBar,\n  Toolbar,\n  Typography,\n  Box,\n  Container,\n  Stack,\n  Button,\n  Chip,\n  Paper,\n  TextField,\n  Table,\n  TableBody,\n  TableCell,\n  TableContainer,\n  TableHead,\n  TableRow,\n  Checkbox,\n  IconButton,\n  Menu,\n  MenuItem,\n  TablePagination,\n  Divider,\n  Alert,\n  CircularProgress,\n} from \"@mui/material\";\nimport MoreVertIcon from \"@mui/icons-material/MoreVert\";\n\ninterface Video {\n  id: string;\n  title: string;\n  sourceUrl: string;\n  status: string;\n  durationSec: number;\n  createdAt: string;\n  _count: { clips: number };\n}\n\nexport default function Home() {\n  const router = useRouter();\n  const [url, setUrl] = useState(\"\");\n  const [loading, setLoading] = useState(false);\n  const [videos, setVideos] = useState<Video[]>([]);\n  const [error, setError] = useState(\"\");\n  const [authStatus, setAuthStatus] = useState<any>(null);\n  const [connections, setConnections] = useState<any>(null);\n  const [deletingId, setDeletingId] = useState<string | null>(null);\n  const [selectedIds, setSelectedIds] = useState<string[]>([]);\n  const [deletingBatch, setDeletingBatch] = useState(false);\n  const [page, setPage] = useState(0);\n  const [rowsPerPage, setRowsPerPage] = useState(10);\n  const [total, setTotal] = useState(0);\n  const [menuAnchor, setMenuAnchor] = useState<null | HTMLElement>(null);\n  const [openMenuId, setOpenMenuId] = useState<string | null>(null);\n\n  useEffect(() => {\n    checkAuth();\n  }, []);\n\n  useEffect(() => {\n    if (authStatus && connections) {\n      fetchVideos(page + 1, rowsPerPage);\n    }\n  }, [page, rowsPerPage, authStatus, connections]);\n\n  async function checkAuth() {\n    try {\n      const res = await fetch(\"/api/auth/status\");\n      const data = await res.json();\n      setAuthStatus(data);\n      if (!data.isAuthenticated) {\n        router.push(\"/login\");\n        return;\n      }\n      const connectionsRes = await fetch(\"/api/me/connections\");\n      const connectionsData = await connectionsRes.json();\n      setConnections(connectionsData);\n      fetchVideos(1, rowsPerPage);\n    } catch {\n      router.push(\"/login\");\n    }\n  }\n\n  async function fetchVideos(p = 1, ps = 10) {\n    try {\n      const response = await fetch(`/api/videos?page=${p}&pageSize=${ps}`);\n      const data = await response.json();\n      if (response.ok) {\n        setVideos(data.videos);\n        setTotal(data.total);\n        setSelectedIds([]);\n        setOpenMenuId(null);\n        setMenuAnchor(null);\n      }\n    } catch {}\n  }\n\n  async function handleSubmit(e: React.FormEvent) {\n    e.preventDefault();\n    setLoading(true);\n    setError(\"\");\n    try {\n      const response = await fetch(\"/api/submit\", {\n        method: \"POST\",\n        headers: { \"Content-Type\": \"application/json\" },\n        body: JSON.stringify({ url }),\n      });\n      const data = await response.json();\n      if (response.ok) {\n        setUrl(\"\");\n        setPage(0);\n        fetchVideos(1, rowsPerPage);\n      } else {\n        setError(data.error || \"Failed to submit video\");\n      }\n    } catch {\n      setError(\"Failed to submit video\");\n    } finally {\n      setLoading(false);\n    }\n  }\n\n  async function handleLogout() {\n    try {\n      await fetch(\"/api/auth/logout\", { method: \"POST\" });\n      router.push(\"/login\");\n    } catch {}\n  }\n\n  async function handleCancel(videoId: string) {\n    const ok = window.confirm(\"Cancel this video processing?\");\n    if (ok) {\n      try {\n        const response = await fetch(`/api/videos/${videoId}/cancel`, {\n          method: \"POST\",\n        });\n        if (response.ok) {\n          fetchVideos(page + 1, rowsPerPage);\n        } else {\n          const data = await response.json();\n          alert(data.error || \"Failed to cancel video\");\n        }\n      } catch {\n        alert(\"Failed to cancel video\");\n      }\n    }\n  }\n\n  async function handleDelete(videoId: string) {\n    const ok = window.confirm(\"Delete this video and all associated clips?\");\n    if (ok) {\n      setDeletingId(videoId);\n      setError(\"\");\n      try {\n        const res = await fetch(`/api/videos/${videoId}`, { method: \"DELETE\" });\n        const data = await res.json().catch(() => ({}));\n        if (res.ok) {\n          const nextTotal = Math.max(0, total - 1);\n          const maxPage = Math.max(0, Math.ceil(nextTotal / rowsPerPage) - 1);\n          const nextPage = Math.min(page, maxPage);\n          setPage(nextPage);\n          fetchVideos(nextPage + 1, rowsPerPage);\n        } else {\n          setError(data.error || \"Failed to delete video\");\n        }\n      } catch {\n        setError(\"Failed to delete video\");\n      } finally {\n        setDeletingId(null);\n        setOpenMenuId(null);\n        setMenuAnchor(null);\n      }\n    }\n  }\n\n  function toggleOne(id: string) {\n    setSelectedIds((prev) => {\n      if (prev.includes(id)) {\n        return prev.filter((x) => x !== id);\n      }\n      return [...prev, id];\n    });\n  }\n\n  function toggleAll() {\n    if (selectedIds.length === videos.length) {\n      setSelectedIds([]);\n      return;\n    }\n    setSelectedIds(videos.map((v) => v.id));\n  }\n\n  async function handleBatchDelete() {\n    if (selectedIds.length === 0) {\n      return;\n    }\n    const ok = window.confirm(\n      `Delete ${selectedIds.length} video(s) and all associated clips?`,\n    );\n    if (ok) {\n      setDeletingBatch(true);\n      setError(\"\");\n      try {\n        const res = await fetch(`/api/videos/batch-delete`, {\n          method: \"DELETE\",\n          headers: { \"Content-Type\": \"application/json\" },\n          body: JSON.stringify({ ids: selectedIds }),\n        });\n        const data = await res.json().catch(() => ({}));\n        if (res.ok) {\n          const newTotal = Math.max(0, total - selectedIds.length);\n          const maxPage = Math.max(0, Math.ceil(newTotal / rowsPerPage) - 1);\n          const nextPage = Math.min(page, maxPage);\n          setPage(nextPage);\n          fetchVideos(nextPage + 1, rowsPerPage);\n        } else {\n          setError(data.error || \"Failed to delete selected videos\");\n        }\n      } catch {\n        setError(\"Failed to delete selected videos\");\n      } finally {\n        setDeletingBatch(false);\n        setOpenMenuId(null);\n        setMenuAnchor(null);\n      }\n    }\n  }\n\n  const pageCount = useMemo(\n    () => Math.max(1, Math.ceil(total / rowsPerPage)),\n    [total, rowsPerPage],\n  );\n  const loadingGate = !authStatus || !connections;\n\n  if (loadingGate) {\n    return (\n      <Box sx={{ minHeight: \"100vh\", display: \"grid\", placeItems: \"center\" }}>\n        <CircularProgress />\n      </Box>\n    );\n  }\n\n  return (\n    <Box minHeight=\"100vh\" display=\"flex\" flexDirection=\"column\">\n      <AppBar position=\"sticky\">\n        <Toolbar>\n          <Typography variant=\"h5\" sx={{ flexGrow: 1 }}>\n            YT Shortsmith\n          </Typography>\n          <Stack direction=\"row\" spacing={1} alignItems=\"center\">\n            <Chip\n              size=\"small\"\n              color={connections.hasYouTube ? \"success\" : \"error\"}\n              label={\n                connections.hasYouTube ? \"YouTube Connected\" : \"YouTube Disconnected\"\n              }\n            />\n            <Chip\n              size=\"small\"\n              color={connections.hasTikTok ? \"success\" : \"error\"}\n              label={\n                connections.hasTikTok\n                  ? \"TikTok Connected\"\n                  : \"TikTok Disconnected\"\n              }\n            />\n            <Button\n              href=\"/upload-cookies\"\n              color=\"inherit\"\n              variant=\"outlined\"\n            >\n              Manage Cookies\n            </Button>\n            <Button href=\"/api/tiktok/oauth/start\">\n              {connections.hasTikTok ? \"Reconnect TikTok\" : \"Connect TikTok\"}\n            </Button>\n            <Button onClick={handleLogout} color=\"inherit\">\n              Logout\n            </Button>\n          </Stack>\n        </Toolbar>\n      </AppBar>\n\n      <Container sx={{ py: 4, flex: 1 }}>\n        {!connections.hasYouTube && (\n          <Alert severity=\"warning\" sx={{ mb: 3 }}>\n            YouTube not connected. Please contact administrator to connect YouTube OAuth.\n          </Alert>\n        )}\n        \n        {!connections.hasCookies && (\n          <Alert severity=\"warning\" sx={{ mb: 3 }}>\n            YouTube cookies not uploaded. Video downloads may fail.{\" \"}\n            <Button\n              href=\"/upload-cookies\"\n              size=\"small\"\n              sx={{ ml: 1 }}\n            >\n              Upload Cookies\n            </Button>\n          </Alert>\n        )}\n        \n        {connections.hasCookies && connections.cookieAgeDays > 21 && (\n          <Alert severity=\"info\" sx={{ mb: 3 }}>\n            YouTube cookies are {connections.cookieAgeDays} days old and may be expired.{\" \"}\n            <Button\n              href=\"/upload-cookies\"\n              size=\"small\"\n              sx={{ ml: 1 }}\n            >\n              Refresh Cookies\n            </Button>\n          </Alert>\n        )}\n\n        <Paper sx={{ p: 3, mb: 4 }}>\n          <Typography variant=\"h6\" sx={{ mb: 2 }}>\n            Submit YouTube Video\n          </Typography>\n          <Box component=\"form\" onSubmit={handleSubmit} display=\"flex\" gap={2}>\n            <TextField\n              fullWidth\n              placeholder=\"Enter YouTube URL\"\n              value={url}\n              onChange={(e) => setUrl(e.target.value)}\n              disabled={loading || !connections.hasYouTube}\n            />\n            <Button\n              type=\"submit\"\n              disabled={loading || !url || !connections.hasYouTube}\n            >\n              {loading ? \"Submitting…\" : \"Submit\"}\n            </Button>\n          </Box>\n          {error && (\n            <Alert severity=\"error\" sx={{ mt: 2 }}>\n              {error}\n            </Alert>\n          )}\n        </Paper>\n\n        <Paper sx={{ p: 3 }}>\n          <Stack\n            direction=\"row\"\n            justifyContent=\"space-between\"\n            alignItems=\"center\"\n            sx={{ mb: 2 }}\n          >\n            <Typography variant=\"h6\">Videos</Typography>\n            <Stack direction=\"row\" spacing={2} alignItems=\"center\">\n              <Typography variant=\"body2\" color=\"text.secondary\">\n                {selectedIds.length} selected\n              </Typography>\n              <Button\n                color=\"error\"\n                disabled={selectedIds.length === 0 || deletingBatch}\n                onClick={handleBatchDelete}\n              >\n                {deletingBatch ? \"Deleting…\" : \"Delete Selected\"}\n              </Button>\n            </Stack>\n          </Stack>\n\n          <TableContainer>\n            <Table>\n              <TableHead>\n                <TableRow>\n                  <TableCell padding=\"checkbox\">\n                    <Checkbox\n                      checked={\n                        videos.length > 0 &&\n                        selectedIds.length === videos.length\n                      }\n                      indeterminate={\n                        selectedIds.length > 0 &&\n                        selectedIds.length < videos.length\n                      }\n                      onChange={toggleAll}\n                    />\n                  </TableCell>\n                  <TableCell>Title</TableCell>\n                  <TableCell>Status</TableCell>\n                  <TableCell>Clips</TableCell>\n                  <TableCell>Duration</TableCell>\n                  <TableCell>Created</TableCell>\n                  <TableCell align=\"right\">Actions</TableCell>\n                </TableRow>\n              </TableHead>\n              <TableBody>\n                {videos.map((video) => (\n                  <TableRow key={video.id} hover>\n                    <TableCell padding=\"checkbox\">\n                      <Checkbox\n                        checked={selectedIds.includes(video.id)}\n                        onChange={() => toggleOne(video.id)}\n                      />\n                    </TableCell>\n                    <TableCell>\n                      <Link href={`/videos/${video.id}`}>\n                        <Typography\n                          color=\"primary.main\"\n                          sx={{ cursor: \"pointer\" }}\n                        >\n                          {video.title}\n                        </Typography>\n                      </Link>\n                    </TableCell>\n                    <TableCell>\n                      <Chip\n                        size=\"small\"\n                        label={video.status}\n                        color={\n                          video.status === \"completed\"\n                            ? \"success\"\n                            : video.status === \"processing\"\n                              ? \"info\"\n                              : video.status === \"failed\"\n                                ? \"error\"\n                                : \"default\"\n                        }\n                      />\n                    </TableCell>\n                    <TableCell>{video._count.clips}</TableCell>\n                    <TableCell>{Math.floor(video.durationSec / 60)}m</TableCell>\n                    <TableCell>\n                      {new Date(video.createdAt).toLocaleDateString()}\n                    </TableCell>\n                    <TableCell align=\"right\">\n                      <IconButton\n                        onClick={(e) => {\n                          setOpenMenuId(video.id);\n                          setMenuAnchor(e.currentTarget);\n                        }}\n                      >\n                        <MoreVertIcon />\n                      </IconButton>\n                      <Menu\n                        anchorEl={menuAnchor}\n                        open={openMenuId === video.id}\n                        onClose={() => {\n                          setOpenMenuId(null);\n                          setMenuAnchor(null);\n                        }}\n                      >\n                        <MenuItem\n                          onClick={() => {\n                            setOpenMenuId(null);\n                            setMenuAnchor(null);\n                            router.push(`/videos/${video.id}`);\n                          }}\n                        >\n                          Open\n                        </MenuItem>\n                        {video.status === \"processing\" && (\n                          <MenuItem\n                            onClick={() => {\n                              setOpenMenuId(null);\n                              setMenuAnchor(null);\n                              handleCancel(video.id);\n                            }}\n                          >\n                            Cancel\n                          </MenuItem>\n                        )}\n                        <Divider />\n                        <MenuItem\n                          disabled={deletingId === video.id}\n                          onClick={() => {\n                            setOpenMenuId(null);\n                            setMenuAnchor(null);\n                            handleDelete(video.id);\n                          }}\n                          sx={{ color: \"error.main\" }}\n                        >\n                          {deletingId === video.id ? \"Deleting…\" : \"Delete\"}\n                        </MenuItem>\n                      </Menu>\n                    </TableCell>\n                  </TableRow>\n                ))}\n              </TableBody>\n            </Table>\n            {videos.length === 0 && (\n              <Box py={6} textAlign=\"center\" color=\"text.secondary\">\n                No videos yet. Submit a YouTube URL to get started.\n              </Box>\n            )}\n            <TablePagination\n              component=\"div\"\n              count={total}\n              page={page}\n              onPageChange={(_, newPage) => setPage(newPage)}\n              rowsPerPage={rowsPerPage}\n              onRowsPerPageChange={(e) => {\n                setRowsPerPage(parseInt(e.target.value, 10));\n                setPage(0);\n              }}\n              rowsPerPageOptions={[10, 25, 50, 100]}\n              labelDisplayedRows={({ page: p }) =>\n                `Page ${p + 1} of ${pageCount}`\n              }\n            />\n          </TableContainer>\n        </Paper>\n      </Container>\n    </Box>\n  );\n}\n","size_bytes":16334},"app/api/videos/route.ts":{"content":"import { NextRequest, NextResponse } from \"next/server\";\nimport { prisma } from \"@/src/lib/prisma\";\nimport { requireAuth } from \"@/src/lib/session\";\nimport { ensureAccessToken } from \"@/src/services/tiktok\";\nimport { decrypt } from \"@/src/lib/encryption\";\n\nfunction publicUrlForKey(key: string) {\n  const base = process.env.PUBLIC_ASSETS_BASE_URL || \"\";\n  if (base) {\n    return `${base.replace(/\\/$/, \"\")}/${key}`;\n  }\n  const endpoint = (process.env.S3_ENDPOINT || \"\").replace(/\\/$/, \"\");\n  const bucket = process.env.S3_BUCKET || \"\";\n  if (endpoint && bucket) {\n    return `${endpoint}/${bucket}/${key}`;\n  }\n  return \"\";\n}\n\nasync function fetchTikTokStatus(accessToken: string, publishId: string) {\n  const res = await fetch(\n    \"https://open.tiktokapis.com/v2/post/publish/status/fetch/\",\n    {\n      method: \"POST\",\n      headers: {\n        Authorization: `Bearer ${accessToken}`,\n        \"Content-Type\": \"application/json; charset=UTF-8\",\n      },\n      body: JSON.stringify({ publish_id: publishId }),\n    },\n  );\n  const data = await res.json().catch(() => null);\n  return { ok: res.ok, status: res.status, data };\n}\n\nexport async function GET(request: NextRequest) {\n  try {\n    const session = await requireAuth();\n    const { searchParams } = new URL(request.url);\n    const page = Math.max(1, parseInt(searchParams.get(\"page\") || \"1\", 10));\n    const pageSize = Math.min(\n      100,\n      Math.max(1, parseInt(searchParams.get(\"pageSize\") || \"10\", 10)),\n    );\n    const skip = (page - 1) * pageSize;\n\n    const [total, videos] = await Promise.all([\n      prisma.video.count({ where: { userId: session.userId } }),\n      prisma.video.findMany({\n        where: { userId: session.userId },\n        orderBy: { createdAt: \"desc\" },\n        skip,\n        take: pageSize,\n        include: { _count: { select: { clips: true } } },\n      }),\n    ]);\n\n    return NextResponse.json({ total, page, pageSize, videos });\n  } catch {\n    return NextResponse.json(\n      { error: \"Failed to fetch videos\" },\n      { status: 500 },\n    );\n  }\n}\n","size_bytes":2043},"src/lib/cleanup.ts":{"content":"import { existsSync, readdirSync, rmSync } from 'fs'\nimport { join } from 'path'\nimport { tmpdir } from 'os'\n\nexport function cleanupTempFiles(): void {\n  const tmp = tmpdir()\n  \n  const cookieDir = join(tmp, 'yt-cookies')\n  if (existsSync(cookieDir))\n  {\n    try {\n      for (const file of readdirSync(cookieDir))\n      {\n        const filePath = join(cookieDir, file)\n        try {\n          rmSync(filePath, { force: true })\n        }\n        catch (err) {\n          console.error(`Failed to remove cookie file ${file}:`, err)\n        }\n      }\n      console.log('Cleaned up temp cookie files')\n    }\n    catch (err) {\n      console.error('Failed to cleanup cookie directory:', err)\n    }\n  }\n  \n  try {\n    const files = readdirSync(tmp)\n    let cleaned = 0\n    for (const file of files)\n    {\n      if (file.match(/\\.(webm|mp4|m4a|part)$/))\n      {\n        const filePath = join(tmp, file)\n        try {\n          rmSync(filePath, { force: true })\n          cleaned++\n        }\n        catch (err) {\n          console.error(`Failed to remove temp file ${file}:`, err)\n        }\n      }\n    }\n    if (cleaned > 0)\n    {\n      console.log(`Cleaned up ${cleaned} temp video files`)\n    }\n  }\n  catch (err) {\n    console.error('Failed to cleanup temp directory:', err)\n  }\n}\n","size_bytes":1276},"src/services/framingService.ts":{"content":"import * as tf from \"@tensorflow/tfjs-node\";\nimport { detectPersons } from \"./detectors\";\nimport * as fs from \"fs\";\nimport * as path from \"path\";\nimport { execFile } from \"child_process\";\nimport { promisify } from \"util\";\nimport * as faceapi from \"@vladmandic/face-api\";\n\nconst execFileAsync = promisify(execFile);\n\nlet canvasInitialized = false;\n\nexport async function initializeCanvas(): Promise<void> {\n  if (canvasInitialized) {\n    return;\n  }\n  \n  try {\n    const { Canvas, Image, ImageData } = await import(\"canvas\");\n    faceapi.env.monkeyPatch({ Canvas: Canvas as any, Image: Image as any, ImageData: ImageData as any });\n    canvasInitialized = true;\n  }\n  catch (error) {\n    console.error(\"[Framing] Failed to initialize canvas:\", error);\n    throw error;\n  }\n}\n\nexport type TranscriptWord = {\n  t: number;\n  end: number;\n  text: string;\n  speaker?: string;\n};\n\nexport interface ComputeInput {\n  videoPath: string;\n  baseW: number;\n  baseH: number;\n  segStart: number;\n  segEnd: number;\n  transcript: TranscriptWord[];\n}\n\nexport interface Constraints {\n  margin: number;\n  maxPan: number;\n  easeMs: number;\n  centerBiasX: number;\n  centerBiasY: number;\n  safeTop: number;\n  safeBottom: number;\n}\n\nexport type CropKF = {\n  t: number;\n  x: number;\n  y: number;\n  w: number;\n  h: number;\n  z: number;\n  xs: number;\n  ys: number;\n};\n\ntype PersonDet = {\n  x: number;\n  y: number;\n  w: number;\n  h: number;\n  score: number;\n  id?: number;\n  detectorType?: \"face\" | \"pose\";\n};\n\ntype PersonSnapshot = {\n  t: number;\n  dets: PersonDet[];\n};\n\ntype Track = {\n  id: number;\n  x: number;\n  y: number;\n  w: number;\n  h: number;\n  score: number;\n  vx: number;\n  vy: number;\n  lastT: number;\n  age: number;\n  hits: number;\n  miss: number;\n};\n\ntype SpeakerTurn = {\n  start: number;\n  end: number;\n  label: string;\n};\n\ntype SmoothingState = {\n  smoothedX: number | null;\n  smoothedY: number | null;\n};\n\nexport interface FaceDetection {\n  x: number;\n  y: number;\n  width: number;\n  height: number;\n  centerX: number;\n  centerY: number;\n  score: number;\n  area: number;\n}\n\ninterface WeightedValue {\n  value: number;\n  weight: number;\n}\n\ninterface FrameDetections {\n  index: number;\n  time: number;\n  detections: FaceDetection[];\n}\n\ninterface CenterKeyframe {\n  time: number;\n  centerX: number;\n}\n\ninterface CropInfo {\n  cropX: number;\n  cropY: number;\n  cropWidth: number;\n  cropHeight: number;\n  videoWidth: number;\n  videoHeight: number;\n}\n\nlet modelsLoaded = false;\n\nexport async function initializeFaceDetection(): Promise<void> {\n  if (modelsLoaded) {\n    return;\n  }\n\n  try {\n    const modelsPath = path.join(process.cwd(), \"models\");\n\n    await fs.promises.mkdir(modelsPath, { recursive: true });\n\n    await faceapi.nets.tinyFaceDetector.loadFromDisk(modelsPath);\n\n    modelsLoaded = true;\n    console.log(\"Face detection models loaded successfully\");\n  }\n  catch (error) {\n    console.error(\"Error loading face detection models:\", error);\n    throw new Error(\"Failed to initialize face detection models\");\n  }\n}\n\nexport async function detectFacesInImage(\n  imagePath: string,\n  videoWidth?: number,\n  videoHeight?: number,\n): Promise<FaceDetection[]> {\n  if (!modelsLoaded) {\n    throw new Error(\"Face detection models not loaded\");\n  }\n\n  try {\n    await initializeCanvas();\n    const { loadImage } = await import(\"canvas\");\n    const img = await loadImage(imagePath);\n\n    const frameWidth = img.width;\n    const frameHeight = img.height;\n\n    const targetWidth = videoWidth ?? frameWidth;\n    const targetHeight = videoHeight ?? frameHeight;\n\n    const scaleX = targetWidth / frameWidth;\n    const scaleY = targetHeight / frameHeight;\n\n    const detections = await faceapi.detectAllFaces(\n      img as any,\n      new faceapi.TinyFaceDetectorOptions({\n        inputSize: 512,\n        scoreThreshold: 0.2,\n      }),\n    );\n\n    const minWidth = targetWidth * 0.035;\n    const minHeight = targetHeight * 0.035;\n\n    const result: FaceDetection[] = [];\n\n    for (const detection of detections) {\n      const box = detection.box;\n\n      const rawX = box.x;\n      const rawY = box.y;\n      const rawW = box.width;\n      const rawH = box.height;\n\n      const x = rawX * scaleX;\n      const y = rawY * scaleY;\n      const width = rawW * scaleX;\n      const height = rawH * scaleY;\n\n      const centerX = x + width / 2;\n      const centerY = y + height / 2;\n      const area = width * height;\n\n      if (width < minWidth || height < minHeight) {\n        continue;\n      }\n\n      if (centerY < targetHeight * 0.15 || centerY > targetHeight * 0.95) {\n        continue;\n      }\n\n      result.push({\n        x,\n        y,\n        width,\n        height,\n        centerX,\n        centerY,\n        score: detection.score,\n        area,\n      });\n    }\n\n    return result;\n  }\n  catch (error: any) {\n    const fileName = path.basename(imagePath);\n    console.warn(\n      `Failed to load or detect faces in frame ${fileName}: ${error.message}`,\n    );\n    return [];\n  }\n}\n\nfunction weightedMedian(values: WeightedValue[]): number {\n  if (values.length === 0) {\n    return 0;\n  }\n\n  const sorted = [...values].sort((a, b) => {\n    if (a.value < b.value) {\n      return -1;\n    }\n    if (a.value > b.value) {\n      return 1;\n    }\n    return 0;\n  });\n\n  const totalWeight = sorted.reduce((sum, item) => sum + item.weight, 0);\n  const half = totalWeight / 2;\n\n  let acc = 0;\n  for (const item of sorted) {\n    acc += item.weight;\n    if (acc >= half) {\n      return item.value;\n    }\n  }\n\n  return sorted[sorted.length - 1].value;\n}\n\nfunction getFrameCenterX(\n  detections: FaceDetection[],\n  videoWidth: number,\n): number {\n  const videoCenterX = videoWidth / 2;\n\n  if (detections.length === 0) {\n    return videoCenterX;\n  }\n\n  const sorted = [...detections].sort((a, b) => b.area - a.area);\n\n  const top1 = sorted[0];\n  const top2 = sorted[1];\n\n  if (!top2 || top2.area < top1.area * 0.55) {\n    return top1.centerX;\n  }\n\n  const distance = Math.abs(top2.centerX - top1.centerX);\n\n  if (distance > videoWidth * 0.25) {\n    return (top1.centerX + top2.centerX) / 2;\n  }\n\n  return top1.centerX;\n}\n\nfunction buildCenterTimeline(\n  resolution: { width: number; height: number },\n  frames: FrameDetections[],\n  startSeconds: number,\n  durationSeconds: number,\n): CenterKeyframe[] {\n  const { width: videoWidth } = resolution;\n  const videoCenterX = videoWidth / 2;\n\n  if (frames.length === 0) {\n    return [\n      {\n        time: startSeconds,\n        centerX: videoCenterX,\n      },\n    ];\n  }\n\n  const raw: CenterKeyframe[] = frames.map((f) => ({\n    time: f.time,\n    centerX: getFrameCenterX(f.detections, videoWidth),\n  }));\n\n  const smoothed: CenterKeyframe[] = [];\n\n  for (let i = 0; i < raw.length; i++) {\n    let sum = 0;\n    let weight = 0;\n\n    for (let j = i - 1; j <= i + 1; j++) {\n      if (j < 0 || j >= raw.length) continue;\n      const w = j === i ? 2 : 1;\n      sum += raw[j].centerX * w;\n      weight += w;\n    }\n\n    smoothed.push({\n      time: raw[i].time,\n      centerX: sum / weight,\n    });\n  }\n\n  return smoothed;\n}\n\nfunction calculateCrop(\n  resolution: { width: number; height: number },\n  detections: FaceDetection[],\n): CropInfo {\n  const { width: videoWidth, height: videoHeight } = resolution;\n\n  const STRONG_MIN_AREA_RATIO = 0.008;\n  const STRONG_MIN_SCORE = 0.25;\n  const TWO_SPEAKERS_MIN_SIDE_RATIO = 0.18;\n  const VIDEO_CENTER_BLEND = 0.1;\n  const MAX_SHIFT_FROM_CENTER_RATIO = 0.22;\n\n  const maxCropHeight = videoHeight;\n  let maxCropWidth = Math.round((maxCropHeight * 9) / 16);\n\n  if (maxCropWidth > videoWidth) {\n    maxCropWidth = videoWidth;\n  }\n\n  if (detections.length === 0) {\n    const centerCropX = Math.max(\n      0,\n      Math.floor((videoWidth - maxCropWidth) / 2),\n    );\n    return {\n      cropX: centerCropX,\n      cropY: 0,\n      cropWidth: maxCropWidth,\n      cropHeight: maxCropHeight,\n      videoWidth,\n      videoHeight,\n    };\n  }\n\n  const frameArea = videoWidth * videoHeight;\n  const strongDetections = detections.filter((d) => {\n    return (\n      d.score >= STRONG_MIN_SCORE && d.area >= STRONG_MIN_AREA_RATIO * frameArea\n    );\n  });\n\n  const usedDetections =\n    strongDetections.length >= 8 ? strongDetections : detections;\n\n  let facesLeft = Infinity;\n  let facesRight = -Infinity;\n  let facesTop = Infinity;\n  let facesBottom = -Infinity;\n\n  const centerXValues: WeightedValue[] = [];\n  const centerYValues: WeightedValue[] = [];\n\n  for (const d of usedDetections) {\n    const right = d.x + d.width;\n    const bottom = d.y + d.height;\n\n    if (d.x < facesLeft) facesLeft = d.x;\n    if (right > facesRight) facesRight = right;\n    if (d.y < facesTop) facesTop = d.y;\n    if (bottom > facesBottom) facesBottom = bottom;\n\n    const w = Math.max(d.area, 1);\n    centerXValues.push({ value: d.centerX, weight: w });\n    centerYValues.push({ value: d.centerY, weight: w });\n  }\n\n  const facesWidth = facesRight - facesLeft;\n  const facesHeight = facesBottom - facesTop;\n\n  const totalWeight = centerXValues.reduce((sum, v) => sum + v.weight, 0);\n  const groupCenterX = (facesLeft + facesRight) / 2;\n  const medianCenterX = weightedMedian(centerXValues);\n\n  const leftCluster = centerXValues.filter((v) => v.value <= groupCenterX);\n  const rightCluster = centerXValues.filter((v) => v.value > groupCenterX);\n\n  const leftWeight = leftCluster.reduce((sum, v) => sum + v.weight, 0);\n  const rightWeight = rightCluster.reduce((sum, v) => sum + v.weight, 0);\n\n  const hasTwoSides =\n    totalWeight > 0 &&\n    leftWeight / totalWeight >= TWO_SPEAKERS_MIN_SIDE_RATIO &&\n    rightWeight / totalWeight >= TWO_SPEAKERS_MIN_SIDE_RATIO;\n\n  let centerFromFaces = medianCenterX;\n  let anchorsCenterX = groupCenterX;\n\n  if (hasTwoSides && leftCluster.length > 0 && rightCluster.length > 0) {\n    const leftMedian = weightedMedian(leftCluster);\n    const rightMedian = weightedMedian(rightCluster);\n    anchorsCenterX = (leftMedian + rightMedian) / 2;\n    centerFromFaces = anchorsCenterX * 0.8 + medianCenterX * 0.2;\n  }\n  else {\n    centerFromFaces = groupCenterX * 0.6 + medianCenterX * 0.4;\n  }\n\n  const videoCenterX = videoWidth / 2;\n  let centerX =\n    centerFromFaces * (1 - VIDEO_CENTER_BLEND) +\n    videoCenterX * VIDEO_CENTER_BLEND;\n\n  const maxShift = videoWidth * MAX_SHIFT_FROM_CENTER_RATIO;\n  const minCenter = videoCenterX - maxShift;\n  const maxCenter = videoCenterX + maxShift;\n\n  if (centerX < minCenter) centerX = minCenter;\n  if (centerX > maxCenter) centerX = maxCenter;\n\n  const cropHeight = maxCropHeight;\n  const cropWidth = maxCropWidth;\n\n  let cropX = Math.round(centerX - cropWidth / 2);\n  let cropY = 0;\n\n  if (cropX < 0) cropX = 0;\n  if (cropX + cropWidth > videoWidth) cropX = videoWidth - cropWidth;\n  if (cropY < 0) cropY = 0;\n  if (cropY + cropHeight > videoHeight) cropY = videoHeight - cropHeight;\n\n  console.log(\"[framing]\", {\n    totalDetections: detections.length,\n    usedDetections: usedDetections.length,\n    facesLeft,\n    facesRight,\n    facesTop,\n    facesBottom,\n    facesWidth,\n    facesHeight,\n    totalWeight,\n    leftWeight,\n    rightWeight,\n    hasTwoSides,\n    groupCenterX,\n    medianCenterX,\n    anchorsCenterX,\n    centerFromFaces,\n    videoCenterX,\n    centerX,\n    cropX,\n    cropY,\n    cropWidth,\n    cropHeight,\n    videoWidth,\n    videoHeight,\n  });\n\n  return {\n    cropX,\n    cropY,\n    cropWidth,\n    cropHeight,\n    videoWidth,\n    videoHeight,\n  };\n}\n\nconst DETECT_EVERY = Math.max(1, Number(process.env.PERSON_DETECT_EVERY || 3));\nconst DEFAULT_SAMPLE_FPS = Math.max(\n  1,\n  Number(process.env.FRAMING_SAMPLE_FPS || 12),\n);\nconst PERSON_DETECT_WIDTH = Number(process.env.PERSON_DETECT_WIDTH || -1);\nconst TRACK_IOU_THRESH = Math.max(\n  0,\n  Math.min(1, Number(process.env.TRACK_IOU_THRESH || 0.35)),\n);\nconst TRACK_MAX_AGE_S = Math.max(\n  0.1,\n  Number(process.env.TRACK_MAX_AGE_S || 0.8),\n);\nconst TRACK_MIN_HITS = Math.max(1, Number(process.env.TRACK_MIN_HITS || 2));\nconst MIN_TRACK_AREA = Math.max(1, Number(process.env.TRACK_MIN_AREA || 300));\nconst MIN_DET_AREA = Math.max(1, Number(process.env.MIN_DET_AREA || 100));\nconst SMOOTH_ALPHA = Math.max(\n  0.01,\n  Math.min(1, Number(process.env.FRAMING_SMOOTH_ALPHA || 0.2)),\n);\nconst DEADZONE_X = Math.max(0, Number(process.env.FRAMING_DEADZONE_X || 70));\nconst DEADZONE_Y = Math.max(0, Number(process.env.FRAMING_DEADZONE_Y || 50));\nconst MAX_ACCEL = Math.max(0, Number(process.env.FRAMING_MAX_ACCEL || 900));\nconst GLOB_LAMBDA_V = Math.max(\n  0,\n  Number(process.env.FRAMING_GLOB_LAMBDA_V || 80),\n);\nconst GLOB_LAMBDA_A = Math.max(\n  0,\n  Number(process.env.FRAMING_GLOB_LAMBDA_A || 500),\n);\nconst GLOB_ITERS = Math.max(1, Number(process.env.FRAMING_GLOB_ITERS || 80));\nconst GLOB_LR = Math.max(0.001, Number(process.env.FRAMING_GLOB_LR || 0.15));\nconst Z_MIN = Math.max(\n  0.8,\n  Math.min(1, Number(process.env.FRAMING_Z_MIN || 0.88)),\n);\nconst Z_DECAY = Math.max(\n  0.01,\n  Math.min(1, Number(process.env.FRAMING_Z_DECAY || 0.2)),\n);\n\n/*export async function computeCropMapPersonStatic(\n  input: ComputeInput,\n  constraints: Constraints,\n  globalCrop?: { cropX: number; cropY: number; cropW: number; cropH: number; zMin: number } | null,\n): Promise<CropKF[] | null> {\n  if (globalCrop) {\n    console.log(\n      `[Framing Static] Using GLOBAL crop: ${globalCrop.cropW}x${globalCrop.cropH} @ (${globalCrop.cropX},${globalCrop.cropY})`\n    );\n\n    const kf: CropKF = {\n      t: input.segStart,\n      x: globalCrop.cropX,\n      y: globalCrop.cropY,\n      w: globalCrop.cropW,\n      h: globalCrop.cropH,\n      z: globalCrop.zMin,\n      xs: 0,\n      ys: 0,\n    };\n\n    return [kf];\n  }\n\n  const snaps = await detectPersonsTimeline(\n    input.videoPath,\n    input.segStart,\n    input.segEnd,\n    input.baseW,\n    input.baseH,\n  );\n  if (snaps.length === 0) {\n    return null;\n  }\n\n  const allCentersX: number[] = [];\n  const allCentersY: number[] = [];\n\n  for (const snap of snaps) {\n    for (const det of snap.dets) {\n      const centerX = det.x + det.w / 2;\n      const centerY = det.y + det.h / 2;\n      allCentersX.push(centerX);\n      allCentersY.push(centerY);\n    }\n  }\n\n  if (allCentersX.length === 0) {\n    return null;\n  }\n\n  allCentersX.sort((a, b) => a - b);\n  allCentersY.sort((a, b) => a - b);\n\n  const isEvenX = allCentersX.length % 2 === 0;\n  const isEvenY = allCentersY.length % 2 === 0;\n\n  const medianX = isEvenX\n    ? (allCentersX[allCentersX.length / 2 - 1] + allCentersX[allCentersX.length / 2]) / 2\n    : allCentersX[Math.floor(allCentersX.length / 2)];\n\n  const medianY = isEvenY\n    ? (allCentersY[allCentersY.length / 2 - 1] + allCentersY[allCentersY.length / 2]) / 2\n    : allCentersY[Math.floor(allCentersY.length / 2)];\n\n  const targetW = Math.min(Math.floor((input.baseH * 9) / 16), input.baseW);\n  const evenTargetW = Math.floor(targetW / 2) * 2;\n  const targetH = input.baseH;\n  const zMin = Math.max(evenTargetW / input.baseW, 1.0);\n\n  const cropW = evenTargetW;\n  const cropH = targetH;\n  \n  const idealCropX = Math.round(medianX - cropW / 2);\n  const clampedCropX = Math.max(0, Math.min(input.baseW - cropW, idealCropX));\n  const cropX = clampedCropX;\n  \n  const idealCropY = Math.round(medianY - cropH / 2);\n  const clampedCropY = Math.max(0, Math.min(input.baseH - cropH, idealCropY));\n  const cropY = clampedCropY;\n\n  const personOffsetInCrop = medianX - cropX;\n  const centerOffset = cropW / 2;\n  const isPersonCentered = Math.abs(personOffsetInCrop - centerOffset) < 10;\n\n  console.log(\n    `[Framing Static] Video: ${input.baseW}x${input.baseH}, Crop: ${cropW}x${cropH} @ (${cropX},${cropY})`\n  );\n  console.log(\n    `[Framing Static] Detections: ${allCentersX.length}, Median: (${Math.round(medianX)},${Math.round(medianY)})`\n  );\n  console.log(\n    `[Framing Static] Ideal cropX: ${idealCropX}, Clamped: ${clampedCropX}, Person offset in crop: ${Math.round(personOffsetInCrop)}/${Math.round(centerOffset)} ${isPersonCentered ? '✓ centered' : '✗ OFF-CENTER'}`\n  );\n  console.log(\n    `[Framing Static] z_min: ${zMin.toFixed(2)}, Crop range: X[${cropX}-${cropX + cropW}] Y[${cropY}-${cropY + cropH}]`\n  );\n\n  const kf: CropKF = {\n    t: input.segStart,\n    x: cropX,\n    y: cropY,\n    w: evenTargetW,\n    h: targetH,\n    z: zMin,\n    xs: 0,\n    ys: 0,\n  };\n\n  return [kf];\n}*/\nexport async function computeCropMapPersonStatic(\n  input: ComputeInput,\n  constraints: Constraints,\n  globalCrop?: {\n    cropX: number;\n    cropY: number;\n    cropW: number;\n    cropH: number;\n    zMin: number;\n  } | null,\n): Promise<CropKF[] | null> {\n  if (globalCrop) {\n    console.log(\n      `[Framing Static] Using GLOBAL crop: ${globalCrop.cropW}x${globalCrop.cropH} @ (${globalCrop.cropX},${globalCrop.cropY})`,\n    );\n\n    const kf: CropKF = {\n      t: input.segStart,\n      x: globalCrop.cropX,\n      y: globalCrop.cropY,\n      w: globalCrop.cropW,\n      h: globalCrop.cropH,\n      z: globalCrop.zMin,\n      xs: 0,\n      ys: 0,\n    };\n\n    return [kf];\n  }\n\n  const snaps = await detectPersonsTimeline(\n    input.videoPath,\n    input.segStart,\n    input.segEnd,\n    input.baseW,\n    input.baseH,\n  );\n\n  if (snaps.length === 0) {\n    return null;\n  }\n\n  const rawTargetW = Math.min(Math.floor((input.baseH * 9) / 16), input.baseW);\n  const targetW = Math.floor(rawTargetW / 2) * 2;\n  const evenTargetW = targetW;\n  const maxAnchorWidth = targetW * 0.9;\n\n  const anchorCentersX: number[] = [];\n  const anchorCentersY: number[] = [];\n  const anchorWidths: number[] = [];\n  const anchorHeights: number[] = [];\n  const pairedAnchors: { x: number; y: number; w: number; h: number }[] = [];\n\n  for (const snap of snaps) {\n    if (snap.dets.length === 0) {\n      continue;\n    }\n\n    const anchor = chooseAnchor(snap.dets, input.baseW, input.baseH);\n    const effectiveW = Math.min(anchor.w, maxAnchorWidth);\n    const cx = anchor.x + anchor.w / 2;\n    const cy = anchor.y + anchor.h / 2;\n    anchorCentersX.push(cx);\n    anchorCentersY.push(cy);\n    anchorWidths.push(effectiveW);\n    anchorHeights.push(anchor.h);\n    pairedAnchors.push({\n      x: anchor.x,\n      y: anchor.y,\n      w: effectiveW,\n      h: anchor.h,\n    });\n  }\n\n  if (anchorCentersX.length === 0) {\n    return null;\n  }\n\n  anchorCentersX.sort((a, b) => a - b);\n  anchorCentersY.sort((a, b) => a - b);\n  anchorWidths.sort((a, b) => a - b);\n  anchorHeights.sort((a, b) => a - b);\n\n  const medianCx = median(anchorCentersX);\n  const medianCy = median(anchorCentersY);\n  const p90Width = percentile(anchorWidths, 90);\n  const p90Height = percentile(anchorHeights, 90);\n  const zMinWidth = evenTargetW / input.baseW;\n  const zMinHeight = 1.0;\n  const zMin = Math.max(zMinWidth, zMinHeight, 0.88);\n\n  const frameCenterX = input.baseW / 2;\n  const direction = Math.sign(medianCx - frameCenterX);\n  //alterar aqui\n  const bias = evenTargetW * 0.35;\n\n  let biasedCenterX = medianCx;\n\n  if (direction !== 0) {\n    biasedCenterX = medianCx - direction * bias;\n  }\n\n  let cropXFinal = Math.round(biasedCenterX - evenTargetW / 2);\n  const maxX = input.baseW - evenTargetW;\n\n  if (cropXFinal < 0) {\n    cropXFinal = 0;\n  }\n\n  if (cropXFinal > maxX) {\n    cropXFinal = maxX;\n  }\n\n  console.log(\n    `[Framing Static] Segment framing: ${anchorCentersX.length} anchors, targetW: ${targetW}, medianCx: ${Math.round(medianCx)}, biasedCenterX: ${Math.round(biasedCenterX)}, cropXFinal: ${cropXFinal}, z: ${zMin.toFixed(2)}`,\n  );\n\n  const kf: CropKF = {\n    t: input.segStart,\n    x: cropXFinal,\n    y: 0,\n    w: evenTargetW,\n    h: input.baseH,\n    z: zMin,\n    xs: 0,\n    ys: 0,\n  };\n\n  return [kf];\n}\n\nfunction percentile(sortedValues: number[], p: number): number {\n  if (sortedValues.length === 0) {\n    return 0;\n  }\n  const index = Math.ceil((sortedValues.length * p) / 100) - 1;\n  return sortedValues[Math.max(0, Math.min(index, sortedValues.length - 1))];\n}\n\nfunction median(sortedValues: number[]): number {\n  if (sortedValues.length === 0) {\n    return 0;\n  }\n  const isEven = sortedValues.length % 2 === 0;\n  if (isEven) {\n    return (\n      (sortedValues[sortedValues.length / 2 - 1] +\n        sortedValues[sortedValues.length / 2]) /\n      2\n    );\n  }\n  return sortedValues[Math.floor(sortedValues.length / 2)];\n}\n\nasync function detectFacesTimeline(\n  videoPath: string,\n  resolution: { width: number; height: number },\n  startTime: number,\n  duration: number,\n  fps: number = DEFAULT_SAMPLE_FPS,\n): Promise<FrameDetections[]> {\n  const expectedFrames = Math.ceil(duration * fps);\n  const tempDir = path.join(process.cwd(), \"tmp\", `faces_${Date.now()}`);\n  fs.mkdirSync(tempDir, { recursive: true });\n\n  try {\n    await extractFrames(videoPath, startTime, duration, fps, tempDir);\n    const frameFiles = fs\n      .readdirSync(tempDir)\n      .filter((f) => f.endsWith(\".jpg\"))\n      .sort();\n\n    const out: FrameDetections[] = [];\n    const frameInterval = 1 / fps;\n\n    for (let i = 0; i < frameFiles.length && i < expectedFrames; i++) {\n      const framePath = path.join(tempDir, frameFiles[i]);\n      const t = startTime + i * frameInterval;\n      \n      try {\n        const detections = await detectFacesInImage(framePath, resolution.width, resolution.height);\n        const scoreFiltered = detections.filter((d) => d.score >= 0.3);\n        \n        if (scoreFiltered.length > 0) {\n          console.log(\n            `[Face Detection] Frame ${i}: Found ${scoreFiltered.length} faces (score ≥ 0.3), avg confidence: ${(scoreFiltered.reduce((s, d) => s + d.score, 0) / scoreFiltered.length).toFixed(3)}`,\n          );\n        }\n\n        out.push({\n          index: i,\n          time: t,\n          detections: scoreFiltered,\n        });\n      }\n      catch (err) {\n        console.warn(\n          `[Face Detection] Frame ${i}: Detection failed - ${err}`,\n        );\n        out.push({\n          index: i,\n          time: t,\n          detections: [],\n        });\n      }\n    }\n\n    return out;\n  }\n  finally {\n    if (fs.existsSync(tempDir)) {\n      fs.rmSync(tempDir, { recursive: true, force: true });\n    }\n  }\n}\n\nfunction isAnchorFullyInside(\n  anchorX: number,\n  anchorY: number,\n  anchorW: number,\n  anchorH: number,\n  cropX: number,\n  cropY: number,\n  cropW: number,\n  cropH: number,\n  margin: number,\n): boolean {\n  const anchorLeft = anchorX;\n  const anchorRight = anchorX + anchorW;\n  const anchorTop = anchorY;\n  const anchorBottom = anchorY + anchorH;\n\n  const cropLeft = cropX + margin;\n  const cropRight = cropX + cropW - margin;\n  const cropTop = cropY + margin;\n  const cropBottom = cropY + cropH - margin;\n\n  return (\n    anchorLeft >= cropLeft &&\n    anchorRight <= cropRight &&\n    anchorTop >= cropTop &&\n    anchorBottom <= cropBottom\n  );\n}\n\nexport async function computeGlobalStaticCrop(\n  videoPath: string,\n  videoDurationSec: number,\n  baseW: number,\n  baseH: number,\n  options?: { skipUntilSec?: number },\n): Promise<{\n  cropX: number;\n  cropY: number;\n  cropW: number;\n  cropH: number;\n  zMin: number;\n} | null> {\n  await initializeFaceDetection();\n  \n  const sampleDurationSec = 15;\n  const maxSamples = 6;\n\n  const skipUntil = Math.max(0, options?.skipUntilSec ?? 0);\n\n  const defaultStart = videoDurationSec * 0.1;\n  const defaultEnd = videoDurationSec * 0.9;\n\n  let usableStart = Math.max(skipUntil, defaultStart);\n  let usableEnd = defaultEnd;\n\n  if (usableStart > videoDurationSec - sampleDurationSec) {\n    usableStart = Math.max(0, videoDurationSec - sampleDurationSec);\n  }\n\n  if (usableEnd < usableStart + sampleDurationSec) {\n    usableEnd = Math.min(videoDurationSec, usableStart + sampleDurationSec);\n  }\n\n  const usableDuration = Math.max(sampleDurationSec, usableEnd - usableStart);\n\n  const numSamples = Math.min(\n    maxSamples,\n    Math.max(3, Math.floor(usableDuration / sampleDurationSec)),\n  );\n\n  const rawTargetW = Math.min(Math.floor((baseH * 9) / 16), baseW);\n  const targetW = Math.floor(rawTargetW / 2) * 2;\n\n  const allDetections: FaceDetection[] = [];\n  let successfulSamples = 0;\n  let failedSamples = 0;\n\n  console.log(\n    `[Global Crop] Sampling ${numSamples} intervals (${sampleDurationSec}s each) between ${usableStart.toFixed(1)}s and ${usableEnd.toFixed(1)}s using TinyFaceDetector`,\n  );\n\n  for (let i = 0; i < numSamples; i++) {\n    const t =\n      numSamples === 1\n        ? usableStart + (usableDuration - sampleDurationSec) / 2\n        : usableStart + (i * (usableDuration - sampleDurationSec)) / (numSamples - 1);\n\n    const sampleStart = Math.max(0, Math.min(t, videoDurationSec - sampleDurationSec));\n    const sampleEnd = Math.min(sampleStart + sampleDurationSec, videoDurationSec);\n    const beforeCount = allDetections.length;\n\n    try {\n      const frames = await detectFacesTimeline(\n        videoPath,\n        { width: baseW, height: baseH },\n        sampleStart,\n        Math.min(sampleEnd - sampleStart, sampleDurationSec),\n        DEFAULT_SAMPLE_FPS,\n      );\n\n      for (const frame of frames) {\n        for (const det of frame.detections) {\n          allDetections.push(det);\n        }\n      }\n\n      const detectionsInSample = allDetections.length - beforeCount;\n      if (detectionsInSample > 0) {\n        console.log(\n          `[Global Crop] Sample ${i + 1}/${numSamples} @ ${sampleStart}s-${sampleEnd}s: ${detectionsInSample} faces ✓`,\n        );\n        successfulSamples++;\n      }\n      else {\n        console.log(\n          `[Global Crop] Sample ${i + 1}/${numSamples} @ ${sampleStart}s-${sampleEnd}s: 0 faces`,\n        );\n        failedSamples++;\n      }\n    }\n    catch (err) {\n      console.error(\n        `[Global Crop] Sample ${i + 1}/${numSamples} @ ${sampleStart}s-${sampleEnd}s: ERROR - ${err}`,\n      );\n      failedSamples++;\n    }\n  }\n\n  const minDetectionsRequired = 50;\n  if (allDetections.length < minDetectionsRequired) {\n    console.log(\n      `[Global Crop] Insufficient faces: ${allDetections.length} found, ${minDetectionsRequired} required (${successfulSamples} successful samples)`,\n    );\n    return null;\n  }\n\n  console.log(\n    `[Global Crop] Aggregated ${allDetections.length} face detections, using two-speaker-aware crop calculation`,\n  );\n\n  const cropResult = calculateCrop(\n    { width: baseW, height: baseH },\n    allDetections,\n  );\n\n  const finalCropW = Math.floor(cropResult.cropWidth / 2) * 2;\n  const zMinWidth = finalCropW / baseW;\n  const zMinHeight = 1.0;\n  const zMin = Math.max(zMinWidth, zMinHeight, 0.88);\n\n  return {\n    cropX: cropResult.cropX,\n    cropY: cropResult.cropY,\n    cropW: finalCropW,\n    cropH: cropResult.cropHeight,\n    zMin,\n  };\n}\n\n/*\nexport async function computeGlobalStaticCrop(\n  videoPath: string,\n  videoDurationSec: number,\n  baseW: number,\n  baseH: number,\n): Promise<{ cropX: number; cropY: number; cropW: number; cropH: number; zMin: number } | null> {\n  const sampleIntervalSec = 30;\n  const sampleDurationSec = 15;\n  const numSamples = Math.max(3, Math.floor(videoDurationSec / sampleIntervalSec));\n  \n  const allCentersX: number[] = [];\n  const allCentersY: number[] = [];\n  \n  let successfulSamples = 0;\n  let failedSamples = 0;\n\n  console.log(`[Global Crop] Sampling ${numSamples} intervals (${sampleDurationSec}s each) across ${videoDurationSec}s video`);\n\n  for (let i = 0; i < numSamples; i++) {\n    const sampleStart = Math.floor((i * videoDurationSec) / numSamples);\n    const sampleEnd = Math.min(sampleStart + sampleDurationSec, videoDurationSec);\n    \n    const beforeCount = allCentersX.length;\n    \n    try {\n      const snaps = await detectPersonsTimeline(\n        videoPath,\n        sampleStart,\n        sampleEnd,\n        baseW,\n        baseH,\n      );\n\n      for (const snap of snaps) {\n        for (const det of snap.dets) {\n          const centerX = det.x + det.w / 2;\n          const centerY = det.y + det.h / 2;\n          allCentersX.push(centerX);\n          allCentersY.push(centerY);\n        }\n      }\n      \n      const detectionsInSample = allCentersX.length - beforeCount;\n      if (detectionsInSample > 0) {\n        console.log(`[Global Crop] Sample ${i + 1}/${numSamples} @ ${sampleStart}s-${sampleEnd}s: ${detectionsInSample} detections ✓`);\n        successfulSamples++;\n      } else {\n        console.log(`[Global Crop] Sample ${i + 1}/${numSamples} @ ${sampleStart}s-${sampleEnd}s: 0 detections (person may be off-screen)`);\n        failedSamples++;\n      }\n    } catch (err) {\n      console.error(`[Global Crop] Sample ${i + 1}/${numSamples} @ ${sampleStart}s-${sampleEnd}s: ERROR - ${err}`);\n      failedSamples++;\n    }\n  }\n\n  const minDetectionsRequired = 50;\n  \n  if (allCentersX.length < minDetectionsRequired) {\n    console.log(`[Global Crop] Insufficient detections: ${allCentersX.length} found, ${minDetectionsRequired} required (${successfulSamples} successful samples, ${failedSamples} failed)`);\n    return null;\n  }\n  \n  console.log(`[Global Crop] Aggregated ${allCentersX.length} detections from ${successfulSamples}/${numSamples} successful samples`);\n\n  if (allCentersX.length === 0) {\n    console.log(`[Global Crop] No detections found across video`);\n    return null;\n  }\n\n  allCentersX.sort((a, b) => a - b);\n  allCentersY.sort((a, b) => a - b);\n\n  const isEvenX = allCentersX.length % 2 === 0;\n  const isEvenY = allCentersY.length % 2 === 0;\n\n  const medianX = isEvenX\n    ? (allCentersX[allCentersX.length / 2 - 1] + allCentersX[allCentersX.length / 2]) / 2\n    : allCentersX[Math.floor(allCentersX.length / 2)];\n\n  const medianY = isEvenY\n    ? (allCentersY[allCentersY.length / 2 - 1] + allCentersY[allCentersY.length / 2]) / 2\n    : allCentersY[Math.floor(allCentersY.length / 2)];\n\n  const targetW = Math.min(Math.floor((baseH * 9) / 16), baseW);\n  const evenTargetW = Math.floor(targetW / 2) * 2;\n  const targetH = baseH;\n  const zMin = Math.max(evenTargetW / baseW, 1.0);\n\n  const cropW = evenTargetW;\n  const cropH = targetH;\n  \n  const idealCropX = Math.round(medianX - cropW / 2);\n  const clampedCropX = Math.max(0, Math.min(baseW - cropW, idealCropX));\n  const cropX = clampedCropX;\n  \n  const idealCropY = Math.round(medianY - cropH / 2);\n  const clampedCropY = Math.max(0, Math.min(baseH - cropH, idealCropY));\n  const cropY = clampedCropY;\n\n  console.log(\n    `[Global Crop] Analyzed ${allCentersX.length} detections from ${numSamples} samples`\n  );\n  console.log(\n    `[Global Crop] Median: (${Math.round(medianX)},${Math.round(medianY)}), Crop: ${cropW}x${cropH} @ (${cropX},${cropY})`\n  );\n\n  return {\n    cropX,\n    cropY,\n    cropW,\n    cropH,\n    zMin,\n  };\n}\n*/\nexport async function computeCropMapPerson(\n  input: ComputeInput,\n  constraints: Constraints,\n): Promise<CropKF[] | null> {\n  const snaps = await detectPersonsTimeline(\n    input.videoPath,\n    input.segStart,\n    input.segEnd,\n    input.baseW,\n    input.baseH,\n  );\n  if (snaps.length === 0) {\n    return null;\n  }\n  const tracksTimeline = buildTracks(snaps, input.baseW, input.baseH);\n  if (tracksTimeline.length === 0) {\n    return null;\n  }\n  const turns = buildSpeakerTurns(\n    input.transcript,\n    input.segStart,\n    input.segEnd,\n  );\n  const raw = computeGlobalCropKeyframesFromTracksWithSpeechAndZoom(\n    tracksTimeline,\n    turns,\n    input.baseW,\n    input.baseH,\n    constraints,\n  );\n  if (raw.length === 0) {\n    return null;\n  }\n  const smoothed = smoothKeyframes(raw);\n  const deadzoned = applyDeadzone(smoothed, DEADZONE_X, DEADZONE_Y);\n  const limited = applyPanLimits(deadzoned, constraints.maxPan);\n  const accelLimited = applyAccelLimits(limited, MAX_ACCEL);\n  const eased = easeSegmentEdges(\n    accelLimited,\n    input.segStart,\n    input.segEnd,\n    constraints.easeMs / 1000,\n  );\n  const withScaled = applyScaledCoords(eased, input.baseW, input.baseH);\n  const deduped = withScaled.length > 0 ? dedupeByTime(withScaled, 0.1) : [];\n\n  if (deduped.length === 0) {\n    return null;\n  }\n\n  const compressed = compressCropMap(deduped, 120);\n  const duration = deduped[deduped.length - 1].t - deduped[0].t;\n  console.log(\n    `[Framing] Keyframe compression: ${deduped.length} → ${compressed.length} (${((1 - compressed.length / deduped.length) * 100).toFixed(1)}% reduction, 1 KF every ${(duration / compressed.length).toFixed(1)}s)`,\n  );\n\n  return compressed;\n}\n\nexport function buildPiecewiseExpr(\n  kf: CropKF[],\n  key: \"x\" | \"y\" | \"xs\" | \"ys\",\n): string {\n  if (kf.length === 0) {\n    return \"0\";\n  }\n\n  const values = kf.map((k) => {\n    return key === \"x\" ? k.x : key === \"y\" ? k.y : key === \"xs\" ? k.xs : k.ys;\n  });\n\n  const minVal = Math.min(...values);\n  const maxVal = Math.max(...values);\n  const isConstant = maxVal - minVal < 1;\n\n  if (isConstant) {\n    return String(Math.round(values[0]));\n  }\n\n  const parts: string[] = [];\n  const first = values[0];\n  parts.push(`lt(t,${kf[0].t.toFixed(3)})*${Math.round(first)}`);\n  for (let i = 0; i < kf.length - 1; i++) {\n    const a = kf[i];\n    const b = kf[i + 1];\n    const ta = a.t;\n    const tb = b.t;\n    const va = values[i];\n    const vb = values[i + 1];\n    const slope = (vb - va) / Math.max(0.001, tb - ta);\n    parts.push(\n      `between(t,${ta.toFixed(3)},${tb.toFixed(3)})*(${Math.round(va)}+(${slope.toFixed(6)})*(t-${ta.toFixed(3)}))`,\n    );\n  }\n  const last = values[values.length - 1];\n  parts.push(`gte(t,${kf[kf.length - 1].t.toFixed(3)})*${Math.round(last)}`);\n  return parts.join(\"+\");\n}\n\nexport function buildPiecewiseExprZ(kf: CropKF[]): string {\n  if (kf.length === 0) {\n    return \"1\";\n  }\n\n  const zValues = kf.map((k) => {\n    return k.z;\n  });\n  const minZ = Math.min(...zValues);\n  const maxZ = Math.max(...zValues);\n  const isConstant = maxZ - minZ < 0.001;\n\n  if (isConstant) {\n    return zValues[0].toFixed(4);\n  }\n\n  const parts: string[] = [];\n  parts.push(`lt(t,${kf[0].t.toFixed(3)})*${kf[0].z.toFixed(4)}`);\n  for (let i = 0; i < kf.length - 1; i++) {\n    const a = kf[i];\n    const b = kf[i + 1];\n    const ta = a.t;\n    const tb = b.t;\n    const slope = (b.z - a.z) / Math.max(0.001, tb - ta);\n    parts.push(\n      `between(t,${ta.toFixed(3)},${tb.toFixed(3)})*(${a.z.toFixed(4)}+(${slope.toFixed(6)})*(t-${ta.toFixed(3)}))`,\n    );\n  }\n  parts.push(\n    `gte(t,${kf[kf.length - 1].t.toFixed(3)})*${kf[kf.length - 1].z.toFixed(4)}`,\n  );\n  return parts.join(\"+\");\n}\n\nfunction buildCropDimensionExpr(kf: CropKF[], baseDim: number): string {\n  if (kf.length === 0) {\n    return String(baseDim);\n  }\n\n  const dimensions = kf.map((k) => {\n    return Math.round(baseDim / k.z);\n  });\n  const minDim = Math.min(...dimensions);\n  const maxDim = Math.max(...dimensions);\n  const isConstant = maxDim - minDim < 1;\n\n  if (isConstant) {\n    return String(dimensions[0]);\n  }\n\n  const parts: string[] = [];\n  const firstDim = dimensions[0];\n  parts.push(`lt(t,${kf[0].t.toFixed(3)})*${firstDim}`);\n  for (let i = 0; i < kf.length - 1; i++) {\n    const a = kf[i];\n    const b = kf[i + 1];\n    const ta = a.t;\n    const tb = b.t;\n    const va = baseDim / a.z;\n    const vb = baseDim / b.z;\n    const slope = (vb - va) / Math.max(0.001, tb - ta);\n    parts.push(\n      `between(t,${ta.toFixed(3)},${tb.toFixed(3)})*(${Math.round(va)}+(${slope.toFixed(6)})*(t-${ta.toFixed(3)}))`,\n    );\n  }\n  const lastDim = dimensions[dimensions.length - 1];\n  parts.push(`gte(t,${kf[kf.length - 1].t.toFixed(3)})*${lastDim}`);\n  return parts.join(\"+\");\n}\n\nexport function buildFFmpegFilter(\n  baseW: number,\n  baseH: number,\n  kf: CropKF[],\n): string {\n  const targetW = Math.floor((baseH * 9) / 16);\n  const evenTargetW = Math.floor(targetW / 2) * 2;\n  const zMin = Math.max(evenTargetW / baseW, 1.0, 0.88);\n  let hasInvalidCrop = false;\n\n  for (let i = 0; i < Math.min(5, kf.length); i++) {\n    const k = kf[i];\n    const cropW = Math.round(evenTargetW / k.z);\n    const cropH = Math.round(baseH / k.z);\n\n    if (cropW > baseW || cropH > baseH) {\n      console.error(\n        `[Framing] ERROR: Crop dimensions exceed video at t=${k.t.toFixed(2)}s: crop=(${cropW}x${cropH}) > video=(${baseW}x${baseH}), z=${k.z.toFixed(3)} (z_min should be ${zMin.toFixed(3)})`,\n      );\n      hasInvalidCrop = true;\n    }\n\n    if (k.x < 0 || k.y < 0 || k.x + cropW > baseW || k.y + cropH > baseH) {\n      console.error(\n        `[Framing] ERROR: Invalid crop position at t=${k.t.toFixed(2)}s: pos=(${k.x},${k.y}) size=(${cropW}x${cropH}) video=(${baseW}x${baseH})`,\n      );\n      hasInvalidCrop = true;\n    }\n  }\n\n  if (hasInvalidCrop) {\n    throw new Error(\n      `[Framing] Invalid crop parameters detected. This indicates a bug in the zoom calculation.`,\n    );\n  }\n\n  const xExpr = buildPiecewiseExpr(kf, \"x\");\n  const yExpr = buildPiecewiseExpr(kf, \"y\");\n  const cropWExpr = buildCropDimensionExpr(kf, evenTargetW);\n  const cropHExpr = buildCropDimensionExpr(kf, baseH);\n\n  const isConstantX = !xExpr.includes(\"between\");\n  const isConstantY = !yExpr.includes(\"between\");\n  const isConstantW = !cropWExpr.includes(\"between\");\n  const isConstantH = !cropHExpr.includes(\"between\");\n  const constantCount = [\n    isConstantX,\n    isConstantY,\n    isConstantW,\n    isConstantH,\n  ].filter(Boolean).length;\n\n  if (constantCount > 0) {\n    console.log(\n      `[Framing] Static video optimization: ${constantCount}/4 expressions simplified to constants (saves ~${Math.round(kf.length * constantCount * 60)}B per constant)`,\n    );\n  }\n\n  const crop = `crop='${cropWExpr}':'${cropHExpr}':'${xExpr}':'${yExpr}'`;\n  const scale = `scale=${evenTargetW}:${baseH}:eval=frame`;\n  const fmt = `format=yuv420p`;\n\n  const fullFilter = [crop, scale, fmt].join(\",\");\n  const filterLength = fullFilter.length;\n\n  console.log(\n    `[Framing] Filter generated with ${kf.length} keyframes, zoom range: ${Math.min(...kf.map((k) => k.z)).toFixed(2)}-${Math.max(...kf.map((k) => k.z)).toFixed(2)}, expression length: ${(filterLength / 1024).toFixed(1)}KB`,\n  );\n\n  if (filterLength > 65536) {\n    throw new Error(\n      `[Framing] Filter expression is ${(filterLength / 1024).toFixed(1)}KB, exceeding FFmpeg limits (64KB). This indicates keyframe compression failed. Try reducing video duration or keyframe density.`,\n    );\n  }\n\n  if (filterLength > 32768) {\n    console.warn(\n      `[Framing] WARNING: Filter expression is ${(filterLength / 1024).toFixed(1)}KB, approaching FFmpeg limits. This may cause issues with some FFmpeg versions.`,\n    );\n  }\n\n  return fullFilter;\n}\n\nasync function detectPersonsTimeline(\n  videoPath: string,\n  segStart: number,\n  segEnd: number,\n  baseW: number,\n  baseH: number,\n): Promise<PersonSnapshot[]> {\n  const duration = Math.max(0, segEnd - segStart);\n  if (duration === 0) {\n    return [];\n  }\n  const fps = DEFAULT_SAMPLE_FPS;\n  const frameInterval = 1 / fps;\n  const expectedFrames = Math.ceil(duration * fps);\n  const tempDir = path.join(process.cwd(), \"tmp\", `pframes_${Date.now()}`);\n  fs.mkdirSync(tempDir, { recursive: true });\n  const FACE_CONF = Math.max(\n    0,\n    Math.min(1, Number(process.env.FACE_CONF || 0.5)),\n  );\n  const POSE_CONF = Math.max(\n    0,\n    Math.min(1, Number(process.env.POSE_CONF || 0.3)),\n  );\n  console.log(\n    `[Person Detection] Config: PERSON_DETECT_WIDTH=${PERSON_DETECT_WIDTH}, FACE_CONF=${FACE_CONF}, POSE_CONF=${POSE_CONF}, MIN_DET_AREA=${MIN_DET_AREA}`,\n  );\n  console.log(\n    `[Person Detection] Extracting ${expectedFrames} frames at ${fps}fps from ${duration.toFixed(1)}s segment`,\n  );\n  try {\n    await extractFrames(videoPath, segStart, duration, fps, tempDir);\n    const frameFiles = fs\n      .readdirSync(tempDir)\n      .filter((f) => {\n        return f.endsWith(\".jpg\");\n      })\n      .sort();\n    console.log(\n      `[Person Detection] Extracted ${frameFiles.length} frames to ${tempDir}`,\n    );\n    const out: PersonSnapshot[] = [];\n    let frameIdx = 0;\n    let tracks: Track[] = [];\n    let nextTrackId = 1;\n    for (let i = 0; i < frameFiles.length && i < expectedFrames; i++) {\n      const framePath = path.join(tempDir, frameFiles[i]);\n      const buffer = fs.readFileSync(framePath);\n      const tensor = tf.node.decodeImage(buffer, 3) as tf.Tensor3D;\n      try {\n        const t = segStart + i * frameInterval;\n        let dets: PersonDet[] = [];\n        const doDetect = frameIdx % DETECT_EVERY === 0 || tracks.length === 0;\n        if (doDetect) {\n          dets = await detectPersons(tensor, baseW, baseH);\n          const rawCount = dets.length;\n          const detectorType =\n            dets.length > 0 ? dets[0].detectorType : undefined;\n          dets = dets.filter((d) => {\n            return d.w * d.h >= MIN_DET_AREA;\n          });\n          const filteredCount = dets.length;\n          if (rawCount > 0) {\n            const avgConf =\n              dets.length > 0\n                ? (\n                    dets.reduce((sum, d) => sum + d.score, 0) / dets.length\n                  ).toFixed(3)\n                : \"0.000\";\n            const detectorLabel =\n              detectorType === \"face\"\n                ? \"Face Detection\"\n                : detectorType === \"pose\"\n                  ? \"Pose Detection\"\n                  : \"Person Detection\";\n            const detectionTypeLabel =\n              detectorType === \"face\"\n                ? \"faces\"\n                : detectorType === \"pose\"\n                  ? \"bodies\"\n                  : \"detections\";\n            console.log(\n              `[${detectorLabel}] Frame ${frameIdx}: Found ${filteredCount} ${detectionTypeLabel} (${rawCount} raw, filtered by MIN_DET_AREA=${MIN_DET_AREA}), avg confidence: ${avgConf}`,\n            );\n          }\n          tracks = associateAndUpdate(\n            tracks,\n            dets,\n            t,\n            baseW,\n            baseH,\n            nextTrackId,\n          );\n          const maxId = tracks.reduce((m, tr) => {\n            return Math.max(m, tr.id);\n          }, nextTrackId);\n          nextTrackId = Math.max(nextTrackId, maxId + 1);\n        } else {\n          tracks = predictOnly(tracks, t, frameInterval, baseW, baseH);\n        }\n        const alive = tracks.filter((tr) => {\n          return tr.hits >= TRACK_MIN_HITS && tr.w * tr.h >= MIN_TRACK_AREA;\n        });\n        const detLike: PersonDet[] = alive.map((tr) => {\n          return {\n            x: tr.x,\n            y: tr.y,\n            w: tr.w,\n            h: tr.h,\n            score: tr.score,\n            id: tr.id,\n          };\n        });\n        out.push({ t, dets: detLike });\n        frameIdx++;\n      } finally {\n        tensor.dispose();\n      }\n    }\n    return out;\n  } catch (e) {\n    console.error(`[Person Detection] ERROR in detectPersonsTimeline:`, e);\n    console.error(`[Person Detection] Stack:`, (e as Error).stack);\n    return [];\n  } finally {\n    if (fs.existsSync(tempDir)) {\n      fs.rmSync(tempDir, { recursive: true, force: true });\n    }\n  }\n}\n\nasync function extractFrames(\n  videoPath: string,\n  segStart: number,\n  duration: number,\n  fps: number,\n  outputDir: string,\n): Promise<void> {\n  const ffmpegPath = require(\"ffmpeg-static\");\n  const scaleFilter =\n    PERSON_DETECT_WIDTH > 0\n      ? `fps=${fps},scale=${PERSON_DETECT_WIDTH}:-1`\n      : `fps=${fps}`;\n  await execFileAsync(ffmpegPath, [\n    \"-ss\",\n    segStart.toFixed(3),\n    \"-i\",\n    videoPath,\n    \"-t\",\n    duration.toFixed(3),\n    \"-vf\",\n    scaleFilter,\n    \"-q:v\",\n    \"2\",\n    path.join(outputDir, \"frame_%04d.jpg\"),\n  ]);\n}\n\nfunction iou(a: PersonDet, b: PersonDet): number {\n  const ax1 = a.x;\n  const ay1 = a.y;\n  const ax2 = a.x + a.w;\n  const ay2 = a.y + a.h;\n  const bx1 = b.x;\n  const by1 = b.y;\n  const bx2 = b.x + b.w;\n  const by2 = b.y + b.h;\n  const xx1 = Math.max(ax1, bx1);\n  const yy1 = Math.max(ay1, by1);\n  const xx2 = Math.min(ax2, bx2);\n  const yy2 = Math.min(ay2, by2);\n  if (xx2 <= xx1 || yy2 <= yy1) {\n    return 0;\n  }\n  const inter = (xx2 - xx1) * (yy2 - yy1);\n  const ra = a.w * a.h;\n  const rb = b.w * b.h;\n  const uni = ra + rb - inter;\n  if (uni <= 0) {\n    return 0;\n  }\n  return inter / uni;\n}\n\nfunction associateAndUpdate(\n  prev: Track[],\n  dets: PersonDet[],\n  t: number,\n  baseW: number,\n  baseH: number,\n  nextId: number,\n): Track[] {\n  const predicted = prev.map((tr) => {\n    const dt = Math.max(0.001, t - tr.lastT);\n    const nx = clamp(Math.round(tr.x + tr.vx * dt), 0, baseW - tr.w);\n    const ny = clamp(Math.round(tr.y + tr.vy * dt), 0, baseH - tr.h);\n    return { ...tr, x: nx, y: ny };\n  });\n  const matches: Array<{ ti: number; di: number; iou: number }> = [];\n  const usedD = new Set<number>();\n  const usedT = new Set<number>();\n  for (let ti = 0; ti < predicted.length; ti++) {\n    let bestIdx = -1;\n    let bestIou = 0;\n    for (let di = 0; di < dets.length; di++) {\n      if (usedD.has(di)) {\n        continue;\n      }\n      const iv = iouTrackDet(predicted[ti], dets[di]);\n      if (iv > bestIou) {\n        bestIou = iv;\n        bestIdx = di;\n      }\n    }\n    if (bestIdx >= 0 && bestIou >= TRACK_IOU_THRESH) {\n      matches.push({ ti, di: bestIdx, iou: bestIou });\n      usedD.add(bestIdx);\n      usedT.add(ti);\n    }\n  }\n  const updated: Track[] = [];\n  for (let i = 0; i < matches.length; i++) {\n    const m = matches[i];\n    const tr = predicted[m.ti];\n    const det = dets[m.di];\n    const dt = Math.max(0.001, t - tr.lastT);\n    const pcx = tr.x + tr.w / 2;\n    const pcy = tr.y + tr.h / 2;\n    const dcx = det.x + det.w / 2;\n    const dcy = det.y + det.h / 2;\n    const vx = (dcx - pcx) / dt;\n    const vy = (dcy - pcy) / dt;\n    updated.push({\n      id: tr.id,\n      x: det.x,\n      y: det.y,\n      w: det.w,\n      h: det.h,\n      score: det.score,\n      vx,\n      vy,\n      lastT: t,\n      age: tr.age + dt,\n      hits: tr.hits + 1,\n      miss: 0,\n    });\n  }\n  for (let ti = 0; ti < predicted.length; ti++) {\n    if (usedT.has(ti)) {\n      continue;\n    }\n    const tr = predicted[ti];\n    const dt = Math.max(0.001, t - tr.lastT);\n    const newAge = tr.age + dt;\n    const newMiss = tr.miss + dt;\n    if (newMiss <= TRACK_MAX_AGE_S) {\n      updated.push({ ...tr, lastT: t, age: newAge, miss: newMiss });\n    }\n  }\n  for (let di = 0; di < dets.length; di++) {\n    if (usedD.has(di)) {\n      continue;\n    }\n    const det = dets[di];\n    updated.push({\n      id: nextId++,\n      x: det.x,\n      y: det.y,\n      w: det.w,\n      h: det.h,\n      score: det.score,\n      vx: 0,\n      vy: 0,\n      lastT: t,\n      age: 0,\n      hits: 1,\n      miss: 0,\n    });\n  }\n  return updated;\n}\n\nfunction predictOnly(\n  prev: Track[],\n  t: number,\n  dtDefault: number,\n  baseW: number,\n  baseH: number,\n): Track[] {\n  const out: Track[] = [];\n  for (let i = 0; i < prev.length; i++) {\n    const tr = prev[i];\n    const dt = Math.max(0.001, t - tr.lastT || dtDefault);\n    const nx = clamp(Math.round(tr.x + tr.vx * dt), 0, baseW - tr.w);\n    const ny = clamp(Math.round(tr.y + tr.vy * dt), 0, baseH - tr.h);\n    const miss = tr.miss + dt;\n    if (miss <= TRACK_MAX_AGE_S) {\n      out.push({ ...tr, x: nx, y: ny, lastT: t, age: tr.age + dt, miss });\n    }\n  }\n  return out;\n}\n\nfunction iouTrackDet(tr: Track, d: PersonDet): number {\n  const ax1 = tr.x;\n  const ay1 = tr.y;\n  const ax2 = tr.x + tr.w;\n  const ay2 = tr.y + tr.h;\n  const bx1 = d.x;\n  const by1 = d.y;\n  const bx2 = d.x + d.w;\n  const by2 = d.y + d.h;\n  const xx1 = Math.max(ax1, bx1);\n  const yy1 = Math.max(ay1, by1);\n  const xx2 = Math.min(ax2, bx2);\n  const yy2 = Math.min(ay2, by2);\n  if (xx2 <= xx1 || yy2 <= yy1) {\n    return 0;\n  }\n  const inter = (xx2 - xx1) * (yy2 - yy1);\n  const ra = tr.w * tr.h;\n  const rb = d.w * d.h;\n  const uni = ra + rb - inter;\n  if (uni <= 0) {\n    return 0;\n  }\n  return inter / uni;\n}\n\nfunction buildTracks(\n  snaps: PersonSnapshot[],\n  baseW: number,\n  baseH: number,\n): PersonSnapshot[] {\n  const out: PersonSnapshot[] = [];\n  let tracks: Track[] = [];\n  let nextId = 1;\n  for (let i = 0; i < snaps.length; i++) {\n    const s = snaps[i];\n    tracks = associateAndUpdate(tracks, s.dets, s.t, baseW, baseH, nextId);\n    const maxId = tracks.reduce((m, tr) => {\n      return Math.max(m, tr.id);\n    }, nextId);\n    nextId = Math.max(nextId, maxId + 1);\n    const alive = tracks.filter((tr) => {\n      return (\n        tr.hits >= TRACK_MIN_HITS &&\n        tr.miss <= TRACK_MAX_AGE_S &&\n        tr.w * tr.h >= MIN_TRACK_AREA\n      );\n    });\n    const detLike: PersonDet[] = alive.map((tr) => {\n      return { x: tr.x, y: tr.y, w: tr.w, h: tr.h, score: tr.score, id: tr.id };\n    });\n    out.push({ t: s.t, dets: detLike });\n  }\n  return out;\n}\n\nfunction computeGlobalCropKeyframesFromTracksWithSpeechAndZoom(\n  timeline: PersonSnapshot[],\n  turns: SpeakerTurn[],\n  baseW: number,\n  baseH: number,\n  constraints: Constraints,\n): CropKF[] {\n  const targetW = Math.floor((baseH * 9) / 16);\n\n  const map: Record<string, number> = {};\n  const anchors: PersonDet[] = [];\n  const bounds: Array<ReturnType<typeof boundsToGroup>> = [];\n  const times: number[] = [];\n\n  for (let i = 0; i < timeline.length; i++) {\n    const snap = timeline[i];\n    if (snap.dets.length === 0) {\n      continue;\n    }\n    const spk = activeSpeakerAt(snap.t, turns);\n    let anchor: PersonDet | null = null;\n    if (spk) {\n      const mapped = map[spk];\n      if (mapped !== undefined) {\n        const found = snap.dets.find((d) => {\n          return d.id === mapped;\n        });\n        if (found) {\n          anchor = found;\n        }\n      }\n      if (!anchor) {\n        const pick = chooseAnchor(snap.dets, baseW, baseH);\n        if (pick) {\n          anchor = pick;\n          if (pick.id !== undefined) {\n            map[spk] = pick.id;\n          }\n        }\n      }\n    }\n    if (!anchor) {\n      anchor = chooseAnchor(snap.dets, baseW, baseH);\n    }\n    const b = computeBounds(snap.dets);\n    anchors.push(anchor);\n    bounds.push(boundsToGroup(b));\n    times.push(snap.t);\n  }\n\n  if (anchors.length === 0) {\n    return [];\n  }\n\n  const smoothingState: SmoothingState = {\n    smoothedX: null,\n    smoothedY: null,\n  };\n  const smoothedAnchors: Array<{ smoothedX: number; smoothedY: number }> = [];\n\n  for (let i = 0; i < anchors.length; i++) {\n    const anchor = anchors[i];\n    const rawX = anchor.x + anchor.w / 2;\n    const rawY = anchor.y + anchor.h / 2;\n    const smoothed = applyTemporalSmoothing(rawX, rawY, smoothingState);\n    smoothedAnchors.push(smoothed);\n  }\n\n  const feasT: number[] = [];\n  const xL: number[] = [];\n  const xU: number[] = [];\n  const yL: number[] = [];\n  const yU: number[] = [];\n  const xDes: number[] = [];\n  const yDes: number[] = [];\n  const wts: number[] = [];\n  const groups: Array<{\n    w: number;\n    h: number;\n    insetX: number;\n    insetY: number;\n  }> = [];\n\n  for (let i = 0; i < anchors.length; i++) {\n    const anchor = anchors[i];\n    const smoothed = smoothedAnchors[i];\n    const f = buildFeasible(\n      bounds[i],\n      anchor,\n      targetW,\n      baseH,\n      baseW,\n      baseH,\n      constraints,\n      smoothed,\n    );\n    feasT.push(times[i]);\n    xL.push(f.xL);\n    xU.push(f.xU);\n    yL.push(f.yL);\n    yU.push(f.yU);\n    xDes.push(f.xDes);\n    yDes.push(f.yDes);\n    wts.push(anchor.score);\n    groups.push({\n      w: f.groupW,\n      h: f.groupH,\n      insetX: f.insetX,\n      insetY: f.insetY,\n    });\n  }\n  if (feasT.length === 0) {\n    return [];\n  }\n  const xs = globalOptimize1D(\n    feasT,\n    xDes,\n    xL,\n    xU,\n    wts,\n    GLOB_LAMBDA_V,\n    GLOB_LAMBDA_A,\n    GLOB_ITERS,\n    GLOB_LR,\n  );\n  const ys = globalOptimize1D(\n    feasT,\n    yDes,\n    yL,\n    yU,\n    wts,\n    GLOB_LAMBDA_V,\n    GLOB_LAMBDA_A,\n    GLOB_ITERS,\n    GLOB_LR,\n  );\n  const zMinWidth = targetW / baseW;\n  const zMinHeight = 1.0;\n  const zMin = Math.max(zMinWidth, zMinHeight, Z_MIN);\n  console.log(\n    `[Framing] Dimension-aware z_min: ${zMin.toFixed(3)} (width: ${zMinWidth.toFixed(3)}, height: ${zMinHeight.toFixed(3)}, user: ${Z_MIN.toFixed(3)})`,\n  );\n  const zs = solveZoomSeries(xs, ys, groups, targetW, baseH, zMin);\n  const out: CropKF[] = [];\n  for (let i = 0; i < feasT.length; i++) {\n    const z = zs[i];\n    const cropW = Math.round(targetW / z);\n    const cropH = Math.round(baseH / z);\n\n    const maxX = baseW - cropW;\n    const maxY = baseH - cropH;\n\n    const x = Math.max(0, Math.min(Math.round(xs[i]), maxX));\n    const y = Math.max(0, Math.min(Math.round(ys[i]), maxY));\n\n    if (i < 3) {\n      console.log(\n        `[Framing] KF ${i} at t=${feasT[i].toFixed(2)}s: pos=(${x},${y}) crop=(${cropW}x${cropH}) video=(${baseW}x${baseH}) z=${z.toFixed(3)}`,\n      );\n    }\n\n    out.push({ t: feasT[i], x, y, w: targetW, h: baseH, z, xs: 0, ys: 0 });\n  }\n  return out;\n}\n\nfunction compressCropMap(kf: CropKF[], maxKeyframes: number = 120): CropKF[] {\n  if (kf.length <= 2) {\n    return kf;\n  }\n\n  if (kf.length <= maxKeyframes) {\n    const filtered: CropKF[] = [kf[0]];\n\n    for (let i = 1; i < kf.length - 1; i++) {\n      const prev = filtered[filtered.length - 1];\n      const curr = kf[i];\n\n      const timeDelta = curr.t - prev.t;\n      if (timeDelta < 0.15) {\n        continue;\n      }\n\n      const positionDelta = Math.sqrt(\n        Math.pow(curr.x - prev.x, 2) + Math.pow(curr.y - prev.y, 2),\n      );\n      const zoomDelta = Math.abs(curr.z - prev.z);\n\n      if (positionDelta < 1 && zoomDelta < 0.005) {\n        continue;\n      }\n\n      filtered.push(curr);\n    }\n\n    filtered.push(kf[kf.length - 1]);\n    return filtered;\n  }\n\n  const step = kf.length / (maxKeyframes - 1);\n  const decimated: CropKF[] = [kf[0]];\n\n  for (let i = 1; i < maxKeyframes - 1; i++) {\n    const idx = Math.round(i * step);\n    decimated.push(kf[idx]);\n  }\n\n  decimated.push(kf[kf.length - 1]);\n\n  return decimated;\n}\n\nfunction boundsToGroup(b: {\n  minX: number;\n  maxX: number;\n  minY: number;\n  maxY: number;\n}): {\n  groupW: number;\n  groupH: number;\n  minX: number;\n  maxX: number;\n  minY: number;\n  maxY: number;\n} {\n  return {\n    groupW: b.maxX - b.minX,\n    groupH: b.maxY - b.minY,\n    minX: b.minX,\n    maxX: b.maxX,\n    minY: b.minY,\n    maxY: b.maxY,\n  };\n}\n\nfunction applyTemporalSmoothing(\n  rawX: number,\n  rawY: number,\n  smoothingState: SmoothingState,\n): { smoothedX: number; smoothedY: number } {\n  if (smoothingState.smoothedX === null || smoothingState.smoothedY === null) {\n    smoothingState.smoothedX = rawX;\n    smoothingState.smoothedY = rawY;\n    return { smoothedX: rawX, smoothedY: rawY };\n  }\n\n  const deltaX = rawX - smoothingState.smoothedX;\n  const deltaY = rawY - smoothingState.smoothedY;\n\n  let targetX = rawX;\n  let targetY = rawY;\n\n  if (Math.abs(deltaX) <= DEADZONE_X) {\n    targetX = smoothingState.smoothedX;\n  }\n\n  if (Math.abs(deltaY) <= DEADZONE_Y) {\n    targetY = smoothingState.smoothedY;\n  }\n\n  const newX =\n    smoothingState.smoothedX +\n    SMOOTH_ALPHA * (targetX - smoothingState.smoothedX);\n  const newY =\n    smoothingState.smoothedY +\n    SMOOTH_ALPHA * (targetY - smoothingState.smoothedY);\n\n  smoothingState.smoothedX = newX;\n  smoothingState.smoothedY = newY;\n\n  return { smoothedX: newX, smoothedY: newY };\n}\n\nfunction buildFeasible(\n  group: {\n    groupW: number;\n    groupH: number;\n    minX: number;\n    maxX: number;\n    minY: number;\n    maxY: number;\n  },\n  anchor: PersonDet,\n  targetW: number,\n  targetH: number,\n  baseW: number,\n  baseH: number,\n  constraints: Constraints,\n  smoothedPosition: { smoothedX: number; smoothedY: number },\n): {\n  xL: number;\n  xU: number;\n  yL: number;\n  yU: number;\n  xDes: number;\n  yDes: number;\n  groupW: number;\n  groupH: number;\n  insetX: number;\n  insetY: number;\n} {\n  const marginX = Math.max(0, constraints.margin) * targetW;\n  const minMarginY = targetH * 0.1;\n  const adaptiveMarginY = Math.max(minMarginY, group.groupH * 0.6);\n  const topPad = Math.max(minMarginY, adaptiveMarginY * 0.6);\n  const bottomPad = adaptiveMarginY;\n  const paddedMinX = clamp(group.minX - marginX, 0, baseW);\n  const paddedMaxX = clamp(group.maxX + marginX, 0, baseW);\n  const paddedMinY = clamp(group.minY - topPad, 0, baseH);\n  const paddedMaxY = clamp(group.maxY + bottomPad, 0, baseH);\n  const frameCenterX = baseW / 2;\n  const blend = Math.max(0, Math.min(1, constraints.centerBiasX));\n  const desiredCenterX =\n    smoothedPosition.smoothedX * blend + frameCenterX * (1 - blend);\n  const torsoBias = Math.min(targetH * 0.18, group.groupH * 0.6);\n  const desiredCenterY =\n    (paddedMinY + paddedMaxY) / 2 +\n    torsoBias -\n    targetH * constraints.centerBiasY;\n  const insetX = Math.max(8, Math.floor(targetW * 0.06));\n  const insetY = Math.max(8, Math.floor(targetH * 0.08));\n  let xL = clamp(\n    paddedMaxX + insetX - targetW,\n    0,\n    Math.max(0, baseW - targetW),\n  );\n  let xU = clamp(paddedMinX - insetX, 0, Math.max(0, baseW - targetW));\n  if (xL > xU) {\n    const m = clamp(\n      (paddedMinX + paddedMaxX) / 2 - targetW / 2,\n      0,\n      Math.max(0, baseW - targetW),\n    );\n    xL = m;\n    xU = m;\n  }\n  const safeTop = -Math.round(targetH * constraints.safeTop);\n  const safeBottom = Math.round(targetH * constraints.safeBottom);\n  const minY = safeTop;\n  const maxY = baseH - targetH + safeBottom;\n  let yL = clamp(paddedMaxY + insetY - targetH, minY, maxY);\n  let yU = clamp(paddedMinY - insetY, minY, maxY);\n  if (yL > yU) {\n    const m = clamp((paddedMinY + paddedMaxY) / 2 - targetH / 2, minY, maxY);\n    yL = m;\n    yU = m;\n  }\n  const xDes = clamp(desiredCenterX - targetW / 2, xL, xU);\n  const yDes = clamp(desiredCenterY - targetH / 2, yL, yU);\n  return {\n    xL,\n    xU,\n    yL,\n    yU,\n    xDes,\n    yDes,\n    groupW: paddedMaxX - paddedMinX,\n    groupH: paddedMaxY - paddedMinY,\n    insetX,\n    insetY,\n  };\n}\n\nfunction solveZoomSeries(\n  xs: number[],\n  ys: number[],\n  groups: Array<{ w: number; h: number; insetX: number; insetY: number }>,\n  targetW: number,\n  targetH: number,\n  zMin: number,\n): number[] {\n  const z: number[] = [];\n  for (let i = 0; i < xs.length; i++) {\n    const gw = groups[i].w;\n    const gh = groups[i].h;\n    const needX = (gw + 2 * groups[i].insetX) / targetW;\n    const needY = (gh + 2 * groups[i].insetY) / targetH;\n    let zr = 1;\n    if (needX > 1 || needY > 1) {\n      const need = Math.max(needX, needY);\n      zr = Math.max(zMin, 1 / need);\n    }\n    if (i === 0) {\n      z.push(zr);\n    } else {\n      const p = z[z.length - 1];\n      const blended = p + Z_DECAY * (zr - p);\n      z.push(Math.max(zMin, blended));\n    }\n  }\n  return z;\n}\n\nfunction computeBounds(dets: PersonDet[]): {\n  minX: number;\n  maxX: number;\n  minY: number;\n  maxY: number;\n} {\n  let minX = Number.POSITIVE_INFINITY;\n  let minY = Number.POSITIVE_INFINITY;\n  let maxX = Number.NEGATIVE_INFINITY;\n  let maxY = Number.NEGATIVE_INFINITY;\n  for (let i = 0; i < dets.length; i++) {\n    const d = dets[i];\n    minX = Math.min(minX, d.x);\n    minY = Math.min(minY, d.y);\n    maxX = Math.max(maxX, d.x + d.w);\n    maxY = Math.max(maxY, d.y + d.h);\n  }\n  return { minX, maxX, minY, maxY };\n}\n\nfunction chooseAnchor(\n  dets: PersonDet[],\n  baseW: number,\n  baseH: number,\n): PersonDet {\n  let best = dets[0];\n  let bestScore = anchorScore(dets[0], baseW, baseH);\n  for (let i = 1; i < dets.length; i++) {\n    const s = anchorScore(dets[i], baseW, baseH);\n    if (s > bestScore) {\n      best = dets[i];\n      bestScore = s;\n    }\n  }\n  return best;\n}\n\nfunction anchorScore(d: PersonDet, baseW: number, baseH: number): number {\n  const area = d.w * d.h;\n  const cx = d.x + d.w / 2;\n  const cy = d.y + d.h / 2;\n  const dx = Math.abs(cx - baseW / 2) / (baseW / 2);\n  const dy = Math.abs(cy - baseH / 2) / (baseH / 2);\n  const centr = 1 - Math.min(1, Math.hypot(dx, dy));\n  return area * d.score * (0.5 + 0.5 * centr);\n}\n\nfunction buildSpeakerTurns(\n  words: TranscriptWord[],\n  segStart: number,\n  segEnd: number,\n): SpeakerTurn[] {\n  const filtered = words\n    .filter((w) => {\n      return w.speaker && w.end >= segStart && w.t <= segEnd;\n    })\n    .sort((a, b) => {\n      return a.t - b.t;\n    });\n  if (filtered.length === 0) {\n    return [];\n  }\n  const turns: SpeakerTurn[] = [];\n  let cur: SpeakerTurn | null = null;\n  for (let i = 0; i < filtered.length; i++) {\n    const w = filtered[i];\n    if (!cur) {\n      cur = {\n        start: Math.max(segStart, w.t),\n        end: Math.min(segEnd, w.end),\n        label: w.speaker!,\n      };\n      continue;\n    }\n    if (w.speaker === cur.label && w.t - cur.end <= 0.6) {\n      cur.end = Math.min(segEnd, Math.max(cur.end, w.end));\n    } else {\n      turns.push(cur);\n      cur = {\n        start: Math.max(segStart, w.t),\n        end: Math.min(segEnd, w.end),\n        label: w.speaker!,\n      };\n    }\n  }\n  if (cur) {\n    turns.push(cur);\n  }\n  return turns;\n}\n\nfunction activeSpeakerAt(t: number, turns: SpeakerTurn[]): string | null {\n  let best: string | null = null;\n  let bestOverlap = 0;\n  const a = t - 0.6;\n  const b = t + 0.2;\n  for (let i = 0; i < turns.length; i++) {\n    const u = Math.max(a, turns[i].start);\n    const v = Math.min(b, turns[i].end);\n    const ov = Math.max(0, v - u);\n    if (ov > bestOverlap) {\n      bestOverlap = ov;\n      best = turns[i].label;\n    }\n  }\n  return best;\n}\n\nfunction smoothKeyframes(kf: CropKF[]): CropKF[] {\n  if (kf.length < 2) {\n    return kf;\n  }\n  const out: CropKF[] = [];\n  let sx = kf[0].x;\n  let sy = kf[0].y;\n  let sz = kf[0].z;\n  out.push({\n    t: kf[0].t,\n    x: Math.round(sx),\n    y: Math.round(sy),\n    w: kf[0].w,\n    h: kf[0].h,\n    z: sz,\n    xs: 0,\n    ys: 0,\n  });\n  for (let i = 1; i < kf.length; i++) {\n    sx = sx + SMOOTH_ALPHA * (kf[i].x - sx);\n    sy = sy + SMOOTH_ALPHA * (kf[i].y - sy);\n    sz = sz + SMOOTH_ALPHA * (kf[i].z - sz);\n    out.push({\n      t: kf[i].t,\n      x: Math.round(sx),\n      y: Math.round(sy),\n      w: kf[i].w,\n      h: kf[i].h,\n      z: sz,\n      xs: 0,\n      ys: 0,\n    });\n  }\n  return out;\n}\n\nfunction applyDeadzone(kf: CropKF[], dzx: number, dzy: number): CropKF[] {\n  if (kf.length < 2) {\n    return kf;\n  }\n  const out: CropKF[] = [kf[0]];\n  for (let i = 1; i < kf.length; i++) {\n    const prev = out[out.length - 1];\n    const cur = kf[i];\n    let nx = cur.x;\n    let ny = cur.y;\n    let nz = cur.z;\n    if (Math.abs(cur.x - prev.x) < dzx) {\n      nx = prev.x;\n    }\n    if (Math.abs(cur.y - prev.y) < dzy) {\n      ny = prev.y;\n    }\n    if (Math.abs(cur.z - prev.z) < 0.01) {\n      nz = prev.z;\n    }\n    out.push({\n      t: cur.t,\n      x: nx,\n      y: ny,\n      w: cur.w,\n      h: cur.h,\n      z: nz,\n      xs: 0,\n      ys: 0,\n    });\n  }\n  return out;\n}\n\nfunction applyPanLimits(kf: CropKF[], maxPanPerSecond: number): CropKF[] {\n  if (kf.length < 2) {\n    return kf;\n  }\n  const limited: CropKF[] = [kf[0]];\n  for (let i = 1; i < kf.length; i++) {\n    const prev = limited[limited.length - 1];\n    const cur = kf[i];\n    const dt = Math.max(0.001, cur.t - prev.t);\n    const maxDelta = Math.max(0, maxPanPerSecond) * dt;\n    const nextX = stepToward(prev.x, cur.x, maxDelta);\n    const nextY = stepToward(prev.y, cur.y, maxDelta);\n    limited.push({\n      t: cur.t,\n      x: nextX,\n      y: nextY,\n      w: cur.w,\n      h: cur.h,\n      z: cur.z,\n      xs: 0,\n      ys: 0,\n    });\n  }\n  return limited;\n}\n\nfunction applyAccelLimits(kf: CropKF[], maxAccelPerSecond: number): CropKF[] {\n  if (kf.length < 3) {\n    return kf;\n  }\n  const out: CropKF[] = [kf[0]];\n  let vx = 0;\n  let vy = 0;\n  for (let i = 1; i < kf.length; i++) {\n    const prev = out[out.length - 1];\n    const cur = kf[i];\n    const dt = Math.max(0.001, cur.t - prev.t);\n    const tx = (cur.x - prev.x) / dt;\n    const ty = (cur.y - prev.y) / dt;\n    const ax = stepToward(vx, tx, maxAccelPerSecond * dt) - vx;\n    const ay = stepToward(vy, ty, maxAccelPerSecond * dt) - vy;\n    vx = vx + ax;\n    vy = vy + ay;\n    const nx = Math.round(prev.x + vx * dt);\n    const ny = Math.round(prev.y + vy * dt);\n    out.push({\n      t: cur.t,\n      x: nx,\n      y: ny,\n      w: cur.w,\n      h: cur.h,\n      z: cur.z,\n      xs: 0,\n      ys: 0,\n    });\n  }\n  return out;\n}\n\nfunction easeSegmentEdges(\n  kf: CropKF[],\n  segStart: number,\n  segEnd: number,\n  easeSeconds: number,\n): CropKF[] {\n  if (kf.length === 0 || easeSeconds <= 0) {\n    return kf;\n  }\n  const eased: CropKF[] = [];\n  for (let i = 0; i < kf.length; i++) {\n    const frame = kf[i];\n    const fromStart = Math.min(\n      1,\n      Math.max(0, (frame.t - segStart) / easeSeconds),\n    );\n    const fromEnd = Math.min(1, Math.max(0, (segEnd - frame.t) / easeSeconds));\n    const influence = Math.min(fromStart, fromEnd);\n    if (eased.length === 0) {\n      eased.push(frame);\n      continue;\n    }\n    const prev = eased[eased.length - 1];\n    const x = Math.round(prev.x + (frame.x - prev.x) * influence);\n    const y = Math.round(prev.y + (frame.y - prev.y) * influence);\n    const z = prev.z + (frame.z - prev.z) * influence;\n    eased.push({ t: frame.t, x, y, w: frame.w, h: frame.h, z, xs: 0, ys: 0 });\n  }\n  return eased;\n}\n\nfunction applyScaledCoords(\n  kf: CropKF[],\n  baseW: number,\n  baseH: number,\n): CropKF[] {\n  if (kf.length === 0) {\n    return kf;\n  }\n  const targetW = Math.min(Math.floor((baseH * 9) / 16), baseW);\n  const out: CropKF[] = [];\n  for (let i = 0; i < kf.length; i++) {\n    const z = kf[i].z;\n    const cropW = Math.round(targetW / z);\n    const cropH = Math.round(baseH / z);\n    const maxX = Math.max(0, baseW - cropW);\n    const maxY = Math.max(0, baseH - cropH);\n    const clampedX = clamp(kf[i].x, 0, maxX);\n    const clampedY = clamp(kf[i].y, 0, maxY);\n    out.push({\n      t: kf[i].t,\n      x: clampedX,\n      y: clampedY,\n      w: cropW,\n      h: cropH,\n      z,\n      xs: 0,\n      ys: 0,\n    });\n  }\n  return out;\n}\n\nfunction dedupeByTime(kf: CropKF[], minDelta: number): CropKF[] {\n  if (kf.length === 0) {\n    return kf;\n  }\n  const out: CropKF[] = [kf[0]];\n  for (let i = 1; i < kf.length; i++) {\n    if (kf[i].t - out[out.length - 1].t >= minDelta) {\n      out.push(kf[i]);\n    }\n  }\n  return out;\n}\n\nfunction stepToward(current: number, target: number, maxDelta: number): number {\n  if (Math.abs(target - current) <= maxDelta) {\n    return target;\n  }\n  return target > current ? current + maxDelta : current - maxDelta;\n}\n\nfunction clamp(value: number, min: number, max: number): number {\n  if (value < min) {\n    return min;\n  }\n  if (value > max) {\n    return max;\n  }\n  return value;\n}\n\nfunction globalOptimize1D(\n  t: number[],\n  des: number[],\n  L: number[],\n  U: number[],\n  w: number[],\n  lambdaV: number,\n  lambdaA: number,\n  iters: number,\n  lr: number,\n): number[] {\n  const n = des.length;\n  const x = des.slice();\n  const ww = w.map((s) => {\n    return Math.max(0.2, Math.min(1, s));\n  });\n  for (let it = 0; it < iters; it++) {\n    const g = new Array(n).fill(0);\n    for (let i = 0; i < n; i++) {\n      g[i] += 2 * ww[i] * (x[i] - des[i]);\n    }\n    for (let i = 1; i < n; i++) {\n      const dv = x[i] - x[i - 1];\n      g[i] += 2 * lambdaV * dv;\n      g[i - 1] -= 2 * lambdaV * dv;\n    }\n    for (let i = 1; i < n - 1; i++) {\n      const da = x[i + 1] - 2 * x[i] + x[i - 1];\n      g[i + 1] += 2 * lambdaA * da;\n      g[i] += -4 * lambdaA * da;\n      g[i - 1] += 2 * lambdaA * da;\n    }\n    for (let i = 0; i < n; i++) {\n      x[i] = x[i] - lr * g[i];\n      if (x[i] < L[i]) {\n        x[i] = L[i];\n      }\n      if (x[i] > U[i]) {\n        x[i] = U[i];\n      }\n    }\n  }\n  return x;\n}\n\nexport const __testables = {\n  detectPersonsTimeline,\n  buildTracks,\n  computeGlobalCropKeyframesFromTracksWithSpeechAndZoom,\n  buildSpeakerTurns,\n  activeSpeakerAt,\n  buildFFmpegFilter,\n};\n","size_bytes":66781},"scripts/retry-video.ts":{"content":"import { videoQueue } from '../src/lib/queue'\n\nconst videoId = process.argv[2]\nconst userId = process.argv[3]\n\nif (!videoId || !userId)\n{\n  console.error('Usage: ts-node scripts/retry-video.ts <videoId> <userId>')\n  process.exit(1)\n}\n\nasync function retry() {\n  try {\n    await videoQueue.add('process', {\n      videoId,\n      userId\n    })\n    console.log(`Job added to queue for video ${videoId}`)\n    process.exit(0)\n  }\n  catch (error) {\n    console.error('Error adding job:', error)\n    process.exit(1)\n  }\n}\n\nretry()\n","size_bytes":523},"app/api/tiktok/oauth/callback/route.ts":{"content":"import { NextRequest, NextResponse } from \"next/server\";\nimport { prisma } from \"@/src/lib/prisma\";\nimport { encrypt } from \"@/src/lib/encryption\";\nimport { getCurrentUserId } from \"@/src/lib/session\";\n\nfunction redirectAbs(req: NextRequest, path: string) {\n  const xfProto = req.headers.get(\"x-forwarded-proto\") || \"https\";\n  const xfHost = req.headers.get(\"x-forwarded-host\") || req.headers.get(\"host\");\n  const origin = xfHost ? `${xfProto}://${xfHost}` : req.nextUrl.origin;\n  return NextResponse.redirect(new URL(path, origin));\n}\n\nexport async function GET(req: NextRequest) {\n  try {\n    const { searchParams } = new URL(req.url);\n    const err = searchParams.get(\"error\");\n    if (err) {\n      return redirectAbs(req, \"/?tiktok=error\");\n    }\n    const code = searchParams.get(\"code\");\n    if (!code) {\n      return redirectAbs(req, \"/?tiktok=error_no_code\");\n    }\n\n    const body = new URLSearchParams();\n    body.set(\"client_key\", process.env.TIKTOK_CLIENT_KEY || \"\");\n    body.set(\"client_secret\", process.env.TIKTOK_CLIENT_SECRET || \"\");\n    body.set(\"code\", code);\n    body.set(\"grant_type\", \"authorization_code\");\n    body.set(\"redirect_uri\", process.env.TIKTOK_REDIRECT_URI || \"\");\n\n    const resp = await fetch(\"https://open.tiktokapis.com/v2/oauth/token/\", {\n      method: \"POST\",\n      headers: { \"Content-Type\": \"application/x-www-form-urlencoded\" },\n      body,\n    });\n    const data = await resp.json();\n    if (!resp.ok) {\n      return redirectAbs(req, \"/?tiktok=error_exchange\");\n    }\n\n    const userId = await getCurrentUserId();\n    if (!userId) {\n      return redirectAbs(req, \"/login?error=1\");\n    }\n\n    const accessToken = data?.access_token as string | undefined;\n    const refreshToken = data?.refresh_token as string | undefined;\n    const openId = data?.open_id as string | undefined;\n    const expiresIn = Number(data?.expires_in || 0);\n    if (!accessToken || !refreshToken || !openId || !expiresIn) {\n      return redirectAbs(req, \"/?tiktok=error_payload\");\n    }\n\n    const expiresAt = new Date(Date.now() + expiresIn * 1000);\n    await prisma.tikTokConnection.upsert({\n      where: { openId },\n      update: {\n        userId,\n        accessToken: encrypt(accessToken),\n        refreshToken: encrypt(refreshToken),\n        scopes: Array.isArray(data.scope)\n          ? data.scope.join(\",\")\n          : data.scope || \"\",\n        expiresAt,\n      },\n      create: {\n        userId,\n        openId,\n        accessToken: encrypt(accessToken),\n        refreshToken: encrypt(refreshToken),\n        scopes: Array.isArray(data.scope)\n          ? data.scope.join(\",\")\n          : data.scope || \"\",\n        expiresAt,\n      },\n    });\n\n    return redirectAbs(req, \"/?tiktok=connected\");\n  } catch {\n    return redirectAbs(req, \"/?tiktok=error_500\");\n  }\n}\n","size_bytes":2787},"src/server/tiktokAuth.ts":{"content":"import { prisma } from \"@/src/lib/prisma\";\nimport { encrypt, decrypt } from \"@/src/lib/encryption\";\n\nexport async function getTikTokAccessTokenByUserId(userId: string) {\n  const row = await prisma.tikTokConnection.findFirst({ where: { userId } });\n  if (!row) {\n    return null;\n  }\n  if (Date.now() < row.expiresAt.getTime()) {\n    return decrypt(row.accessToken);\n  }\n  const body = new URLSearchParams();\n  body.set(\"client_key\", process.env.TIKTOK_CLIENT_KEY || \"\");\n  console.log(\"client_key\", process.env.TIKTOK_CLIENT_KEY);\n  body.set(\"client_secret\", process.env.TIKTOK_CLIENT_SECRET || \"\");\n  body.set(\"grant_type\", \"refresh_token\");\n  body.set(\"refresh_token\", decrypt(row.refreshToken));\n  const r = await fetch(\"https://open.tiktokapis.com/v2/oauth/token/\", {\n    method: \"POST\",\n    headers: { \"Content-Type\": \"application/x-www-form-urlencoded\" },\n    body,\n  });\n  const data = await r.json();\n  if (!data.access_token) {\n    return null;\n  }\n  const expiresAt = new Date(Date.now() + data.expires_in * 1000);\n  await prisma.tikTokConnection.update({\n    where: { id: row.id },\n    data: {\n      accessToken: encrypt(data.access_token),\n      refreshToken: encrypt(data.refresh_token || decrypt(row.refreshToken)),\n      expiresAt,\n    },\n  });\n  return data.access_token;\n}\n","size_bytes":1290},"app/api/tiktok/oauth/start/route.ts":{"content":"import { NextResponse } from \"next/server\";\n\nexport async function GET() {\n  const params = new URLSearchParams();\n  params.set(\"client_key\", process.env.TIKTOK_CLIENT_KEY || \"\");\n  params.set(\"response_type\", \"code\");\n  params.set(\"redirect_uri\", process.env.TIKTOK_REDIRECT_URI || \"\");\n  params.set(\"scope\", \"user.info.basic,video.upload\");\n  params.set(\"state\", crypto.randomUUID());\n  return NextResponse.redirect(\n    `https://www.tiktok.com/v2/auth/authorize/?${params.toString()}`,\n  );\n}\n","size_bytes":496},"src/services/tiktok.ts":{"content":"import { promises as fsp } from \"fs\";\nimport { createReadStream } from \"fs\";\n\ntype EnsureTokenInput = {\n  accessToken: string;\n  refreshToken: string;\n  expiresAt: Date;\n};\n\ntype EnsureTokenOutput = {\n  accessToken: string;\n  refreshToken: string;\n  expiresAt: Date;\n  rotated: boolean;\n};\n\nasync function throwIfNotOk(res: Response, label: string) {\n  if (res.ok) {\n    return;\n  }\n  let body: any = null;\n  try {\n    body = await res.json();\n  } catch {}\n  const detail = body ? JSON.stringify(body) : await res.text().catch(() => \"\");\n  throw new Error(`${label} ${res.status} ${detail}`);\n}\n\nexport async function ensureAccessToken(\n  input: EnsureTokenInput,\n): Promise<EnsureTokenOutput> {\n  const now = Date.now();\n  const exp = input.expiresAt ? input.expiresAt.getTime() : 0;\n  if (exp - now > 120000) {\n    return {\n      accessToken: input.accessToken,\n      refreshToken: input.refreshToken,\n      expiresAt: input.expiresAt,\n      rotated: false,\n    };\n  }\n  const res = await fetch(\"https://open.tiktokapis.com/v2/oauth/token/\", {\n    method: \"POST\",\n    headers: { \"content-type\": \"application/x-www-form-urlencoded\" },\n    body: new URLSearchParams({\n      client_key: process.env.TIKTOK_CLIENT_KEY as string,\n      client_secret: process.env.TIKTOK_CLIENT_SECRET as string,\n      grant_type: \"refresh_token\",\n      refresh_token: input.refreshToken,\n    }),\n  });\n  await throwIfNotOk(res, \"tiktok refresh failed\");\n  if (!res.ok) {\n    throw new Error(`tiktok refresh failed ${res.status}`);\n  }\n  const j = await res.json();\n  const at = j.access_token as string;\n  const rt = (j.refresh_token as string) || input.refreshToken;\n  const expiresIn = Number(j.expires_in || 3600);\n  const until = new Date(Date.now() + expiresIn * 1000);\n  return { accessToken: at, refreshToken: rt, expiresAt: until, rotated: true };\n}\n\ntype InitUploadInput = {\n  accessToken: string;\n  filePath: string;\n  chunkSizeBytes?: number;\n};\n\ntype InitUploadOutput = {\n  uploadUrl: string;\n  publishId: string;\n  fileSize: number;\n  chunkSizeBytes: number;\n  totalChunks: number;\n};\n\nexport async function initUpload(input: {\n  accessToken: string;\n  filePath: string;\n  chunkSizeBytes?: number;\n}): Promise<{\n  uploadUrl: string;\n  publishId: string;\n  fileSize: number;\n  chunkSizeBytes: number;\n  totalChunks: number;\n}> {\n  const stat = await fsp.stat(input.filePath);\n  const fileSize = stat.size;\n\n  const MB = 1_000_000;\n  const FIVE_MB = 5 * MB;\n  const SIXTY_FOUR_MB = 64 * MB;\n\n  let chunkSizeBytes: number;\n  let totalChunks: number;\n\n  if (fileSize < FIVE_MB) {\n    chunkSizeBytes = fileSize;\n    totalChunks = 1;\n  } else if (fileSize <= SIXTY_FOUR_MB) {\n    chunkSizeBytes = fileSize;\n    totalChunks = 1;\n  } else {\n    const preferred =\n      input.chunkSizeBytes && input.chunkSizeBytes > 0\n        ? input.chunkSizeBytes\n        : 10 * MB;\n    chunkSizeBytes = Math.max(\n      FIVE_MB,\n      Math.min(preferred, SIXTY_FOUR_MB, fileSize),\n    );\n    totalChunks = Math.floor(fileSize / chunkSizeBytes);\n    if (totalChunks < 1) totalChunks = 1;\n    if (totalChunks > 1000) {\n      chunkSizeBytes = Math.ceil(fileSize / 1000);\n      chunkSizeBytes = Math.max(\n        FIVE_MB,\n        Math.min(chunkSizeBytes, SIXTY_FOUR_MB, fileSize),\n      );\n      totalChunks = Math.max(1, Math.floor(fileSize / chunkSizeBytes));\n    }\n  }\n\n  const res = await fetch(\n    \"https://open.tiktokapis.com/v2/post/publish/inbox/video/init/\",\n    {\n      method: \"POST\",\n      headers: {\n        authorization: `Bearer ${input.accessToken}`,\n        \"content-type\": \"application/json; charset=UTF-8\",\n      },\n      body: JSON.stringify({\n        source_info: {\n          source: \"FILE_UPLOAD\",\n          video_size: fileSize,\n          chunk_size: chunkSizeBytes,\n          total_chunk_count: totalChunks,\n        },\n      }),\n    },\n  );\n\n  await throwIfNotOk(res, \"tiktok init failed\");\n  const j = await res.json();\n  const uploadUrl = j.data?.upload_url as string;\n  const publishId = j.data?.publish_id as string;\n  if (!uploadUrl || !publishId)\n    throw new Error(\"tiktok init missing upload_url or publish_id\");\n\n  return { uploadUrl, publishId, fileSize, chunkSizeBytes, totalChunks };\n}\n\ntype UploadToUrlInput = {\n  uploadUrl: string;\n  filePath: string;\n  fileSize: number;\n  chunkSizeBytes: number;\n};\n\nexport async function uploadToUrl(input: {\n  uploadUrl: string;\n  filePath: string;\n  fileSize: number;\n  chunkSizeBytes: number;\n  totalChunks: number;\n}) {\n  const { uploadUrl, filePath, fileSize, chunkSizeBytes, totalChunks } = input;\n  if (fileSize === 0) return;\n\n  for (let i = 0; i < totalChunks; i++) {\n    const start = i * chunkSizeBytes;\n    const endExclusive =\n      i < totalChunks - 1 ? start + chunkSizeBytes : fileSize;\n    const lastByte = endExclusive - 1;\n    const length = endExclusive - start;\n\n    const nodeStream = createReadStream(filePath, { start, end: lastByte });\n\n    const res = await fetch(uploadUrl, {\n      method: \"PUT\",\n      ...({ duplex: \"half\" } as any),\n      body: nodeStream as any,\n      headers: {\n        \"Content-Type\": \"video/mp4\",\n        \"Content-Length\": String(length),\n        \"Content-Range\": `bytes ${start}-${lastByte}/${fileSize}`,\n      },\n    });\n\n    await throwIfNotOk(res, \"tiktok upload failed\");\n  }\n}\n\ntype PublishVideoInput = {\n  accessToken: string;\n  publishId: string;\n  caption: string;\n  mode: \"draft\" | \"publish\";\n};\n\ntype PublishVideoOutput = {\n  ok: boolean;\n};\n\nexport async function publishVideo(\n  input: PublishVideoInput,\n): Promise<PublishVideoOutput> {\n  if (input.mode !== \"publish\") {\n    throw new Error('publishVideo requires mode \"publish\"');\n  }\n  const res = await fetch(\n    \"https://open.tiktokapis.com/v2/post/publish/video/\",\n    {\n      method: \"POST\",\n      headers: {\n        authorization: `Bearer ${input.accessToken}`,\n        \"content-type\": \"application/json\",\n      },\n      body: JSON.stringify({\n        publish_id: input.publishId,\n        post_info: {\n          title: input.caption,\n          visibility: \"public\",\n          disable_duet: true,\n          disable_stitch: true,\n          disable_comment: false,\n        },\n      }),\n    },\n  );\n  await throwIfNotOk(res, \"tiktok publish failed\");\n  if (!res.ok) {\n    throw new Error(`tiktok publish failed ${res.status}`);\n  }\n  return { ok: true };\n}\n","size_bytes":6317},"app/api/tiktok/clip/[id]/post/route.ts":{"content":"import { NextRequest, NextResponse } from \"next/server\";\nimport { prisma } from \"@/src/lib/prisma\";\nimport { getCurrentUserId } from \"@/src/lib/session\";\nimport { Queue } from \"bullmq\";\nimport { connection } from \"@/src/lib/queue\";\n\nconst tiktokQueue = new Queue(\"tiktok.post\", { connection });\n\nexport async function POST(\n  req: NextRequest,\n  { params }: { params: { id: string } },\n) {\n  const userId = await getCurrentUserId();\n  if (!userId) {\n    return NextResponse.json({ ok: false }, { status: 401 });\n  }\n  const body = await req.json();\n  const mode = body?.mode;\n  if (mode !== \"draft\" && mode !== \"publish\") {\n    return NextResponse.json({ ok: false }, { status: 400 });\n  }\n  const clip = await prisma.clip.findFirst({\n    where: { id: params.id, Video: { userId } },\n  });\n  if (!clip) {\n    return NextResponse.json({ ok: false }, { status: 404 });\n  }\n  await tiktokQueue.add(\"post\", { userId, clipId: clip.id, mode });\n  return NextResponse.json({ ok: true });\n}\n","size_bytes":983},"app/api/videos/[id]/retry/route.ts":{"content":"import { NextRequest, NextResponse } from 'next/server'\nimport { requireAuth } from '@/src/lib/session'\nimport { prisma } from '@/src/lib/prisma'\nimport { videoQueue } from '@/src/lib/queue'\n\nexport async function POST(\n  request: NextRequest,\n  { params }: { params: { id: string } }\n) {\n  try {\n    const session = await requireAuth()\n    const { id } = params\n    \n    const video = await prisma.video.findUnique({\n      where: { id, userId: session.userId }\n    })\n    \n    if (!video)\n    {\n      return NextResponse.json({ error: 'Video not found' }, { status: 404 })\n    }\n    \n    if (video.status === 'completed')\n    {\n      return NextResponse.json({ error: 'Video already completed' }, { status: 400 })\n    }\n    \n    const job = await videoQueue.getJob(id)\n    \n    if (job)\n    {\n      await job.remove()\n    }\n    \n    await prisma.video.update({\n      where: { id },\n      data: { status: 'queued' }\n    })\n    \n    await videoQueue.add('process', {\n      videoId: video.id,\n      userId: session.userId\n    })\n    \n    return NextResponse.json({ success: true, status: 'queued' })\n  }\n  catch (error) {\n    console.error('Error retrying video:', error)\n    return NextResponse.json({ error: 'Failed to retry video' }, { status: 500 })\n  }\n}\n","size_bytes":1258},"app/components/tiktokPostButtons.tsx":{"content":"\"use client\";\nimport { useState, useEffect } from \"react\";\n\nexport function TikTokPostButtons({ \n  clipId, \n  initialStatus \n}: { \n  clipId: string;\n  initialStatus?: string | null;\n}) {\n  const [loading, setLoading] = useState<null | \"draft\" | \"publish\">(null);\n  const [status, setStatus] = useState<string | null>(initialStatus || null);\n\n  useEffect(() => {\n    if (loading && (status === \"draft\" || status === \"published\" || status === \"failed\")) {\n      setLoading(null);\n    }\n  }, [status, loading]);\n\n  useEffect(() => {\n    if (!loading) {\n      return;\n    }\n    const interval = setInterval(async () => {\n      const res = await fetch(`/api/tiktok/clip/${clipId}/status`);\n      if (res.ok) {\n        const data = await res.json();\n        if (data.tiktokStatus) {\n          setStatus(data.tiktokStatus);\n        }\n      }\n    }, 2000);\n    return () => clearInterval(interval);\n  }, [clipId, loading]);\n\n  async function send(mode: \"draft\" | \"publish\") {\n    setLoading(mode);\n    setStatus(null);\n    const res = await fetch(`/api/tiktok/clip/${clipId}/post`, {\n      method: \"POST\",\n      headers: { \"Content-Type\": \"application/json\" },\n      body: JSON.stringify({ mode }),\n    });\n    if (!res.ok) {\n      setLoading(null);\n      setStatus(\"failed\");\n    }\n  }\n\n  const getButtonText = (mode: \"draft\" | \"publish\") => {\n    if (loading === mode) {\n      if (status === \"draft\") {\n        return \"Saved as Draft!\";\n      }\n      if (status === \"published\") {\n        return \"Published!\";\n      }\n      if (status === \"failed\") {\n        return \"Failed\";\n      }\n      return \"Sending to TikTok...\";\n    }\n    if (status === \"draft\" && mode === \"draft\") {\n      return \"Sent as Draft ✓\";\n    }\n    if (status === \"published\" && mode === \"publish\") {\n      return \"Published ✓\";\n    }\n    return mode === \"draft\" ? \"Post as Draft\" : \"Publish\";\n  };\n\n  const getButtonClass = (mode: \"draft\" | \"publish\") => {\n    const base = \"rounded-2xl px-3 py-2 border\";\n    if (loading === mode && (status === \"draft\" || status === \"published\")) {\n      return `${base} bg-green-600 text-white border-green-600`;\n    }\n    if (loading === mode && status === \"failed\") {\n      return `${base} bg-red-600 text-white border-red-600`;\n    }\n    if ((status === \"draft\" && mode === \"draft\") || (status === \"published\" && mode === \"publish\")) {\n      return `${base} bg-green-600/20 border-green-600`;\n    }\n    return base;\n  };\n\n  return (\n    <div className=\"flex gap-2\">\n      <button\n        onClick={() => send(\"draft\")}\n        disabled={loading !== null}\n        className={getButtonClass(\"draft\")}\n      >\n        {getButtonText(\"draft\")}\n      </button>\n      <button\n        onClick={() => send(\"publish\")}\n        disabled={loading !== null}\n        className={getButtonClass(\"publish\")}\n      >\n        {getButtonText(\"publish\")}\n      </button>\n    </div>\n  );\n}\n","size_bytes":2876},"src/services/segmentation-v2.ts":{"content":"import { TranscriptSegment, TranscriptWord, isSentenceBoundaryToken } from './openai'\nimport { SceneChange } from './ffmpeg'\nimport { Chapter } from './youtube'\n\nexport interface EnhancedSegment {\n  startSec: number\n  endSec: number\n  durationSec: number\n  words: TranscriptWord[]\n  text: string\n  hook: string\n  score: number\n  features: SegmentFeatures\n  rationaleShort: string\n  durationChoice: 't60' | 't120'\n  chapterTitle?: string\n}\n\nexport interface SegmentFeatures {\n  hookScore: number\n  retentionScore: number\n  clarityScore: number\n  visualScore: number\n  noveltyScore: number\n  engagementScore: number\n  safetyScore: number\n  speechRate: number\n  pauseDensity: number\n  energyLevel: number\n  hasQuestion: boolean\n  hasBoldClaim: boolean\n  hasNumbers: boolean\n  sceneChangeCount: number\n  wordCount: number\n  coherenceScore: number\n  closureScore: number\n  arcScore: number\n  semanticDensity: number\n}\n\nexport interface CommentHotspot { timeSec: number; density: number }\n\nexport function mineTimestampsFromComments(comments: { text: string }[]): number[] {\n  const ts = /(?<!\\d)(\\d{1,2}):(\\d{2})(?::(\\d{2}))?(?!\\d)/g\n  const marks: number[] = []\n  for (const c of comments) {\n    const m = c.text.matchAll(ts)\n    for (const x of m) {\n      const h = x[3] ? parseInt(x[1], 10) : 0\n      const mm = x[3] ? parseInt(x[2], 10) : parseInt(x[1], 10)\n      const ss = x[3] ? parseInt(x[3], 10) : parseInt(x[2], 10)\n      const sec = h * 3600 + mm * 60 + ss\n      marks.push(sec)\n    }\n  }\n  return clusterPeaks(marks, 30)\n}\n\nfunction clusterPeaks(xs: number[], windowSec: number): number[] {\n  if (xs.length === 0) { return [] }\n  const s = [...xs].sort((a, b) => a - b)\n  const peaks: number[] = []\n  let acc: number[] = []\n  for (const x of s) {\n    if (acc.length === 0) { acc.push(x) }\n    else {\n      const last = acc[acc.length - 1]\n      if (x - last <= windowSec) { acc.push(x) }\n      else { peaks.push(median(acc)); acc = [x] }\n    }\n  }\n  if (acc.length > 0) { peaks.push(median(acc)) }\n  return peaks\n}\n\nfunction median(xs: number[]): number {\n  if (xs.length === 0) { return 0 }\n  const s = [...xs].sort((a, b) => a - b)\n  const m = Math.floor(s.length / 2)\n  return s.length % 2 ? s[m] : Math.floor((s[m - 1] + s[m]) / 2)\n}\n\nexport function generateChapterWindows(chapters: Chapter[], introIdx: number | null): Array<{ start: number; end: number; chapterTitle: string }> {\n  const ws: Array<{ start: number; end: number; chapterTitle: string }> = []\n  for (let i = 0; i < chapters.length; i++) {\n    if (introIdx !== null && i === introIdx) { continue }\n    const c = chapters[i]\n    const span = Math.max(0, c.endSec - c.startSec)\n    const step = Math.max(50, Math.floor(span * 0.12))\n    let t = c.startSec\n    const chapterWindows: Array<{ start: number; end: number; chapterTitle: string }> = []\n    while (t + 45 <= c.endSec) { const end = Math.min(c.endSec, t + 135); chapterWindows.push({ start: t, end, chapterTitle: c.title }); t += step }\n    if (chapterWindows.length === 0) { chapterWindows.push({ start: c.startSec, end: Math.min(c.endSec, c.startSec + Math.min(135, span || 135)), chapterTitle: c.title }) }\n    else {\n      const last = chapterWindows[chapterWindows.length - 1]\n      if (last.end < c.endSec - 8) {\n        const tailStart = Math.max(c.startSec, c.endSec - 135)\n        if (chapterWindows[chapterWindows.length - 1].start !== tailStart) { chapterWindows.push({ start: tailStart, end: c.endSec, chapterTitle: c.title }) }\n      }\n    }\n    ws.push(...chapterWindows)\n  }\n  return ws\n}\n\nfunction generateFullCoverageWindows(allWords: TranscriptWord[], videoDuration: number): Array<{ start: number; end: number; chapterTitle: string }> {\n  const lastEnd = allWords.length > 0 ? allWords[allWords.length - 1].end : 0\n  const coverageEnd = Math.max(videoDuration, lastEnd)\n  if (coverageEnd <= 0) { return [] }\n  const windows: Array<{ start: number; end: number; chapterTitle: string }> = []\n  const stride = coverageEnd > 1800 ? 90 : 65\n  const winLen = 135\n  for (let start = 0; start < coverageEnd; start += stride) { const end = Math.min(coverageEnd, start + winLen); windows.push({ start, end, chapterTitle: 'Full Video' }) }\n  if (windows.length === 0) { windows.push({ start: 0, end: coverageEnd, chapterTitle: 'Full Video' }) }\n  else {\n    const last = windows[windows.length - 1]\n    if (last.end < coverageEnd - 5) {\n      const tailStart = Math.max(0, coverageEnd - winLen)\n      if (windows[windows.length - 1].start !== tailStart) { windows.push({ start: tailStart, end: coverageEnd, chapterTitle: 'Full Video' }) }\n      else { windows[windows.length - 1] = { start: tailStart, end: coverageEnd, chapterTitle: 'Full Video' } }\n    }\n  }\n  return windows\n}\n\nfunction detectHookPatterns(text: string): { hasQuestion: boolean; hasBoldClaim: boolean; hasNumbers: boolean } {\n  const hasQuestion = /^(how|what|why|when|where|who|can|will|should|is|are|do|does|did)\\s/i.test(text) || /\\?/.test(text)\n  const bold = [/^(this is|here's|the best|the worst|never|always|you need|you must|don't|stop)/i, /^(secret|truth|fact|proven|guaranteed|ultimate|perfect)/i, /(vs\\.|versus|vs|compared to)/i, /^(shocking|amazing|incredible|unbelievable)/i]\n  const hasBoldClaim = bold.some(p => p.test(text))\n  const hasNumbers = /\\b\\d+\\b/.test(text)\n  return { hasQuestion, hasBoldClaim, hasNumbers }\n}\n\nfunction analyzeSpeechDynamics(words: TranscriptWord[], start: number): { rate: number; pauseDensity: number; energy: number } {\n  if (words.length === 0) { return { rate: 0, pauseDensity: 1, energy: 0 } }\n  const w = words.filter(x => x.start - start < 5)\n  if (w.length === 0) { return { rate: 0, pauseDensity: 1, energy: 0 } }\n  const dur = Math.max(0.1, w[w.length - 1].end - w[0].start)\n  const rate = w.length / dur\n  let gap = 0\n  for (let i = 0; i < w.length - 1; i++) { gap = gap + Math.max(0, w[i + 1].start - w[i].end) }\n  const pauseDensity = gap / dur\n  const energy = w.filter(x => /[A-Z]{2,}/.test(x.word) || /[!?]/.test(x.word) || x.word.length > 8).length / w.length\n  return { rate, pauseDensity, energy }\n}\n\nconst STOP = new Set(['a','about','above','after','again','against','all','am','an','and','any','are','as','at','be','because','been','before','being','below','between','both','but','by','could','did','do','does','doing','down','during','each','few','for','from','further','had','has','have','having','he','her','here','hers','herself','him','himself','his','how','i','if','in','into','is','it','its','itself','me','more','most','my','myself','no','nor','not','of','off','on','once','only','or','other','our','ours','ourselves','out','over','own','same','she','should','so','some','such','than','that','the','their','theirs','them','themselves','then','there','these','they','this','those','through','to','too','under','until','up','very','was','we','were','what','when','where','which','while','who','whom','why','with','you','your','yours','yourself','yourselves'])\n\nfunction calcFeatures(words: TranscriptWord[], start: number, end: number, scenes: SceneChange[], hotspots: number[]): SegmentFeatures {\n  const text = words.map(w => w.word).join(' ')\n  const hookText = words.filter(w => w.start - start < 3).map(w => w.word).join(' ').trim()\n  const lower = words.map(w => w.word.toLowerCase())\n  const clean = lower.map(w => w.replace(/[^a-z0-9']/gi, ''))\n  const content = clean.filter(w => w.length > 2 && !STOP.has(w))\n  const semanticDensity = Math.min(1, content.length / Math.max(1, words.length))\n  const hp = detectHookPatterns(hookText || text.substring(0, 100))\n  const dyn = analyzeSpeechDynamics(words, start)\n  const sc = scenes.filter(s => s.timeSec >= start && s.timeSec <= end).length\n  let hookScore = 0.5\n  if (hp.hasQuestion) { hookScore = hookScore + 0.2 }\n  if (hp.hasBoldClaim) { hookScore = hookScore + 0.2 }\n  if (hp.hasNumbers) { hookScore = hookScore + 0.1 }\n  if (dyn.energy > 0.3) { hookScore = hookScore + 0.15 }\n  if (dyn.rate > 2.5) { hookScore = hookScore + 0.1 }\n  if (hookScore > 1) { hookScore = 1 }\n  let retentionScore = 0.5\n  if (dyn.rate > 2) { retentionScore = retentionScore + 0.2 }\n  if (dyn.pauseDensity < 0.2) { retentionScore = retentionScore + 0.15 }\n  if (sc >= 2 && sc <= 4) { retentionScore = retentionScore + 0.15 }\n  if (retentionScore > 1) { retentionScore = 1 }\n  const filler = words.filter(w => /^(um|uh|like|you know|sort of|kind of)$/i.test(w.word.trim())).length\n  const fillerRatio = filler / Math.max(1, words.length)\n  const clarityScore = Math.max(0, 1 - 2 * fillerRatio)\n  const visualScore = sc >= 1 && sc <= 6 ? 0.8 : 0.5\n  const noveltyScore = 0.6\n  const nearHot = hotspots.some(h => Math.abs(h - start) < 30)\n  const engagementScore = nearHot ? 0.8 : 0.4\n  const bad = /\\b(fuck|shit|damn|hell|ass|bitch)\\b/i.test(text)\n  const safetyScore = bad ? 0.3 : 0.9\n  const sentEnd = text.match(/[.!?]/g)?.length ?? 0\n  const sent = Math.max(1, sentEnd || Math.ceil((end - start) / 7))\n  const avgWps = words.length / sent\n  let coherenceScore = 0.55\n  if (avgWps >= 8 && avgWps <= 28) { coherenceScore = coherenceScore + 0.2 }\n  if (semanticDensity > 0.55) { coherenceScore = coherenceScore + 0.15 }\n  if (fillerRatio < 0.12) { coherenceScore = coherenceScore + 0.1 }\n  if (clarityScore > 0.75) { coherenceScore = coherenceScore + 0.05 }\n  if (coherenceScore > 1) { coherenceScore = 1 }\n  if (coherenceScore < 0.3) { coherenceScore = 0.3 }\n  const closeWin = words.filter(w => end - w.end < 4)\n  const closeText = closeWin.map(w => w.word.toLowerCase()).join(' ')\n  const last = closeWin[closeWin.length - 1]?.word ?? words[words.length - 1]?.word ?? ''\n  const hardStop = isSentenceBoundaryToken(last)\n  const closurePhrases = [\"that's why\",\"so you can\",\"and that's\",\"that's how\",\"in the end\",\"the point is\"]\n  const trailingFill = /(um|uh|like)$/i.test(last?.trim?.() || '')\n  let closureScore = 0.45\n  if (hardStop) { closureScore = closureScore + 0.25 }\n  if (closurePhrases.some(p => closeText.includes(p))) { closureScore = closureScore + 0.15 }\n  if (closeWin.length > 0 && closeWin.some(w => /\\bso\\b|\\btherefore\\b|\\bmeaning\\b/i.test(w.word))) { closureScore = closureScore + 0.1 }\n  if (trailingFill) { closureScore = closureScore - 0.15 }\n  if (closureScore > 1) { closureScore = 1 }\n  if (closureScore < 0.2) { closureScore = 0.2 }\n  const resolution = ['because','so','that\\'s why','that means','which means','therefore','result','here\\'s']\n  const payoff = ['so you can','that\\'s how','in the end','the reason','the secret','so the','what happens']\n  const lt = text.toLowerCase()\n  const hasRes = resolution.some(k => lt.includes(k))\n  const hasPay = payoff.some(k => lt.includes(k))\n  const earlyQ = words.filter(w => w.start - start < 6).some(w => /\\?$/.test(w.word) || /^(how|why|what|when|where|who|can|should|would)\\b/i.test(w.word))\n  let arcScore = 0.45\n  if (hp.hasQuestion || earlyQ) { arcScore = arcScore + 0.2 }\n  if (hasRes) { arcScore = arcScore + 0.2 }\n  if (hasPay) { arcScore = arcScore + 0.1 }\n  if (closureScore > 0.7) { arcScore = arcScore + 0.05 }\n  if (arcScore > 1) { arcScore = 1 }\n  if (arcScore < 0.25) { arcScore = 0.25 }\n  return { hookScore, retentionScore, clarityScore, visualScore, noveltyScore, engagementScore, safetyScore, speechRate: dyn.rate, pauseDensity: dyn.pauseDensity, energyLevel: dyn.energy, hasQuestion: hp.hasQuestion, hasBoldClaim: hp.hasBoldClaim, hasNumbers: hp.hasNumbers, sceneChangeCount: sc, wordCount: words.length, coherenceScore, closureScore, arcScore, semanticDensity }\n}\n\nfunction scoreSegment(f: SegmentFeatures): number {\n  return 0.24 * f.hookScore + 0.18 * f.retentionScore + 0.12 * f.clarityScore + 0.10 * f.coherenceScore + 0.10 * f.closureScore + 0.08 * f.arcScore + 0.08 * f.engagementScore + 0.05 * f.noveltyScore + 0.03 * f.visualScore + 0.02 * f.safetyScore\n}\n\nfunction chooseDuration(targets: number[], candidate: number, f: SegmentFeatures): { target: number; choice: 't60' | 't120' } {\n  const t60 = 60\n  const t120 = 120\n  const can120 = candidate >= 105\n  const longSignals = (f.retentionScore + f.closureScore + f.arcScore) / 3 > 0.68 && f.coherenceScore > 0.65 && f.semanticDensity > 0.55\n  let pick = t60\n  if (can120 && longSignals) { pick = t120 }\n  const choice: 't60' | 't120' = pick === 120 ? 't120' : 't60'\n  return { target: pick, choice }\n}\n\nfunction withinTolerance(d: number, target: number): boolean {\n  const low = target - 15\n  const high = target + 15\n  if (d < low) { return false }\n  if (d > high) { return false }\n  return true\n}\n\nfunction adjustToBoundary(words: TranscriptWord[], start: number, hardEnd: number, target: number): { slice: TranscriptWord[]; end: number } {\n  if (words.length === 0) { return { slice: [], end: start } }\n  const desired = start + target\n  const maxEnd = Math.min(hardEnd, start + target + 15)\n  const minEnd = Math.max(start + target - 15, start + 20)\n  let idx = words.findIndex(w => w.end >= desired)\n  if (idx === -1) { idx = words.length - 1 }\n  let pick = idx\n  for (let i = idx; i < words.length; i++) {\n    const w = words[i]\n    if (w.end > maxEnd) { break }\n    const tok = w.word\n    const next = words[i + 1]\n    const gap = next ? next.start - w.end : 0\n    if (isSentenceBoundaryToken(tok)) { pick = i; break }\n    if (gap >= 0.8 && w.end >= minEnd) { pick = i; break }\n  }\n  if (words[pick].end - start < 20 && words[words.length - 1].end - start >= 20) { while (pick < words.length - 1 && words[pick].end - start < 20) { pick = pick + 1 } }\n  const end = Math.min(maxEnd, words[pick].end)\n  const slice = words.filter(w => w.end <= end + 1e-3)\n  return { slice, end }\n}\n\nfunction qualityGuards(s: EnhancedSegment): boolean {\n  const first3 = s.words.filter(w => w.start - s.startSec < 3)\n  if (first3.length < 3) { return false }\n  if (s.features.safetyScore < 0.5) { return false }\n  if (s.features.clarityScore < 0.3) { return false }\n  if (s.features.coherenceScore < 0.45) { return false }\n  if (s.features.closureScore < 0.4) { return false }\n  return true\n}\n\nfunction jaccard(a: string, b: string): number {\n  const sa = new Set(a.toLowerCase().split(/\\s+/))\n  const sb = new Set(b.toLowerCase().split(/\\s+/))\n  let inter = 0\n  for (const w of sa) { if (sb.has(w)) { inter = inter + 1 } }\n  const uni = sa.size + sb.size - inter\n  if (uni === 0) { return 0 }\n  return inter / uni\n}\n\nfunction diversify(xs: EnhancedSegment[], thr: number): EnhancedSegment[] {\n  if (xs.length === 0) { return [] }\n  const s = [...xs].sort((a, b) => b.score - a.score)\n  const out: EnhancedSegment[] = [s[0]]\n  for (let i = 1; i < s.length; i++) {\n    let ok = true\n    for (const e of out) { if (jaccard(s[i].text, e.text) > thr) { ok = false; break } }\n    if (ok) { out.push(s[i]) }\n  }\n  return out\n}\n\nfunction removeOverlaps(segments: EnhancedSegment[]): EnhancedSegment[] {\n  if (segments.length === 0) { return [] }\n  const sorted = [...segments].sort((a, b) => b.score - a.score)\n  const out: EnhancedSegment[] = []\n  for (const s of sorted) {\n    let overlaps = false\n    for (const e of out) { if (!(s.endSec <= e.startSec || e.endSec <= s.startSec)) { overlaps = true; break } }\n    if (!overlaps) { out.push(s) }\n  }\n  return out.sort((a, b) => a.startSec - b.startSec)\n}\n\nfunction rationale(f: SegmentFeatures, score: number): string {\n  const rs: Array<{ t: string; v: number }> = []\n  if (f.hookScore > 0.7) { rs.push({ t: 'strong hook', v: f.hookScore }) }\n  if (f.retentionScore > 0.7) { rs.push({ t: 'high retention', v: f.retentionScore }) }\n  if (f.clarityScore > 0.8) { rs.push({ t: 'clear message', v: f.clarityScore }) }\n  if (f.engagementScore > 0.7) { rs.push({ t: 'audience hotspot', v: f.engagementScore }) }\n  if (f.coherenceScore > 0.7) { rs.push({ t: 'coherent flow', v: f.coherenceScore }) }\n  if (f.closureScore > 0.65) { rs.push({ t: 'satisfying payoff', v: f.closureScore }) }\n  if (f.arcScore > 0.65) { rs.push({ t: 'question→answer arc', v: f.arcScore }) }\n  if (f.hasQuestion) { rs.push({ t: 'question hook', v: 0.8 }) }\n  if (f.hasBoldClaim) { rs.push({ t: 'bold claim', v: 0.75 }) }\n  if (f.sceneChangeCount >= 2 && f.sceneChangeCount <= 4) { rs.push({ t: 'good visual pacing', v: 0.7 }) }\n  const top = rs.sort((a, b) => b.v - a.v).slice(0, 3).map(x => x.t)\n  if (top.length === 0) { return `Segment scored ${(score * 100).toFixed(0)}/100` }\n  return `Strong because: ${top.join(', ')}`\n}\n\nfunction findIntroChapterIndex(chapters: Chapter[], detectedLanguage?: string): number | null {\n  if (chapters.length === 0) { return null }\n  const map: Record<string, string[]> = { en: ['intro','introduction','opening','welcome'], es: ['intro','introduccion','introducción','apertura','inicio'], pt: ['intro','introducao','introdução','apresentação','abertura'], fr: ['intro','introduction','ouverture'], de: ['intro','einführung','einleitung'] }\n  const keys = detectedLanguage && map[detectedLanguage] ? map[detectedLanguage] : Object.values(map).flat()\n  const t = chapters[0].title.toLowerCase()\n  for (const k of keys) { if (t.includes(k)) { return 0 } }\n  return null\n}\n\nexport function detectEnhancedSegments(transcript: TranscriptSegment[], sceneChanges: SceneChange[], chapters: Chapter[], videoDuration: number, commentHotspots: number[] = []): EnhancedSegment[] {\n  const all: TranscriptWord[] = []\n  for (const s of transcript) { for (const w of s.words) { all.push(w) } }\n  if (all.length === 0) { return [] }\n  const lang = transcript[0]?.language\n  const introIdx = findIntroChapterIndex(chapters, lang)\n  let windows = chapters.length > 0 ? generateChapterWindows(chapters, introIdx) : generateFullCoverageWindows(all, videoDuration)\n  if (chapters.length > 0) {\n    const fb = generateFullCoverageWindows(all, videoDuration)\n    const maxEnd = windows.reduce((m, w) => Math.max(m, w.end), 0)\n    const covEnd = fb.length > 0 ? fb[fb.length - 1].end : maxEnd\n    if (covEnd > maxEnd + 5) { windows = windows.concat(fb.filter(w => w.start >= maxEnd - 60)) }\n  }\n  windows = windows.sort((a, b) => a.start - b.start).filter((w, i, arr) => i === 0 || w.start !== arr[i - 1].start || w.end !== arr[i - 1].end)\n  const targets = [60, 120]\n  const cands: EnhancedSegment[] = []\n  for (const win of windows) {\n    const ww = all.filter(w => w.start >= win.start && w.start < win.end)\n    if (ww.length < 12) { continue }\n    const cuts: number[] = [0]\n    for (let i = 0; i < ww.length - 1; i++) { const g = ww[i + 1].start - ww[i].end; if (g >= 0.35 && g <= 1.2) { cuts.push(i + 1) } }\n    cuts.push(ww.length)\n    for (let i = 0; i < cuts.length - 1; i++) {\n      for (let j = i + 1; j < cuts.length; j++) {\n        const seg = ww.slice(cuts[i], cuts[j])\n        if (seg.length < 12) { continue }\n        const s = seg[0].start\n        const hardEnd = seg[seg.length - 1].end\n        const prelimF = calcFeatures(seg, s, hardEnd, sceneChanges, commentHotspots)\n        const prelimScore = scoreSegment(prelimF)\n        if (prelimScore < 0.5) { continue }\n        const pick = chooseDuration(targets, hardEnd - s, prelimF)\n        const adj = adjustToBoundary(seg, s, hardEnd, pick.target)\n        const d = adj.end - s\n        if (!withinTolerance(d, pick.target)) { continue }\n        if (adj.slice.length < 12) { continue }\n        const finalF = calcFeatures(adj.slice, s, adj.end, sceneChanges, commentHotspots)\n        const finalScore = scoreSegment(finalF)\n        if (finalScore < 0.52) { continue }\n        const text = adj.slice.map(w => w.word).join(' ')\n        const hook = adj.slice.filter(w => w.start - s < 3).map(w => w.word).join(' ').trim()\n        const r = rationale(finalF, finalScore)\n        cands.push({ startSec: s, endSec: adj.end, durationSec: d, words: adj.slice, text, hook: hook || text.substring(0, 60), score: finalScore, features: finalF, rationaleShort: r, durationChoice: pick.choice, chapterTitle: win.chapterTitle })\n      }\n    }\n  }\n  const guarded = cands.filter(qualityGuards)\n  const diversified = diversify(guarded, 0.7)\n  const nonOverlap = removeOverlaps(diversified)\n  return nonOverlap.slice(0, 12)\n}\n","size_bytes":19983},"src/worker-tiktok.ts":{"content":"import { Worker, Job } from \"bullmq\";\nimport { connection } from \"@/src/lib/queue\";\nimport { prisma } from \"@/src/lib/prisma\";\nimport { tmpdir } from \"os\";\nimport { join } from \"path\";\nimport { existsSync, mkdirSync, rmSync, createWriteStream } from \"fs\";\nimport { S3Client, GetObjectCommand } from \"@aws-sdk/client-s3\";\nimport { Readable } from \"stream\";\nimport { pipeline } from \"stream/promises\";\nimport { promises as fsp } from \"fs\";\n\nimport {\n  ensureAccessToken,\n  initUpload,\n  uploadToUrl,\n  publishVideo,\n} from \"@/src/services/tiktok\";\nimport { decrypt, encrypt } from \"@/src/lib/encryption\";\n\ntype TikTokJob = {\n  userId: string;\n  clipId: string;\n  mode: \"draft\" | \"publish\";\n};\n\nconst s3 = new S3Client({\n  region: process.env.S3_REGION as string,\n  endpoint: process.env.S3_ENDPOINT as string,\n  credentials: {\n    accessKeyId: process.env.S3_ACCESS_KEY_ID as string,\n    secretAccessKey: process.env.S3_SECRET_ACCESS_KEY as string,\n  },\n  forcePathStyle: true,\n});\n\nasync function downloadS3ToFile(key: string, outPath: string) {\n  const obj = await s3.send(\n    new GetObjectCommand({\n      Bucket: process.env.S3_BUCKET as string,\n      Key: key,\n    }),\n  );\n\n  const body = obj.Body as any;\n\n  if (!body) {\n    throw new Error(\"S3 body empty\");\n  }\n\n  const ws = createWriteStream(outPath);\n\n  if (typeof body.pipe === \"function\") {\n    await pipeline(body as Readable, ws);\n    return;\n  }\n\n  if (typeof body.transformToWebStream === \"function\") {\n    const web = body.transformToWebStream();\n    await pipeline(Readable.fromWeb(web as any), ws);\n    return;\n  }\n\n  if (typeof body.getReader === \"function\") {\n    await pipeline(Readable.fromWeb(body as any), ws);\n    return;\n  }\n\n  if (typeof body.arrayBuffer === \"function\") {\n    const buf = Buffer.from(await body.arrayBuffer());\n    await fsp.writeFile(outPath, buf);\n    return;\n  }\n\n  throw new Error(\"Unsupported S3 body type\");\n}\n\nfunction buildCaption(\n  customTitle: string | null,\n  customDescription: string | null,\n  fallbackTitle: string | null,\n  hook: string | null,\n  tags: string[] | null,\n) {\n  if (customTitle || customDescription) {\n    const parts = [customTitle, customDescription].filter(Boolean);\n    const tagStr = (tags || [])\n      .slice(0, 6)\n      .map((t) => (t.startsWith(\"#\") ? t : `#${t}`))\n      .join(\" \");\n    const full = [...parts, tagStr].filter(Boolean).join(\" \").trim();\n    if (full.length <= 200) {\n      return full;\n    }\n    return full.slice(0, 200);\n  }\n  const base = [hook || \"\", fallbackTitle || \"\"]\n    .filter(Boolean)\n    .join(\" · \")\n    .trim();\n  const tagStr = (tags || [])\n    .slice(0, 6)\n    .map((t) => (t.startsWith(\"#\") ? t : `#${t}`))\n    .join(\" \");\n  const full = [base, tagStr].filter(Boolean).join(\" \").trim();\n  if (full.length <= 200) {\n    return full;\n  }\n  return full.slice(0, 200);\n}\n\nasync function processTikTok(job: Job<TikTokJob>) {\n  const { userId, clipId, mode } = job.data;\n\n  if (!userId) {\n    throw new Error(\"userId is required\");\n  }\n\n  if (!clipId) {\n    throw new Error(\"clipId is required\");\n  }\n\n  if (!(mode === \"draft\" || mode === \"publish\")) {\n    throw new Error(\"invalid mode\");\n  }\n\n  const conn = await prisma.tikTokConnection.findFirst({\n    where: { userId },\n    orderBy: { updatedAt: \"desc\" },\n  });\n\n  if (!conn) {\n    throw new Error(\"tiktok connection not found\");\n  }\n\n  const clip = await prisma.clip.findUnique({\n    where: { id: clipId },\n    select: {\n      id: true,\n      s3VideoKey: true,\n      rationaleShort: true,\n      tags: true,\n      tiktokTitle: true,\n      tiktokDescription: true,\n      Video: { select: { title: true } },\n    },\n  });\n\n  if (!clip) {\n    throw new Error(\"clip not found\");\n  }\n\n  if (!clip.s3VideoKey) {\n    throw new Error(\"clip s3VideoKey missing\");\n  }\n\n  const ensured = await ensureAccessToken({\n    accessToken: decrypt(conn.accessToken),\n    refreshToken: decrypt(conn.refreshToken),\n    expiresAt: conn.expiresAt,\n  });\n\n  if (ensured.rotated) {\n    await prisma.tikTokConnection.update({\n      where: { id: conn.id },\n      data: {\n        accessToken: encrypt(ensured.accessToken),\n        refreshToken: encrypt(ensured.refreshToken),\n        expiresAt: ensured.expiresAt,\n      },\n    });\n  }\n\n  const workDir = join(tmpdir(), `tiktok_${clipId}`);\n  if (!existsSync(workDir)) {\n    mkdirSync(workDir, { recursive: true });\n  }\n\n  const filePath = join(workDir, \"clip.mp4\");\n\n  try {\n    await downloadS3ToFile(clip.s3VideoKey, filePath);\n\n    const init = await initUpload({\n      accessToken: ensured.accessToken,\n      filePath,\n    });\n\n    await uploadToUrl({\n      uploadUrl: init.uploadUrl,\n      filePath,\n      fileSize: init.fileSize,\n      chunkSizeBytes: init.chunkSizeBytes,\n      totalChunks: init.totalChunks,\n    });\n\n    if (mode === \"publish\") {\n      const caption = buildCaption(\n        clip.tiktokTitle || null,\n        clip.tiktokDescription || null,\n        clip.Video?.title || null,\n        clip.rationaleShort || null,\n        (clip.tags as string[]) || [],\n      );\n\n      await publishVideo({\n        accessToken: ensured.accessToken,\n        publishId: init.publishId,\n        caption,\n        mode,\n      });\n    }\n\n    await prisma.clip.update({\n      where: { id: clipId },\n      data: {\n        tiktokPublishId: init.publishId,\n        tiktokStatus: mode === \"draft\" ? \"draft\" : \"published\",\n      },\n    });\n\n    return { publishId: init.publishId };\n  } finally {\n    rmSync(workDir, { recursive: true, force: true });\n  }\n}\n\nexport const tiktokWorker = new Worker<TikTokJob>(\n  \"tiktok.post\",\n  processTikTok,\n  {\n    connection,\n    concurrency: 2,\n    lockDuration: 600000,\n    lockRenewTime: 30000,\n    maxStalledCount: 1,\n  },\n);\n\ntiktokWorker.on(\"completed\", (job) => {\n  console.log(`tiktok.post ${job.id} completed`);\n});\n\ntiktokWorker.on(\"failed\", async (job, err) => {\n  if (job?.data?.clipId) {\n    await prisma.clip\n      .update({\n        where: { id: job.data.clipId },\n        data: { tiktokStatus: \"failed\" },\n      })\n      .catch(() => {});\n  }\n  console.error(`tiktok.post ${job?.id} failed`, err);\n});\n","size_bytes":6086},"app/api/tiktok/clip/[id]/status/route.ts":{"content":"import { NextRequest, NextResponse } from \"next/server\";\nimport { prisma } from \"@/src/lib/prisma\";\nimport { getCurrentUserId } from \"@/src/lib/session\";\n\nexport async function GET(\n  req: NextRequest,\n  { params }: { params: { id: string } },\n) {\n  const userId = await getCurrentUserId();\n  if (!userId) {\n    return NextResponse.json({ ok: false }, { status: 401 });\n  }\n  const clip = await prisma.clip.findFirst({\n    where: { id: params.id, Video: { userId } },\n    select: { tiktokStatus: true, tiktokPublishId: true },\n  });\n  if (!clip) {\n    return NextResponse.json({ ok: false }, { status: 404 });\n  }\n  return NextResponse.json({ \n    ok: true, \n    tiktokStatus: clip.tiktokStatus,\n    tiktokPublishId: clip.tiktokPublishId\n  });\n}\n","size_bytes":746},"src/services/youtube-comments.ts":{"content":"export interface YoutubeComment {\n  text: string\n  timestamp?: number\n  likeCount: number\n}\n\nexport async function fetchVideoComments(videoId: string, maxResults: number = 100): Promise<YoutubeComment[]> {\n  const apiKey = process.env.YOUTUBE_API_KEY\n  \n  if (!apiKey) {\n    console.log('YouTube API key not configured, skipping comment fetching')\n    return []\n  }\n  \n  try {\n    const url = new URL('https://www.googleapis.com/youtube/v3/commentThreads')\n    url.searchParams.set('part', 'snippet')\n    url.searchParams.set('videoId', videoId)\n    url.searchParams.set('maxResults', String(Math.min(maxResults, 100)))\n    url.searchParams.set('order', 'relevance')\n    url.searchParams.set('textFormat', 'plainText')\n    url.searchParams.set('key', apiKey)\n    \n    const response = await fetch(url.toString())\n    \n    if (response.status === 403) {\n      const errorData = await response.json()\n      if (errorData.error?.errors?.[0]?.reason === 'commentsDisabled') {\n        console.log('Comments are disabled for this video')\n        return []\n      }\n      throw new Error(`YouTube API forbidden: ${JSON.stringify(errorData)}`)\n    }\n    \n    if (!response.ok) {\n      const errorText = await response.text()\n      throw new Error(`YouTube API error ${response.status}: ${errorText}`)\n    }\n    \n    const data = await response.json()\n    const comments: YoutubeComment[] = []\n    \n    for (const item of data.items || []) {\n      const snippet = item.snippet?.topLevelComment?.snippet\n      if (snippet?.textDisplay) {\n        comments.push({\n          text: snippet.textDisplay,\n          likeCount: snippet.likeCount || 0\n        })\n      }\n    }\n    \n    console.log(`Fetched ${comments.length} comments for video ${videoId}`)\n    return comments\n  }\n  catch (error) {\n    console.error('Failed to fetch YouTube comments:', error)\n    return []\n  }\n}\n\nexport function extractVideoIdFromUrl(url: string): string | null {\n  try {\n    const urlObj = new URL(url)\n    \n    if (urlObj.hostname.includes('youtube.com')) {\n      return urlObj.searchParams.get('v')\n    }\n    \n    if (urlObj.hostname === 'youtu.be') {\n      return urlObj.pathname.slice(1)\n    }\n    \n    return null\n  }\n  catch {\n    return null\n  }\n}\n","size_bytes":2223},"app/api/tiktok/status/route.ts":{"content":"import { NextRequest, NextResponse } from \"next/server\";\nimport { prisma } from \"@/src/lib/prisma\";\nimport { decrypt } from \"@/src/lib/encryption\";\nimport { ensureAccessToken } from \"@/src/services/tiktok\";\n\nexport async function GET(req: NextRequest) {\n  try {\n    const publishId = req.nextUrl.searchParams.get(\"publishId\");\n    if (!publishId) {\n      return NextResponse.json(\n        { error: \"publishId is required\" },\n        { status: 400 },\n      );\n    }\n\n    const sessionUserId = req.headers.get(\"x-user-id\"); // or however you identify user\n    if (!sessionUserId) {\n      return NextResponse.json({ error: \"unauthorized\" }, { status: 401 });\n    }\n\n    const conn = await prisma.tikTokConnection.findFirst({\n      where: { userId: sessionUserId },\n      orderBy: { updatedAt: \"desc\" },\n    });\n\n    if (!conn) {\n      return NextResponse.json(\n        { error: \"TikTok connection not found\" },\n        { status: 404 },\n      );\n    }\n\n    const ensured = await ensureAccessToken({\n      accessToken: decrypt(conn.accessToken),\n      refreshToken: decrypt(conn.refreshToken),\n      expiresAt: conn.expiresAt,\n    });\n\n    const res = await fetch(\n      \"https://open.tiktokapis.com/v2/post/publish/status/fetch/\",\n      {\n        method: \"POST\",\n        headers: {\n          Authorization: `Bearer ${ensured.accessToken}`,\n          \"Content-Type\": \"application/json; charset=UTF-8\",\n        },\n        body: JSON.stringify({ publish_id: publishId }),\n      },\n    );\n\n    const data = await res.json();\n\n    return NextResponse.json({\n      ok: res.ok,\n      status: res.status,\n      data,\n    });\n  } catch (err: any) {\n    console.error(\"TikTok status check failed\", err);\n    return NextResponse.json(\n      { error: err.message || \"Unknown error\" },\n      { status: 500 },\n    );\n  }\n}\n","size_bytes":1805},"app/api/clips/[id]/route.ts":{"content":"import { NextRequest, NextResponse } from \"next/server\";\nimport { prisma } from \"@/src/lib/prisma\";\nimport { requireAuth } from \"@/src/lib/session\";\n\nexport async function DELETE(\n  req: NextRequest,\n  { params }: { params: { id: string } },\n) {\n  const session = await requireAuth();\n  if (!session.userId) {\n    return NextResponse.json({ error: \"Unauthorized\" }, { status: 401 });\n  }\n\n  if (!params?.id) {\n    return NextResponse.json({ error: \"Missing id\" }, { status: 400 });\n  }\n\n  const clip = await prisma.clip.findUnique({\n    where: { id: params.id },\n    select: { id: true },\n  });\n\n  if (!clip) {\n    return NextResponse.json({ error: \"Clip not found\" }, { status: 404 });\n  }\n\n  await prisma.clip.delete({ where: { id: params.id } });\n  return NextResponse.json({ ok: true });\n}\n","size_bytes":794},"app/api/videos/batch-delete/route.ts":{"content":"import { NextRequest, NextResponse } from \"next/server\";\nimport { requireAuth } from \"@/src/lib/session\";\nimport { prisma } from \"@/src/lib/prisma\";\nimport { videoQueue } from \"@/src/lib/queue\";\n\nexport async function DELETE(request: NextRequest) {\n  try {\n    const session = await requireAuth();\n    const body = await request.json().catch(() => ({}) as any);\n    const ids = Array.isArray(body.ids)\n      ? body.ids.filter((x: any) => typeof x === \"string\")\n      : [];\n\n    if (ids.length === 0) {\n      return NextResponse.json({ error: \"No ids provided\" }, { status: 400 });\n    }\n\n    const owned = await prisma.video.findMany({\n      where: { id: { in: ids }, userId: session.userId },\n      select: { id: true },\n    });\n    const ownedIds = owned.map((v) => v.id);\n\n    if (ownedIds.length === 0) {\n      return NextResponse.json(\n        { error: \"No matching videos found\" },\n        { status: 404 },\n      );\n    }\n\n    await Promise.all(\n      ownedIds.map(async (id) => {\n        const job = await videoQueue.getJob(id);\n        if (job) {\n          await job.remove();\n        }\n      }),\n    );\n\n    await prisma.$transaction([\n      prisma.clip.deleteMany({ where: { videoId: { in: ownedIds } } }),\n      prisma.video.deleteMany({\n        where: { id: { in: ownedIds }, userId: session.userId },\n      }),\n    ]);\n\n    return NextResponse.json({ success: true, deleted: ownedIds.length });\n  } catch (error) {\n    return NextResponse.json(\n      { error: \"Failed to delete selected videos\" },\n      { status: 500 },\n    );\n  }\n}\n","size_bytes":1547},"src/theme.ts":{"content":"\"use client\";\n\nimport { createTheme } from \"@mui/material/styles\";\n\nexport const theme = createTheme({\n  palette: {\n    mode: \"dark\",\n    primary: { main: \"#3b82f6\" },\n    secondary: { main: \"#14b8a6\" },\n    error: { main: \"#ef4444\" },\n    background: { default: \"#0b0f14\", paper: \"#111827\" },\n    text: { primary: \"#e5e7eb\", secondary: \"#9ca3af\" },\n  },\n  shape: { borderRadius: 14 },\n  typography: {\n    fontFamily: [\n      \"Inter\",\n      \"Roboto\",\n      \"sans-serif\",\n      \"Forever-Freedom-Regular\",\n    ].join(\",\"),\n  },\n});\n","size_bytes":530},"app/api/clips/[id]/update-subs/route.ts":{"content":"import { NextResponse } from \"next/server\";\nimport { prisma } from \"@/src/lib/prisma\";\nimport { requireAuth } from \"@/src/lib/session\";\nimport {\n  renderVerticalClip,\n  createWordByWordSrtFile,\n  createAssWordByWordFile,\n} from \"@/src/services/ffmpeg\";\nimport { tmpdir } from \"os\";\nimport { join } from \"path\";\nimport fs from \"fs/promises\";\nimport {\n  S3Client,\n  GetObjectCommand,\n  PutObjectCommand,\n} from \"@aws-sdk/client-s3\";\nimport { Readable } from \"stream\";\nimport { downloadVideo } from \"@/src/services/youtube\";\n\nconst tag = \"[update-subs]\";\nfunction log(...args: any[]) {\n  console.log(tag, ...args);\n}\nfunction logErr(...args: any[]) {\n  console.error(tag, ...args);\n}\n\nfunction s3() {\n  return new S3Client({\n    region: process.env.S3_REGION,\n    endpoint: process.env.S3_ENDPOINT,\n    credentials: {\n      accessKeyId: process.env.S3_ACCESS_KEY_ID || \"\",\n      secretAccessKey: process.env.S3_SECRET_ACCESS_KEY || \"\",\n    },\n    forcePathStyle: true,\n  });\n}\n\nasync function s3Download(key: string, localPath: string) {\n  log(\"s3Download:start\", { key, localPath });\n  const t0 = Date.now();\n  const client = s3();\n  const res = await client.send(\n    new GetObjectCommand({ Bucket: process.env.S3_BUCKET, Key: key }),\n  );\n  const stream = res.Body as Readable;\n  const chunks: Buffer[] = [];\n  for await (const chunk of stream) {\n    chunks.push(Buffer.from(chunk as Buffer));\n  }\n  await fs.writeFile(localPath, Buffer.concat(chunks));\n  log(\"s3Download:done\", { ms: Date.now() - t0 });\n}\n\nasync function s3Upload(\n  localPath: string,\n  key: string,\n  contentType: string = \"video/mp4\",\n) {\n  log(\"s3Upload:start\", { key, localPath, contentType });\n  const t0 = Date.now();\n  const client = s3();\n  const data = await fs.readFile(localPath);\n  await client.send(\n    new PutObjectCommand({\n      Bucket: process.env.S3_BUCKET,\n      Key: key,\n      Body: data,\n      ContentType: contentType,\n    }),\n  );\n  log(\"s3Upload:done\", { ms: Date.now() - t0 });\n}\n\nfunction publicUrlForKey(key: string) {\n  const base = process.env.PUBLIC_ASSETS_BASE_URL || \"\";\n  return `${base.replace(/\\/$/, \"\")}/${key}`;\n}\n\ntype W = { word: string; start: number; end: number };\n\nfunction coerceNumber(v: any): number | undefined {\n  if (typeof v === \"number\") {\n    return v;\n  }\n  if (typeof v === \"string\") {\n    const n = parseFloat(v);\n    if (Number.isFinite(n)) {\n      return n;\n    }\n  }\n  return undefined;\n}\n\nfunction expandUniform(text: string, start: number, end: number): W[] {\n  const tokens = text.split(/\\s+/).filter((t) => t.length > 0);\n  if (tokens.length === 0) {\n    return [];\n  }\n  const dur = Math.max(0, end - start);\n  const step = tokens.length > 0 ? dur / tokens.length : 0;\n  const out: W[] = [];\n  for (let i = 0; i < tokens.length; i++) {\n    const s = start + i * step;\n    const e = i === tokens.length - 1 ? end : start + (i + 1) * step;\n    out.push({ word: tokens[i], start: s, end: e });\n  }\n  return out;\n}\n\nfunction normalizeTranscript(raw: any): W[] {\n  if (typeof raw === \"string\") {\n    try {\n      raw = JSON.parse(raw);\n    } catch {\n      return [];\n    }\n  }\n  if (Array.isArray(raw)) {\n    const segments = raw;\n    const words: W[] = [];\n    const sample = segments[0]\n      ? {\n          start: segments[0].start,\n          end: segments[0].end,\n          hasWords: Array.isArray(segments[0].words),\n        }\n      : undefined;\n    log(\"transcript:segments:sample\", sample);\n    for (const seg of segments) {\n      const segStart = coerceNumber(seg.start);\n      const segEnd = coerceNumber(seg.end);\n      if (!Number.isFinite(segStart) || !Number.isFinite(segEnd)) {\n        continue;\n      }\n      if (Array.isArray(seg.words) && seg.words.length > 0) {\n        for (const w of seg.words) {\n          const ww =\n            typeof w.word === \"string\"\n              ? w.word\n              : typeof w.text === \"string\"\n                ? w.text\n                : \"\";\n          const ws = coerceNumber(w.start);\n          const we = coerceNumber(w.end);\n          if (ww && Number.isFinite(ws) && Number.isFinite(we)) {\n            words.push({\n              word: ww.trim(),\n              start: ws as number,\n              end: we as number,\n            });\n          }\n        }\n      } else {\n        const text = typeof seg.text === \"string\" ? seg.text : \"\";\n        const expanded = expandUniform(\n          text,\n          segStart as number,\n          segEnd as number,\n        );\n        for (const w of expanded) {\n          words.push(w);\n        }\n      }\n    }\n    return words;\n  }\n  if (raw && Array.isArray(raw.words)) {\n    return raw.words as W[];\n  }\n  return [];\n}\n\nfunction sliceToClip(\n  words: W[],\n  startSec: number,\n  endSec: number,\n  duration: number,\n): W[] {\n  const a = words\n    .filter((w) => {\n      if (w.end > startSec && w.start < endSec) {\n        return true;\n      }\n      return false;\n    })\n    .map((w) => ({\n      word: w.word,\n      start: Math.max(0, w.start - startSec),\n      end: Math.min(duration, w.end - startSec),\n    }))\n    .filter((w) => {\n      if (w.end > w.start) {\n        return true;\n      }\n      return false;\n    });\n  return a;\n}\n\nexport async function POST(req: Request, { params }: { params: { id: string } }) {\n  const reqId = `${params.id}-${Date.now()}`;\n  log(\"POST:start\", { reqId, clipId: params.id });\n\n  try {\n    const body = await req.json();\n    const rawWords = Number(body.wordsPerSubtitle);\n    const wordsPerSubtitle = Number.isFinite(rawWords) ? Math.max(1, Math.min(5, rawWords)) : 1;\n    log(\"request:params\", { wordsPerSubtitle });\n\n    const session = await requireAuth();\n    log(\"auth:ok\", { userId: session.userId });\n\n    if (!session.userId) {\n      logErr(\"auth:missingUserId\");\n      return NextResponse.json({ error: \"Unauthorized\" }, { status: 401 });\n    }\n\n    log(\"db:clip:fetch\", { id: params.id });\n    const clip = await prisma.clip.findUnique({\n      where: { id: params.id },\n      include: {\n        Video: {\n          select: { userId: true, sourceUrl: true, id: true, transcript: true },\n        },\n      },\n    });\n\n    if (!clip) {\n      logErr(\"db:clip:notFound\", { id: params.id });\n      return NextResponse.json({ error: \"Clip not found\" }, { status: 404 });\n    }\n    if (clip.Video.userId !== session.userId) {\n      logErr(\"auth:forbidden\", {\n        owner: clip.Video.userId,\n        requester: session.userId,\n      });\n      return NextResponse.json({ error: \"Forbidden\" }, { status: 403 });\n    }\n    if (!clip.s3VideoKey || !clip.s3SrtKey) {\n      logErr(\"clip:missingS3Keys\", {\n        s3VideoKey: !!clip.s3VideoKey,\n        s3SrtKey: !!clip.s3SrtKey,\n      });\n      return NextResponse.json({ error: \"Missing S3 keys\" }, { status: 400 });\n    }\n    if (!clip.Video.sourceUrl) {\n      logErr(\"video:missingSourceUrl\");\n      return NextResponse.json({ error: \"Missing sourceUrl\" }, { status: 400 });\n    }\n    if (!clip.Video.transcript) {\n      logErr(\"video:missingTranscript\");\n      return NextResponse.json(\n        { error: \"Missing transcript\" },\n        { status: 400 },\n      );\n    }\n\n    const tmp = tmpdir();\n    const sourcePath = join(tmp, `${clip.Video.id}-${clip.id}-source.mp4`);\n    const assPath = join(tmp, `${clip.id}.ass`);\n    const outPath = join(tmp, `${clip.id}-rebuilt.mp4`);\n    log(\"paths\", { sourcePath, assPath, outPath });\n\n    const words = normalizeTranscript(clip.Video.transcript);\n    log(\"transcript:words:stats\", {\n      count: words.length,\n      first: words[0],\n      last: words[words.length - 1],\n    });\n\n    if (words.length === 0) {\n      logErr(\"transcript:words:empty\");\n      return NextResponse.json(\n        { error: \"Transcript has no word timings\" },\n        { status: 400 },\n      );\n    }\n\n    const clipWords = sliceToClip(\n      words,\n      clip.startSec,\n      clip.endSec,\n      clip.durationSec,\n    );\n    log(\"srt:words\", {\n      count: clipWords.length,\n      first: clipWords[0],\n      last: clipWords[clipWords.length - 1],\n    });\n\n    try {\n      log(\"ass:create:start\", { wordsPerSubtitle });\n      createAssWordByWordFile(clipWords, assPath, wordsPerSubtitle);\n      log(\"ass:create:done\");\n    } catch (e: any) {\n      logErr(\"ass:create:error\", e?.message || e);\n      throw new Error(\"Failed to create ASS\");\n    }\n\n    try {\n      log(\"downloadVideo:start\", { url: clip.Video.sourceUrl });\n      await downloadVideo(clip.Video.sourceUrl, sourcePath, session.userId);\n      log(\"downloadVideo:done\");\n    } catch (e: any) {\n      logErr(\"downloadVideo:error\", e?.message || e);\n      throw new Error(\"Failed to download source video\");\n    }\n\n    try {\n      log(\"render:start\", { start: clip.startSec, duration: clip.durationSec });\n      await renderVerticalClip({\n        inputPath: sourcePath,\n        outputPath: outPath,\n        startTime: clip.startSec,\n        duration: clip.durationSec,\n        srtPath: assPath,\n      });\n      log(\"render:done\");\n    } catch (e: any) {\n      logErr(\"render:error\", e?.message || e);\n      throw new Error(\"Failed to render vertical clip\");\n    }\n\n    try {\n      log(\"upload:video:start\", { key: clip.s3VideoKey });\n      await s3Upload(outPath, clip.s3VideoKey, \"video/mp4\");\n      log(\"upload:video:done\");\n    } catch (e: any) {\n      logErr(\"upload:video:error\", e?.message || e);\n      throw new Error(\"Failed to upload video\");\n    }\n\n    try {\n      log(\"upload:ass:start\", { key: clip.s3SrtKey });\n      await s3Upload(assPath, clip.s3SrtKey, \"text/plain\");\n      log(\"upload:ass:done\");\n    } catch (e: any) {\n      logErr(\"upload:ass:error\", e?.message || e);\n      throw new Error(\"Failed to upload ass\");\n    }\n\n    const url = publicUrlForKey(clip.s3VideoKey);\n    log(\"POST:success\", { reqId, url });\n    return NextResponse.json({ success: true, url });\n  } catch (err: any) {\n    logErr(\"POST:error\", err?.message || err);\n    return NextResponse.json(\n      { error: err?.message || \"Failed to update subtitles\" },\n      { status: 500 },\n    );\n  }\n}\n","size_bytes":9962},"app/providers.tsx":{"content":"\"use client\";\n\nimport { ThemeProvider, CssBaseline } from \"@mui/material\";\nimport { theme } from \"@/src/theme\";\n\nexport default function Providers({ children }: { children: React.ReactNode }) {\n  return (\n    <ThemeProvider theme={theme}>\n      <CssBaseline />\n      {children}\n    </ThemeProvider>\n  );\n}\n","size_bytes":306},"app/api/clips/[id]/center/route.ts":{"content":"import { NextResponse } from \"next/server\";\nimport { NextRequest } from \"next/server\";\nimport { prisma } from \"@/src/lib/prisma\";\nimport { computeCropMap, Constraints, CropKF } from \"@/src/services/framingService\";\nimport { renderSmartFramedClip, probeVideo } from \"@/src/services/ffmpeg\";\nimport { downloadVideo } from \"@/src/services/youtube\";\nimport { uploadFile } from \"@/src/services/s3\";\nimport { tmpdir } from \"os\";\nimport { join } from \"path\";\nimport { writeFile, unlink } from \"fs/promises\";\nimport { GetObjectCommand, S3Client } from \"@aws-sdk/client-s3\";\n\nexport const runtime = \"nodejs\";\n\nfunction buildPiecewiseExpr(kf: CropKF[], key: \"x\" | \"y\"): string {\n  if (kf.length === 0) {\n    return \"0\";\n  }\n\n  const parts: string[] = [];\n  const firstVal = key === \"x\" ? kf[0].x : kf[0].y;\n  parts.push(`lt(t,${kf[0].t.toFixed(3)})*${firstVal.toFixed(0)}`);\n\n  for (let i = 0; i < kf.length - 1; i++) {\n    const a = kf[i];\n    const b = kf[i + 1];\n    const ta = a.t;\n    const tb = b.t;\n    const va = key === \"x\" ? a.x : a.y;\n    const vb = key === \"x\" ? b.x : b.y;\n    const slope = (vb - va) / Math.max(0.001, tb - ta);\n    parts.push(\n      `between(t,${ta.toFixed(3)},${tb.toFixed(3)})*(${va.toFixed(0)}+(${slope.toFixed(6)})*(t-${ta.toFixed(3)}))`,\n    );\n  }\n\n  const lastVal = key === \"x\" ? kf[kf.length - 1].x : kf[kf.length - 1].y;\n  parts.push(`gte(t,${kf[kf.length - 1].t.toFixed(3)})*${lastVal.toFixed(0)}`);\n\n  return parts.join(\"+\");\n}\n\nexport async function POST(\n  req: NextRequest,\n  { params }: { params: { id: string } },\n) {\n  const tmp = tmpdir();\n  const videoPath = join(tmp, `center-${params.id}-source.mp4`);\n  const srtPath = join(tmp, `center-${params.id}.srt`);\n  const outputPath = join(tmp, `center-${params.id}-output.mp4`);\n\n  try {\n    const clip = await prisma.clip.findUnique({\n      where: { id: params.id },\n      include: {\n        Video: {\n          select: { userId: true, sourceUrl: true, id: true },\n        },\n      },\n    });\n\n    if (!clip) {\n      return NextResponse.json({ error: \"Clip not found\" }, { status: 404 });\n    }\n\n    if (!clip.Video.sourceUrl) {\n      return NextResponse.json(\n        { error: \"Source video URL not found\" },\n        { status: 400 },\n      );\n    }\n\n    console.log(`Centering clip ${clip.id}...`);\n\n    await downloadVideo(clip.Video.sourceUrl, videoPath, clip.Video.userId);\n    console.log(\"Source video downloaded\");\n\n    const client = new S3Client({\n      endpoint: process.env.S3_ENDPOINT,\n      region: process.env.S3_REGION || \"auto\",\n      credentials:\n        process.env.S3_ACCESS_KEY_ID && process.env.S3_SECRET_ACCESS_KEY\n          ? {\n              accessKeyId: process.env.S3_ACCESS_KEY_ID,\n              secretAccessKey: process.env.S3_SECRET_ACCESS_KEY,\n            }\n          : undefined,\n    });\n\n    const srtData = await client.send(\n      new GetObjectCommand({\n        Bucket: process.env.S3_BUCKET,\n        Key: clip.s3SrtKey,\n      }),\n    );\n\n    if (!srtData.Body) {\n      throw new Error(\"Failed to download SRT file\");\n    }\n\n    const chunks: Uint8Array[] = [];\n    for await (const chunk of srtData.Body as any) {\n      chunks.push(chunk);\n    }\n    await writeFile(srtPath, Buffer.concat(chunks));\n    console.log(\"SRT file downloaded\");\n\n    const probe = await probeVideo(videoPath);\n    console.log(`Source video: ${probe.width}x${probe.height}`);\n\n    const targetW = Math.floor((probe.height * 9) / 16);\n    const maxPanSpeed = Math.max(200, targetW * 0.4);\n\n    const constraints: Constraints = {\n      margin: 0.1,\n      maxPan: maxPanSpeed,\n      easeMs: 500,\n      centerBiasX: 0.3,\n      centerBiasY: 0.5,\n      safeTop: 0.1,\n      safeBottom: 0.1,\n    };\n\n    const input = {\n      videoPath,\n      baseW: probe.width,\n      baseH: probe.height,\n      segStart: clip.startSec,\n      segEnd: clip.endSec,\n      transcript: [],\n    };\n\n    console.log(\"Computing crop map...\");\n    const cropKeyframes: CropKF[] | null = await computeCropMap(\n      input,\n      constraints,\n    );\n\n    if (!cropKeyframes || cropKeyframes.length === 0) {\n      return NextResponse.json(\n        { error: \"Failed to compute crop keyframes - no faces detected\" },\n        { status: 500 },\n      );\n    }\n\n    console.log(`Generated ${cropKeyframes.length} crop keyframes`);\n\n    const exprX = buildPiecewiseExpr(cropKeyframes, \"x\");\n    const exprY = buildPiecewiseExpr(cropKeyframes, \"y\");\n\n    const cropW = cropKeyframes[0].w;\n    const cropH = cropKeyframes[0].h;\n\n    console.log(\"Rendering smart framed clip...\");\n    await renderSmartFramedClip({\n      inputPath: videoPath,\n      outputPath,\n      startTime: clip.startSec,\n      duration: clip.durationSec,\n      srtPath,\n      hookText: clip.hookText || \"\",\n      cropMapExprX: exprX,\n      cropMapExprY: exprY,\n      cropW,\n      cropH,\n    });\n\n    console.log(\"Uploading to S3...\");\n    await uploadFile(clip.s3VideoKey, outputPath, \"video/mp4\");\n\n    console.log(\"Updating database...\");\n    await prisma.clip.update({\n      where: { id: clip.id },\n      data: {\n        smartFramed: true,\n        cropMapJson: cropKeyframes as any,\n      },\n    });\n\n    console.log(`Clip ${clip.id} centered successfully`);\n\n    return NextResponse.json({\n      success: true,\n      message: \"Faces centered successfully\",\n    });\n  } catch (error: any) {\n    console.error(\"Error centering video:\", error);\n    return NextResponse.json(\n      { error: error.message || \"Internal server error\" },\n      { status: 500 },\n    );\n  } finally {\n    await Promise.all([\n      unlink(videoPath).catch(() => {}),\n      unlink(srtPath).catch(() => {}),\n      unlink(outputPath).catch(() => {}),\n    ]);\n  }\n}\n","size_bytes":5663},"src/services/scoring/clipScorer.ts":{"content":"import { Features } from './features'\n\nexport type Pillars = {\n  hook: number\n  watchability: number\n  visuals: number\n  safety: number\n  novelty: number\n  coherence: number\n  durationFit: number\n}\n\nexport type ScoreResult = {\n  score: number\n  pillars: Pillars\n}\n\nexport function scoreFromFeatures(f: Features, aiScore?: number): ScoreResult {\n  const hook = mix(f.hookness, safe(aiScore), 0.6)\n  const watchability = harmonic([f.speechContinuity, f.sentenceCompleteness, f.silenceRatio])\n  const visuals = harmonic([f.facePresence, f.motionScore, f.loudnessScore, 1 - f.cutDensity * 0.2])\n  const safety = f.safety\n  const novelty = f.novelty\n  const coherence = f.coherence\n  const durationFit = 1 - f.durationPenalty\n  const pillars: Pillars = { hook, watchability, visuals, safety, novelty, coherence, durationFit }\n  const base = geometric([hook, watchability, visuals, safety, novelty, coherence, durationFit])\n  const featureCount = 7\n  const difficulty = 0.85 + 0.1 * featureCount\n  const curved = curve(base, difficulty)\n  const score = Math.round(curved * 100)\n  return { score, pillars }\n}\n\nfunction mix(a: number, b: number | undefined, w: number): number {\n  if (b === undefined) { return a }\n  return clamp01(a * (1 - w) + b * w)\n}\n\nfunction safe(v?: number): number {\n  if (v === undefined) { return 0.5 }\n  return clamp01(v)\n}\n\nfunction harmonic(xs: number[]): number {\n  let sum = 0\n  let k = 0\n  for (const x of xs) {\n    if (x <= 0) { continue }\n    sum = sum + 1 / x\n    k = k + 1\n  }\n  if (k === 0) { return 0 }\n  return clamp01(k / sum)\n}\n\nfunction geometric(xs: number[]): number {\n  let p = 1\n  let k = 0\n  for (const x of xs) {\n    const y = clamp01(x)\n    p = p + 0 * y\n    p = p * y\n    k = k + 1\n  }\n  if (k === 0) { return 0 }\n  return Math.pow(p, 1 / k)\n}\n\nfunction curve(x: number, difficulty: number): number {\n  const d = clamp01(difficulty)\n  const a = 4 * d\n  const y = 1 / (1 + Math.exp(-a * (x - 0.5)))\n  return clamp01(y)\n}\n\nfunction clamp01(v: number): number {\n  if (v < 0) { return 0 }\n  if (v > 1) { return 1 }\n  return v\n}\n","size_bytes":2067},"src/services/selection/finalizeRanking.ts":{"content":"import { RankInput, RankOutput, rankClips } from '../scoring/clipRanker'\n\nexport function finalizeBestClips(inputs: RankInput[], maxClips: number = 5): RankOutput[] {\n  const ranked = rankClips(inputs)\n  const t60 = ranked.filter(r => r.durationChoice === 't60')\n  const t120 = ranked.filter(r => r.durationChoice === 't120')\n  const out: RankOutput[] = []\n  const q120 = Math.min(4, Math.ceil(maxClips * 0.33))\n  const q60 = maxClips - q120\n  for (let i = 0; i < t120.length && out.length < q120; i++) { out.push(t120[i]) }\n  for (let i = 0; i < t60.length && out.length < maxClips; i++) { out.push(t60[i]) }\n  if (out.length < maxClips) {\n    const rest = ranked.filter(r => !out.find(x => x.id === r.id))\n    for (let i = 0; i < rest.length && out.length < maxClips; i++) { out.push(rest[i]) }\n  }\n  return out.slice(0, maxClips)\n}\n","size_bytes":835},"src/services/scoring/taxonomy.ts":{"content":"export type Taxonomy = { category: string; hookType: 'question' | 'bold' | 'number' | 'contrast' | 'statement' | 'story'; tone: 'educational' | 'motivational' | 'humor' | 'commentary' | 'news' | 'tech' | 'finance' | 'health' | 'sports' | 'other' }\n\nexport function inferTaxonomy(text: string, hook: string, gptCategory?: string): Taxonomy {\n  const h = hook.toLowerCase()\n  let hookType: Taxonomy['hookType'] = 'statement'\n  if (/\\?/.test(h) || /^(how|why|what|when|where|who|can|should|would)\\b/i.test(h)) { hookType = 'question' }\n  else if (/\\b\\d+\\b/.test(h)) { hookType = 'number' }\n  else if (/(vs\\.|versus|compared to)/i.test(h)) { hookType = 'contrast' }\n  else if (/(secret|truth|never|always|stop|must|best|worst)/i.test(h)) { hookType = 'bold' }\n  let tone: Taxonomy['tone'] = 'other'\n  const t = (gptCategory || '').toLowerCase()\n  if (/(education|tech|tutorial)/.test(t)) { tone = 'educational' }\n  else if (/motivation/.test(t)) { tone = 'motivational' }\n  else if (/humor|comedy/.test(t)) { tone = 'humor' }\n  else if (/commentary|opinion/.test(t)) { tone = 'commentary' }\n  else if (/news/.test(t)) { tone = 'news' }\n  else if (/tech/.test(t)) { tone = 'tech' }\n  else if (/finance/.test(t)) { tone = 'finance' }\n  else if (/health/.test(t)) { tone = 'health' }\n  else if (/sport/.test(t)) { tone = 'sports' }\n  const category = gptCategory || 'Other'\n  return { category, hookType, tone }\n}\n","size_bytes":1407},"src/services/selection/clipSelector.ts":{"content":"import { Segment, computeFeatures } from '../scoring/features'\nimport { scoreFromFeatures } from '../scoring/clipScorer'\n\nexport type SelectOptions = {\n  targetDurations?: number[]\n  maxClips?: number\n  minSimilarityGap?: number\n  isCancelled?: () => boolean\n  videoEmbedding?: number[]\n}\n\nexport type SelectedClip = {\n  id: string\n  videoId: string\n  start: number\n  end: number\n  score: number\n  pillars: {\n    hook: number\n    watchability: number\n    visuals: number\n    safety: number\n    novelty: number\n    coherence: number\n    durationFit: number\n  }\n}\n\nexport function selectClips(candidates: Segment[], options: SelectOptions): SelectedClip[] {\n  const targets = options.targetDurations ?? [60, 120]\n  const maxClips = Math.min(options.maxClips ?? 12, 12)\n  const minGap = options.minSimilarityGap ?? 0.08\n  const scored: SelectedClip[] = []\n  for (const seg of candidates) {\n    if (options.isCancelled && options.isCancelled()) { break }\n    const variants = targets.map(t => evaluateForTarget(seg, t, options.videoEmbedding))\n    const best = variants.sort((a, b) => b.score - a.score)[0]\n    if (!best) { continue }\n    scored.push(best)\n  }\n  const unique = mmr(scored, maxClips, minGap)\n  return unique.slice(0, maxClips)\n}\n\nfunction evaluateForTarget(seg: Segment, target: number, videoEmbedding?: number[]): SelectedClip {\n  const snapped = snapToSentence(seg, target)\n  const feat = computeFeatures(snapped, target, videoEmbedding)\n  const s = scoreFromFeatures(feat, seg.aiScore)\n  return { id: seg.id, videoId: seg.videoId, start: snapped.start, end: snapped.end, score: s.score, pillars: s.pillars }\n}\n\nfunction snapToSentence(seg: Segment, target: number): Segment {\n  const toleranceLow = 10\n  const toleranceHigh = 15\n  const minAccept = Math.max(0, target - toleranceLow)\n  const maxAccept = target + toleranceHigh\n  const words = seg.words.slice().sort((a, b) => a.start - b.start)\n  let start = seg.start\n  let end = seg.end\n  if (words.length > 0) { start = words[0].start; end = words[words.length - 1].end }\n  const desiredEnd = start + target\n  let bestEnd = end\n  let bestDelta = Infinity\n  for (const w of words) {\n    const text = w.text.trim()\n    const isBoundary = /[.!?…]$/.test(text)\n    if (isBoundary) {\n      const t = w.end\n      const within = t >= start + minAccept && t <= start + maxAccept\n      if (within) {\n        const d = Math.abs(t - desiredEnd)\n        if (d < bestDelta) { bestDelta = d; bestEnd = t }\n      }\n    }\n  }\n  if (!isFinite(bestDelta)) {\n    const last = words[words.length - 1]\n    if (last) { bestEnd = Math.min(last.end + 3, start + maxAccept) }\n  }\n  if (bestEnd <= start + minAccept) { bestEnd = Math.min(start + minAccept + 2, start + maxAccept) }\n  return { ...seg, start, end: bestEnd }\n}\n\nfunction mmr(items: SelectedClip[], k: number, minGap: number): SelectedClip[] {\n  const result: SelectedClip[] = []\n  const used: SelectedClip[] = []\n  const pool = items.slice().sort((a, b) => b.score - a.score)\n  while (result.length < k && pool.length > 0) {\n    const x = pool.shift() as SelectedClip\n    if (!x) { break }\n    let ok = true\n    for (const y of used) {\n      const sim = pillarSim(x, y)\n      if (sim > 1 - minGap) { ok = false; break }\n    }\n    if (ok) { result.push(x); used.push(x) }\n  }\n  return result\n}\n\nfunction pillarSim(a: SelectedClip, b: SelectedClip): number {\n  const va = [a.pillars.hook, a.pillars.watchability, a.pillars.visuals, a.pillars.safety, a.pillars.novelty, a.pillars.coherence, a.pillars.durationFit]\n  const vb = [b.pillars.hook, b.pillars.watchability, b.pillars.visuals, b.pillars.safety, b.pillars.novelty, b.pillars.coherence, b.pillars.durationFit]\n  return cosine(va, vb)\n}\n\nfunction cosine(a: number[], b: number[]): number {\n  const len = Math.min(a.length, b.length)\n  if (len === 0) { return 0 }\n  let dot = 0\n  let na = 0\n  let nb = 0\n  for (let i = 0; i < len; i++) {\n    dot = dot + a[i] * b[i]\n    na = na + a[i] * a[i]\n    nb = nb + b[i] * b[i]\n  }\n  const denom = Math.sqrt(na) * Math.sqrt(nb)\n  if (denom === 0) { return 0 }\n  return (dot / denom + 1) / 2\n}\n","size_bytes":4094},"src/services/scoring/features.ts":{"content":"export type Word = {\n  start: number\n  end: number\n  text: string\n  confidence?: number\n}\n\nexport type Segment = {\n  id: string\n  videoId: string\n  start: number\n  end: number\n  transcript: string\n  words: Word[]\n  sceneCuts?: number[]\n  faces?: number\n  motion?: number\n  loudness?: number\n  laughs?: number\n  questions?: number\n  aiScore?: number\n  embedding?: number[]\n  safety?: number\n}\n\nexport type Features = {\n  duration: number\n  durationTarget: number\n  durationPenalty: number\n  speechContinuity: number\n  sentenceCompleteness: number\n  silenceRatio: number\n  cutDensity: number\n  facePresence: number\n  motionScore: number\n  loudnessScore: number\n  hookness: number\n  questionLead: number\n  laughMoments: number\n  novelty: number\n  coherence: number\n  safety: number\n}\n\nexport function computeFeatures(segment: Segment, targetDuration: number, videoEmbedding?: number[]): Features {\n  const duration = Math.max(0, segment.end - segment.start)\n  const toleranceLow = 10\n  const toleranceHigh = 15\n  const minAccept = Math.max(0, targetDuration - toleranceLow)\n  const maxAccept = targetDuration + toleranceHigh\n  const durationPenalty = durationFitPenalty(duration, targetDuration, minAccept, maxAccept)\n  const speechContinuity = continuityScore(segment.words)\n  const sentenceCompleteness = completenessScore(segment.transcript, segment.words)\n  const silenceRatio = silenceScore(segment.words, duration)\n  const cutDensity = cutScore(segment.sceneCuts, segment.start, segment.end)\n  const facePresence = normalize01(segment.faces ?? 0, 0, 1)\n  const motionScore = normalize01(segment.motion ?? 0, 0, 1)\n  const loudnessScore = normalize01(segment.loudness ?? 0, 0, 1)\n  const hookness = hookScore(segment.words, 5)\n  const questionLead = normalize01(segment.questions ?? 0, 0, 3)\n  const laughMoments = normalize01(segment.laughs ?? 0, 0, 3)\n  const novelty = noveltyScore(segment.embedding, videoEmbedding)\n  const coherence = coherenceScore(segment.words)\n  const safety = normalize01(segment.safety ?? 1, 0, 1)\n  return {\n    duration,\n    durationTarget: targetDuration,\n    durationPenalty,\n    speechContinuity,\n    sentenceCompleteness,\n    silenceRatio,\n    cutDensity,\n    facePresence,\n    motionScore,\n    loudnessScore,\n    hookness,\n    questionLead,\n    laughMoments,\n    novelty,\n    coherence,\n    safety\n  }\n}\n\nfunction durationFitPenalty(actual: number, target: number, minAccept: number, maxAccept: number): number {\n  if (actual < minAccept) { return 1 }\n  if (actual > maxAccept) { return 1 }\n  const mid = target\n  const d = Math.abs(actual - mid)\n  const span = Math.max(mid - minAccept, maxAccept - mid)\n  const p = d / span\n  return clamp01(p)\n}\n\nfunction continuityScore(words: Word[]): number {\n  if (words.length < 2) { return 0 }\n  let longPauses = 0\n  for (let i = 1; i < words.length; i++) {\n    const gap = words[i].start - words[i - 1].end\n    if (gap > 0.8) { longPauses = longPauses + 1 }\n  }\n  const rate = longPauses / Math.max(1, words.length - 1)\n  return 1 - clamp01(rate)\n}\n\nfunction completenessScore(transcript: string, words: Word[]): number {\n  const trimmed = transcript.trim()\n  const punct = /[.!?…]$/\n  if (punct.test(trimmed)) { return 1 }\n  const last = words[words.length - 1]\n  if (!last) { return 0 }\n  const trailing = last.end - last.start\n  if (trailing > 0.6) { return 0.7 }\n  return 0.4\n}\n\nfunction silenceScore(words: Word[], duration: number): number {\n  if (duration <= 0) { return 1 }\n  if (words.length === 0) { return 1 }\n  let totalSilence = 0\n  let lastEnd = words[0].end\n  for (let i = 1; i < words.length; i++) {\n    const gap = words[i].start - lastEnd\n    if (gap > 0) { totalSilence = totalSilence + gap }\n    lastEnd = words[i].end\n  }\n  const ratio = clamp01(totalSilence / duration)\n  return 1 - ratio\n}\n\nfunction cutScore(cuts: number[] | undefined, start: number, end: number): number {\n  if (!cuts || cuts.length === 0) { return 0.5 }\n  const within = cuts.filter(c => c >= start && c <= end)\n  const density = within.length / Math.max(1, end - start)\n  const targetDensity = 0.015\n  const diff = Math.abs(density - targetDensity)\n  const span = targetDensity\n  return 1 - clamp01(diff / span)\n}\n\nfunction hookScore(words: Word[], seconds: number): number {\n  if (words.length === 0) { return 0 }\n  const startAt = words[0].start\n  const windowEnd = startAt + seconds\n  const firstWords = words.filter(w => w.start <= windowEnd).map(w => w.text.toLowerCase())\n  const phrases = [\"here's why\",\"the truth is\",\"no one tells you\",\"what happened was\",\"the secret\",\"mistake\",\"hack\",\"watch this\",\"did you know\",\"let me tell you\",\"biggest\"]\n  let score = 0\n  for (const p of phrases) { if (firstWords.join(' ').includes(p)) { score = score + 1 } }\n  const q = firstWords.includes('why') || firstWords.includes('how') || firstWords.includes('what') ? 0.3 : 0\n  return clamp01(score * 0.3 + q)\n}\n\nfunction noveltyScore(emb?: number[], videoEmb?: number[]): number {\n  if (!emb || !videoEmb) { return 0.5 }\n  const sim = cosine(emb, videoEmb)\n  return clamp01(1 - sim)\n}\n\nfunction coherenceScore(words: Word[]): number {\n  if (words.length < 2) { return 0 }\n  let badBreaks = 0\n  for (let i = 1; i < words.length; i++) {\n    const gap = words[i].start - words[i - 1].end\n    if (gap > 1.2) { badBreaks = badBreaks + 1 }\n  }\n  const r = badBreaks / Math.max(1, words.length - 1)\n  return 1 - clamp01(r)\n}\n\nfunction cosine(a: number[], b: number[]): number {\n  const len = Math.min(a.length, b.length)\n  if (len === 0) { return 0 }\n  let dot = 0\n  let na = 0\n  let nb = 0\n  for (let i = 0; i < len; i++) {\n    dot = dot + a[i] * b[i]\n    na = na + a[i] * a[i]\n    nb = nb + b[i] * b[i]\n  }\n  const denom = Math.sqrt(na) * Math.sqrt(nb)\n  if (denom === 0) { return 0 }\n  return clamp01((dot / denom + 1) / 2)\n}\n\nfunction normalize01(v: number, min: number, max: number): number {\n  if (max === min) { return 0 }\n  const n = (v - min) / (max - min)\n  return clamp01(n)\n}\n\nfunction clamp01(v: number): number {\n  if (v < 0) { return 0 }\n  if (v > 1) { return 1 }\n  return v\n}\n","size_bytes":6054},"src/services/scoring/clipRanker.ts":{"content":"export type Pillars = { hook: number; watchability: number; visuals: number; safety: number; novelty: number; coherence: number; durationFit: number }\n\nexport type RankInput = { id: string; videoId: string; start: number; end: number; score: number; pillars: Pillars; aiOverall?: number; durationChoice?: 't60' | 't120'; nearHotspot?: boolean }\n\nexport type RankOutput = RankInput & { rankScore: number; tier: 'S' | 'A' | 'B'; reasons: string[] }\n\nfunction clamp01(v: number): number { if (v < 0) { return 0 } if (v > 1) { return 1 } return v }\n\nfunction gmean(xs: number[]): number { let p = 1; let k = 0; for (const x of xs) { p = p * clamp01(x); k = k + 1 } if (k === 0) { return 0 } return Math.pow(p, 1 / k) }\n\nfunction curve(x: number, difficulty: number): number { const d = clamp01(difficulty); const a = 4 * d; const y = 1 / (1 + Math.exp(-a * (x - 0.5))); return clamp01(y) }\n\nfunction normalize(xs: number[]): number[] { if (xs.length === 0) { return [] } const min = Math.min(...xs); const max = Math.max(...xs); if (max === min) { return xs.map(() => 0.5) } return xs.map(v => (v - min) / (max - min)) }\n\nexport function rankClips(inputs: RankInput[]): RankOutput[] {\n  const baseScores = inputs.map(x => x.score)\n  const baseN = normalize(baseScores)\n  const out: RankOutput[] = []\n  for (let i = 0; i < inputs.length; i++) {\n    const it = inputs[i]\n    const p = it.pillars\n    const base = baseN[i]\n    const hook = p.hook\n    const watch = p.watchability\n    const viz = p.visuals\n    const coh = p.coherence\n    const dur = p.durationFit\n    const safe = p.safety\n    const nov = p.novelty\n    const ai = it.aiOverall === undefined ? 0.5 : clamp01(it.aiOverall / 100)\n    const blend = gmean([base, hook, watch, viz, coh, dur, safe, nov])\n    const withAi = clamp01(0.7 * blend + 0.3 * ai)\n    const hotspotBoost = it.nearHotspot ? 0.04 : 0\n    const durBias = it.durationChoice === 't120' ? 0.02 : 0\n    const raw = clamp01(withAi + hotspotBoost + durBias)\n    const curved = curve(raw, 0.92)\n    const rankScore = curved\n    let tier: 'S' | 'A' | 'B' = 'B'\n    if (rankScore >= 0.78) { tier = 'S' } else if (rankScore >= 0.62) { tier = 'A' }\n    const reasons: string[] = []\n    if (hook > 0.72) { reasons.push('hook') }\n    if (watch > 0.68) { reasons.push('retention') }\n    if (coh > 0.7) { reasons.push('coherence') }\n    if (dur > 0.85) { reasons.push('duration-fit') }\n    if (nov > 0.6) { reasons.push('novelty') }\n    if (it.nearHotspot) { reasons.push('audience-hotspot') }\n    out.push({ ...it, rankScore, tier, reasons })\n  }\n  out.sort((a, b) => b.rankScore - a.rankScore || (a.durationChoice === 't120' ? -1 : 1))\n  return out\n}\n","size_bytes":2663},"src/lib/youtube-oauth.ts":{"content":"import { google } from 'googleapis'\n\nlet connectionSettings: any\n\nasync function getAccessToken() {\n  if (connectionSettings && connectionSettings.settings.expires_at && new Date(connectionSettings.settings.expires_at).getTime() > Date.now()) {\n    return connectionSettings.settings.access_token\n  }\n  \n  const hostname = process.env.REPLIT_CONNECTORS_HOSTNAME\n  const xReplitToken = process.env.REPL_IDENTITY \n    ? 'repl ' + process.env.REPL_IDENTITY \n    : process.env.WEB_REPL_RENEWAL \n    ? 'depl ' + process.env.WEB_REPL_RENEWAL \n    : null\n\n  if (!xReplitToken) {\n    throw new Error('X_REPLIT_TOKEN not found for repl/depl')\n  }\n\n  connectionSettings = await fetch(\n    'https://' + hostname + '/api/v2/connection?include_secrets=true&connector_names=youtube',\n    {\n      headers: {\n        'Accept': 'application/json',\n        'X_REPLIT_TOKEN': xReplitToken\n      }\n    }\n  ).then(res => res.json()).then(data => data.items?.[0])\n\n  if (!connectionSettings) {\n    throw new Error('YouTube not connected')\n  }\n\n  const accessToken = connectionSettings?.settings?.access_token || connectionSettings?.settings?.oauth?.credentials?.access_token\n\n  if (!accessToken) {\n    throw new Error('YouTube not connected')\n  }\n  return accessToken\n}\n\nexport async function getUncachableYouTubeClient() {\n  const accessToken = await getAccessToken()\n  return google.youtube({ version: 'v3', auth: accessToken })\n}\n\nexport async function getYouTubeOAuthToken(): Promise<string> {\n  return await getAccessToken()\n}\n\nexport async function isYouTubeConnected(): Promise<boolean> {\n  try {\n    await getAccessToken()\n    return true\n  }\n  catch {\n    return false\n  }\n}\n","size_bytes":1662},"app/api/youtube/upload-cookies/route.ts":{"content":"import { NextResponse } from 'next/server'\nimport { getSession } from '@/src/lib/session'\nimport { saveCookies } from '@/src/services/cookieGenerator'\n\nfunction validateNetscapeCookieFormat(content: string): { valid: boolean; error?: string } {\n  const lines = content.split('\\n').map(l => l.trim()).filter(l => l.length > 0)\n  \n  if (lines.length === 0) {\n    return { valid: false, error: 'Cookie file is empty' }\n  }\n\n  const hasNetscapeHeader = lines.some(line => \n    line.toLowerCase().includes('netscape') && line.toLowerCase().includes('cookie')\n  )\n  \n  const cookieLines = lines.filter(line => {\n    if (line.startsWith('#HttpOnly_')) {\n      return true\n    }\n    if (line.startsWith('#')) {\n      return false\n    }\n    return true\n  })\n  \n  if (cookieLines.length === 0) {\n    return { valid: false, error: 'No cookie entries found. Make sure to paste the raw Netscape format cookies, not encrypted or base64 encoded data.' }\n  }\n\n  let validCookieCount = 0\n  for (const line of cookieLines) {\n    const cookieLine = line.startsWith('#HttpOnly_') ? line.substring('#HttpOnly_'.length) : line\n    const parts = cookieLine.split('\\t')\n    \n    if (parts.length >= 6) {\n      validCookieCount++\n    }\n  }\n\n  if (validCookieCount === 0) {\n    return { \n      valid: false, \n      error: 'Invalid cookie format. Cookies must be in Netscape format with tab-separated values. Make sure you exported cookies as plain text, not encrypted data.' \n    }\n  }\n\n  const hasYouTubeCookie = lines.some(line => line.includes('youtube.com'))\n  if (!hasYouTubeCookie) {\n    return { \n      valid: false, \n      error: 'No YouTube cookies found. Make sure you export cookies from youtube.com while logged in.' \n    }\n  }\n\n  if (!hasNetscapeHeader) {\n    console.warn('Cookie file missing Netscape header comment, but cookies appear valid')\n  }\n\n  return { valid: true }\n}\n\nexport async function POST(request: Request) {\n  const session = await getSession()\n  \n  if (!session.isAuthenticated || !session.userId) {\n    return NextResponse.json({ error: 'Unauthorized' }, { status: 401 })\n  }\n\n  try {\n    const body = await request.json()\n    const { cookies: cookieContent } = body\n\n    if (!cookieContent || typeof cookieContent !== 'string') {\n      return NextResponse.json(\n        { error: 'Cookie content is required' },\n        { status: 400 }\n      )\n    }\n\n    if (cookieContent.trim().length === 0) {\n      return NextResponse.json(\n        { error: 'Cookie content cannot be empty' },\n        { status: 400 }\n      )\n    }\n\n    const validation = validateNetscapeCookieFormat(cookieContent)\n    if (!validation.valid) {\n      return NextResponse.json(\n        { error: validation.error },\n        { status: 400 }\n      )\n    }\n\n    await saveCookies(session.userId, cookieContent)\n\n    return NextResponse.json({ success: true })\n  }\n  catch (error: any) {\n    console.error('Error uploading YouTube cookies:', error)\n    return NextResponse.json(\n      { error: error.message || 'Failed to upload cookies' },\n      { status: 500 }\n    )\n  }\n}\n","size_bytes":3047},"app/upload-cookies/page.tsx":{"content":"\"use client\"\n\nimport { useState, useEffect } from \"react\"\nimport { useRouter } from \"next/navigation\"\nimport {\n  Container,\n  Paper,\n  Typography,\n  TextField,\n  Button,\n  Alert,\n  Box,\n  Link as MuiLink,\n  Stack,\n} from \"@mui/material\"\n\nexport default function UploadCookiesPage() {\n  const router = useRouter()\n  const [cookies, setCookies] = useState(\"\")\n  const [uploading, setUploading] = useState(false)\n  const [error, setError] = useState(\"\")\n  const [success, setSuccess] = useState(false)\n  const [authChecked, setAuthChecked] = useState(false)\n\n  useEffect(() => {\n    checkAuth()\n  }, [])\n\n  async function checkAuth() {\n    try {\n      const res = await fetch(\"/api/auth/status\")\n      const data = await res.json()\n      if (!data.isAuthenticated) {\n        router.push(\"/login\")\n        return\n      }\n      setAuthChecked(true)\n    }\n    catch {\n      router.push(\"/login\")\n    }\n  }\n\n  async function handleUpload(e: React.FormEvent) {\n    e.preventDefault()\n    setUploading(true)\n    setError(\"\")\n    setSuccess(false)\n\n    try {\n      const res = await fetch(\"/api/youtube/upload-cookies\", {\n        method: \"POST\",\n        headers: { \"Content-Type\": \"application/json\" },\n        body: JSON.stringify({ cookies }),\n      })\n\n      if (res.ok) {\n        setSuccess(true)\n        setCookies(\"\")\n        setTimeout(() => {\n          router.push(\"/\")\n        }, 2000)\n      }\n      else {\n        const data = await res.json()\n        setError(data.error || \"Failed to upload cookies\")\n      }\n    }\n    catch (err) {\n      setError(\"Failed to upload cookies\")\n    }\n    finally {\n      setUploading(false)\n    }\n  }\n\n  if (!authChecked) {\n    return null\n  }\n\n  return (\n    <Container maxWidth=\"md\" sx={{ py: 8 }}>\n      <Paper sx={{ p: 4 }}>\n        <Typography variant=\"h4\" gutterBottom>\n          Upload YouTube Cookies\n        </Typography>\n        \n        <Alert severity=\"info\" sx={{ mb: 2 }}>\n          YouTube requires cookies for authentication. Cookies typically last 2-3 weeks before needing to be refreshed.\n        </Alert>\n\n        <Alert severity=\"warning\" sx={{ mb: 3 }}>\n          <strong>Important:</strong> Cookies must be in plain text Netscape format (tab-separated values), not encrypted or base64 encoded.\n        </Alert>\n\n        <Box sx={{ mb: 3 }}>\n          <Typography variant=\"h6\" gutterBottom>\n            How to export YouTube cookies:\n          </Typography>\n          <Stack spacing={1} sx={{ ml: 2 }}>\n            <Typography variant=\"body2\">\n              1. Install a cookie export extension for your browser:\n            </Typography>\n            <Typography variant=\"body2\" sx={{ ml: 2 }}>\n              • Chrome/Edge: <MuiLink href=\"https://chrome.google.com/webstore/detail/get-cookiestxt-locally/cclelndahbckbenkjhflpdbgdldlbecc\" target=\"_blank\">Get cookies.txt LOCALLY</MuiLink>\n            </Typography>\n            <Typography variant=\"body2\" sx={{ ml: 2 }}>\n              • Firefox: <MuiLink href=\"https://addons.mozilla.org/en-US/firefox/addon/cookies-txt/\" target=\"_blank\">cookies.txt</MuiLink>\n            </Typography>\n            <Typography variant=\"body2\">\n              2. Log into YouTube in your browser\n            </Typography>\n            <Typography variant=\"body2\">\n              3. Go to youtube.com, click the extension icon, and export cookies\n            </Typography>\n            <Typography variant=\"body2\">\n              4. Open the downloaded .txt file and copy ALL contents\n            </Typography>\n            <Typography variant=\"body2\">\n              5. Paste the entire content into the box below\n            </Typography>\n          </Stack>\n        </Box>\n\n        <Box sx={{ mb: 3, p: 2, bgcolor: 'grey.900', borderRadius: 1 }}>\n          <Typography variant=\"subtitle2\" gutterBottom color=\"warning.main\">\n            Expected Format Example:\n          </Typography>\n          <Typography \n            variant=\"body2\" \n            sx={{ \n              fontFamily: 'monospace', \n              fontSize: '0.75rem',\n              color: 'grey.400',\n              whiteSpace: 'pre'\n            }}\n          >\n{`# Netscape HTTP Cookie File\n.youtube.com    TRUE    /       TRUE    1234567890      VISITOR_INFO1_LIVE      xxx\n#HttpOnly_.youtube.com  TRUE    /       TRUE    1234567890      LOGIN_INFO      yyy`}\n          </Typography>\n          <Typography variant=\"caption\" color=\"grey.500\" sx={{ mt: 1, display: 'block' }}>\n            Note: Cookie values are separated by TAB characters, not spaces.\n          </Typography>\n        </Box>\n\n        {error && (\n          <Alert severity=\"error\" sx={{ mb: 2 }}>\n            {error}\n          </Alert>\n        )}\n\n        {success && (\n          <Alert severity=\"success\" sx={{ mb: 2 }}>\n            Cookies uploaded successfully! Redirecting to dashboard...\n          </Alert>\n        )}\n\n        <form onSubmit={handleUpload}>\n          <TextField\n            fullWidth\n            multiline\n            rows={12}\n            variant=\"outlined\"\n            label=\"Paste YouTube cookies here (Netscape format)\"\n            value={cookies}\n            onChange={(e) => setCookies(e.target.value)}\n            placeholder=\"# Netscape HTTP Cookie File&#10;.youtube.com   TRUE    /       TRUE    0       VISITOR_INFO1_LIVE      ...&#10;.youtube.com    TRUE    /       TRUE    0       LOGIN_INFO      ...\"\n            disabled={uploading}\n            sx={{ mb: 2, fontFamily: \"monospace\" }}\n          />\n          \n          <Stack direction=\"row\" spacing={2}>\n            <Button\n              type=\"submit\"\n              variant=\"contained\"\n              disabled={!cookies.trim() || uploading}\n            >\n              {uploading ? \"Uploading...\" : \"Upload Cookies\"}\n            </Button>\n            <Button\n              variant=\"outlined\"\n              onClick={() => router.push(\"/\")}\n              disabled={uploading}\n            >\n              Cancel\n            </Button>\n          </Stack>\n        </form>\n      </Paper>\n    </Container>\n  )\n}\n","size_bytes":6001},"src/services/cookieGenerator.ts":{"content":"import { promises as fs } from 'fs'\nimport { join } from 'path'\nimport { tmpdir } from 'os'\nimport { prisma } from '@/src/lib/prisma'\n\nconst COOKIE_MAX_AGE_DAYS = 21\n\nexport async function saveCookies(userId: string, cookieContent: string): Promise<void> {\n  await prisma.user.update({\n    where: { id: userId },\n    data: {\n      youtubeCookies: cookieContent,\n      youtubeCookiesCreatedAt: new Date(),\n      youtubeCookiesLastUsedAt: new Date(),\n    },\n  })\n  console.log(`Cookies saved for user ${userId}`)\n}\n\nexport async function getCookieFilePath(userId: string): Promise<string | null> {\n  const user = await prisma.user.findUnique({\n    where: { id: userId },\n    select: {\n      youtubeCookies: true,\n      youtubeCookiesCreatedAt: true,\n    },\n  })\n\n  if (!user || !user.youtubeCookies) {\n    console.warn(`No YouTube cookies found for user ${userId}`)\n    return null\n  }\n\n  const tempFile = join(tmpdir(), `yt_cookies_${userId}_${Date.now()}.txt`)\n  await fs.writeFile(tempFile, user.youtubeCookies, { encoding: 'utf-8', mode: 0o600 })\n\n  if (user.youtubeCookiesCreatedAt) {\n    const ageInDays = (Date.now() - user.youtubeCookiesCreatedAt.getTime()) / (1000 * 60 * 60 * 24)\n    \n    if (ageInDays > COOKIE_MAX_AGE_DAYS) {\n      console.warn(`YouTube cookies for user ${userId} are ${Math.floor(ageInDays)} days old and may be expired`)\n    }\n    else {\n      console.log(`Using YouTube cookies for user ${userId} (${Math.floor(ageInDays)} days old)`)\n    }\n  }\n\n  await prisma.user.update({\n    where: { id: userId },\n    data: { youtubeCookiesLastUsedAt: new Date() },\n  })\n\n  return tempFile\n}\n\nexport async function cleanupCookieFile(filePath: string): Promise<void> {\n  try {\n    await fs.unlink(filePath)\n  }\n  catch {\n  }\n}\n\nexport async function getCookieAge(userId: string): Promise<number | null> {\n  const user = await prisma.user.findUnique({\n    where: { id: userId },\n    select: { youtubeCookiesCreatedAt: true },\n  })\n\n  if (!user || !user.youtubeCookiesCreatedAt) {\n    return null\n  }\n\n  return (Date.now() - user.youtubeCookiesCreatedAt.getTime()) / (1000 * 60 * 60 * 24)\n}\n\nexport async function clearCookies(userId: string): Promise<void> {\n  await prisma.user.update({\n    where: { id: userId },\n    data: {\n      youtubeCookies: null,\n      youtubeCookiesCreatedAt: null,\n      youtubeCookiesLastUsedAt: null,\n    },\n  })\n  console.log(`YouTube cookies cleared for user ${userId}`)\n\n  const pattern = `yt_cookies_${userId}_`\n  try {\n    const files = await fs.readdir(tmpdir())\n    for (const file of files)\n    {\n      if (file.startsWith(pattern))\n      {\n        try {\n          await fs.unlink(join(tmpdir(), file))\n        }\n        catch {\n        }\n      }\n    }\n  }\n  catch {\n  }\n}\n","size_bytes":2726},"app/api/test/framing/route.ts":{"content":"import { NextResponse } from \"next/server\"\nimport { spawn } from \"child_process\"\nimport * as fs from \"fs\"\nimport * as path from \"path\"\nimport { promisify } from \"util\"\nimport { computeCropMapPerson, computeCropMapPersonStatic, computeGlobalStaticCrop, CropKF, buildFFmpegFilter } from \"@/src/services/framingService\"\nimport { renderSmartFramedClip } from \"@/src/services/ffmpeg\"\nimport { getIntroEndFromChapters } from \"@/src/services/youtube\"\nimport { getCookieFilePath, cleanupCookieFile } from \"@/src/services/cookieGenerator\"\nimport { getCurrentUserId } from \"@/src/lib/session\"\n\nconst execFile = promisify(require(\"child_process\").execFile)\n\ntype ValidationResult = {\n  passed: boolean\n  message: string\n  details?: any\n}\n\nfunction parseTimeString(timeStr: string): number {\n  if (!timeStr) {\n    return 0\n  }\n  const parts = timeStr.split(':').map(Number)\n  if (parts.length === 1) {\n    return parts[0]\n  }\n  if (parts.length === 2) {\n    return parts[0] * 60 + parts[1]\n  }\n  if (parts.length === 3) {\n    return parts[0] * 3600 + parts[1] * 60 + parts[2]\n  }\n  return 0\n}\n\nasync function downloadTestVideo(url: string, startTime: number, endTime: number, tempDir: string, userId: string): Promise<{ path: string; width: number; height: number; duration: number }> {\n  const outputPath = path.join(tempDir, \"test.mp4\")\n  const duration = endTime - startTime\n\n  console.log(`[Framing Test] Downloading ${duration}s from ${url} (${startTime}s to ${endTime}s)`)\n\n  const cookieFile = await getCookieFilePath(userId)\n  \n  const args = [\n    \"-f\", \"best[height<=1080][ext=mp4]\",\n    \"--download-sections\", `*${startTime}-${endTime}`,\n    \"-o\", outputPath,\n    url\n  ]\n\n  if (cookieFile && fs.existsSync(cookieFile))\n  {\n    args.splice(0, 0, \"--cookies\", cookieFile)\n    console.log(`[Framing Test] Using cookie file for authentication`)\n  }\n  else\n  {\n    console.warn(`[Framing Test] No cookie file available - download may fail for some videos`)\n  }\n\n  try {\n    await execFile(\"yt-dlp\", args, { timeout: 120000, maxBuffer: 50 * 1024 * 1024 })\n  }\n  catch (error: any) {\n    throw new Error(`Video download failed: ${error.message}`)\n  }\n  finally {\n    if (cookieFile)\n    {\n      await cleanupCookieFile(cookieFile)\n    }\n  }\n\n  if (!fs.existsSync(outputPath)) {\n    throw new Error(\"Download failed: output file not found\")\n  }\n\n  const ffprobePath = require(\"ffprobe-static\").path\n  let probe: any\n  \n  try {\n    const { stdout } = await execFile(ffprobePath, [\n      \"-v\", \"error\",\n      \"-select_streams\", \"v:0\",\n      \"-show_entries\", \"stream=width,height,duration\",\n      \"-of\", \"json\",\n      outputPath\n    ])\n    probe = JSON.parse(stdout)\n  }\n  catch (error: any) {\n    throw new Error(`Failed to probe video: ${error.message}`)\n  }\n\n  if (!probe.streams || probe.streams.length === 0) {\n    throw new Error(\"No video stream found in downloaded file\")\n  }\n\n  const width = probe.streams[0].width\n  const height = probe.streams[0].height\n  const videoDuration = parseFloat(probe.streams[0].duration || \"0\")\n\n  if (!width || !height) {\n    throw new Error(`Invalid video dimensions: ${width}x${height}`)\n  }\n\n  if (!videoDuration || videoDuration <= 0) {\n    throw new Error(`Invalid video duration: ${videoDuration}`)\n  }\n\n  console.log(`[Framing Test] Downloaded ${width}x${height} video (${videoDuration.toFixed(2)}s actual) to ${outputPath}`)\n  return { path: outputPath, width, height, duration: videoDuration }\n}\n\nfunction validateZoomConstraints(cropMap: CropKF[], baseW: number, baseH: number): ValidationResult[] {\n  const results: ValidationResult[] = []\n  \n  const targetW = Math.round((baseH * 9) / 16)\n  const zMinWidth = targetW / baseW\n  const zMinHeight = 1.0\n  const expectedZMin = Math.max(zMinWidth, zMinHeight, 0.88)\n\n  results.push({\n    passed: true,\n    message: \"Z_min calculation\",\n    details: {\n      targetW,\n      zMinWidth: zMinWidth.toFixed(3),\n      zMinHeight: zMinHeight.toFixed(3),\n      expectedZMin: expectedZMin.toFixed(3)\n    }\n  })\n\n  const zoomValues = cropMap.map(kf => kf.z)\n  const minZ = Math.min(...zoomValues)\n  const maxZ = Math.max(...zoomValues)\n\n  results.push({\n    passed: minZ >= expectedZMin - 0.001,\n    message: \"Zoom values respect z_min constraint\",\n    details: {\n      minZ: minZ.toFixed(3),\n      maxZ: maxZ.toFixed(3),\n      expectedZMin: expectedZMin.toFixed(3),\n      allValid: minZ >= expectedZMin - 0.001\n    }\n  })\n\n  let invalidCrops = 0\n  for (const kf of cropMap.slice(0, 10)) {\n    const cropW = Math.round(baseW / kf.z)\n    const cropH = Math.round(baseH / kf.z)\n    \n    if (cropW > baseW || cropH > baseH) {\n      invalidCrops++\n    }\n  }\n\n  results.push({\n    passed: invalidCrops === 0,\n    message: \"Crop dimensions within video bounds\",\n    details: {\n      baseW,\n      baseH,\n      invalidCrops,\n      sampledKeyframes: Math.min(10, cropMap.length)\n    }\n  })\n\n  return results\n}\n\nexport async function POST(request: Request) {\n  let userId: string\n  try {\n    userId = await getCurrentUserId()\n  }\n  catch (error) {\n    if (error instanceof Error && error.message === \"Authentication required\") {\n      return NextResponse.json({\n        success: false,\n        error: \"Authentication required. Please log in to use the framing test.\"\n      }, { status: 401 })\n    }\n    throw error\n  }\n  \n  if (!userId) {\n    return NextResponse.json({\n      success: false,\n      error: \"User ID not found in session. Please log in again.\"\n    }, { status: 401 })\n  }\n  \n  const testId = Date.now()\n  const tempDir = path.join(process.cwd(), \"tmp\", `framing_test_${testId}`)\n  fs.mkdirSync(tempDir, { recursive: true })\n  \n  let shouldCleanup = true\n  \n  try {\n    const { initializeCanvas } = await import(\"@/src/services/framingService\");\n    await initializeCanvas();\n    \n    const body = await request.json()\n    const url = body.url || \"https://www.youtube.com/watch?v=EngW7tLk6R8\"\n    const startTimeInput = body.startTime || \"0\"\n    const endTimeInput = body.endTime || (body.duration ? String(body.duration) : \"180\")\n    \n    const startTime = parseTimeString(startTimeInput)\n    const endTime = parseTimeString(endTimeInput)\n    const duration = endTime - startTime\n    \n    if (duration <= 0 || duration > 300) {\n      return NextResponse.json({\n        success: false,\n        error: `Invalid duration: ${duration}s (must be 1-300s)`,\n        testId\n      }, { status: 400 })\n    }\n    \n    console.log(`[Framing Test] Starting test ${testId}`)\n    console.log(`[Framing Test] URL: ${url}`)\n    console.log(`[Framing Test] Time range: ${startTime}s to ${endTime}s (${duration}s)`)\n    \n    const { path: videoPath, width, height, duration: actualDuration } = await downloadTestVideo(url, startTime, endTime, tempDir, userId)\n    \n    const introEndSec = await getIntroEndFromChapters(url)\n    const renderStartTime = introEndSec && introEndSec < actualDuration ? introEndSec : 0\n    const renderDuration = actualDuration - renderStartTime\n    \n    if (introEndSec)\n    {\n      console.log(`[Framing Test] Intro chapter detected at ${introEndSec.toFixed(1)}s, will skip in final render`)\n    }\n    \n    console.log(`[Framing Test] Computing GLOBAL crop for entire video...`)\n    const globalCrop = await computeGlobalStaticCrop(videoPath, actualDuration, width, height, { skipUntilSec: introEndSec ?? 0 })\n    \n    if (globalCrop) {\n      console.log(`[Framing Test] ✓ Global crop computed successfully`)\n    } else {\n      console.log(`[Framing Test] ⚠️  Global crop failed, will use per-segment detection`)\n    }\n    \n    console.log(`[Framing Test] Running person detection and framing (static mode)...`)\n    const cropMap = await computeCropMapPersonStatic(\n      {\n        videoPath,\n        baseW: width,\n        baseH: height,\n        segStart: renderStartTime,\n        segEnd: actualDuration,\n        transcript: []\n      },\n      {\n        margin: 0.02,\n        maxPan: 400,\n        easeMs: 600,\n        centerBiasX: 0.75,\n        centerBiasY: 0.15,\n        safeTop: 0.05,\n        safeBottom: 0.1\n      },\n      globalCrop\n    )\n\n    if (!cropMap || cropMap.length === 0) {\n      return NextResponse.json({\n        success: false,\n        error: \"No people detected in video\",\n        testId\n      })\n    }\n\n    console.log(`[Framing Test] Generated crop map with ${cropMap.length} keyframes`)\n    \n    const validations = validateZoomConstraints(cropMap, width, height)\n    \n    const outputPath = path.join(tempDir, \"framed.mp4\")\n    console.log(`[Framing Test] Rendering framed video from ${renderStartTime.toFixed(1)}s to ${actualDuration.toFixed(1)}s...`)\n    \n    const filterExpr = buildFFmpegFilter(width, height, cropMap)\n    \n    await renderSmartFramedClip({\n      inputPath: videoPath,\n      outputPath,\n      startTime: renderStartTime,\n      duration: renderDuration,\n      srtPath: \"\",\n      filterExpr\n    })\n\n    if (!fs.existsSync(outputPath)) {\n      return NextResponse.json({\n        success: false,\n        error: \"Render failed: output file not found\",\n        testId\n      })\n    }\n\n    let outputW = 1080\n    let outputH = 1920\n    let outputValid = true\n\n    try {\n      const { stdout: outputProbe } = await execFile(require(\"ffprobe-static\").path, [\n        \"-v\", \"error\",\n        \"-select_streams\", \"v:0\",\n        \"-show_entries\", \"stream=width,height\",\n        \"-of\", \"json\",\n        outputPath\n      ])\n      \n      const outputInfo = JSON.parse(outputProbe)\n      \n      if (outputInfo.streams && outputInfo.streams.length > 0) {\n        outputW = outputInfo.streams[0].width || 1080\n        outputH = outputInfo.streams[0].height || 1920\n        outputValid = Math.abs(outputW / outputH - 9 / 16) < 0.01\n      }\n    }\n    catch (error: any) {\n      console.warn(`[Framing Test] Failed to probe output video: ${error.message}. Using defaults.`)\n    }\n\n    console.log(`[Framing Test] Test complete - ${validations.every(v => v.passed) && outputValid ? 'PASSED' : 'FAILED'}`)\n\n    const publicDir = path.join(process.cwd(), \"public\", \"test-results\")\n    fs.mkdirSync(publicDir, { recursive: true })\n    \n    const publicOriginal = path.join(publicDir, `original_${testId}.mp4`)\n    const publicFramed = path.join(publicDir, `framed_${testId}.mp4`)\n    \n    fs.copyFileSync(videoPath, publicOriginal)\n    fs.copyFileSync(outputPath, publicFramed)\n\n    return NextResponse.json({\n      success: true,\n      testId,\n      results: {\n        video: {\n          width,\n          height,\n          duration: actualDuration,\n        },\n        globalCrop: globalCrop ? {\n          cropX: globalCrop.cropX,\n          cropY: globalCrop.cropY,\n          cropW: globalCrop.cropW,\n          cropH: globalCrop.cropH,\n          zMin: globalCrop.zMin,\n        } : null,\n        detection: {\n          keyframes: cropMap.length,\n          requestedDuration: duration,\n          actualDuration: actualDuration\n        },\n        validations: validations.map(v => ({\n          test: v.message,\n          passed: v.passed,\n          details: v.details\n        })),\n        output: {\n          width: outputW,\n          height: outputH,\n          aspectRatio: (outputW / outputH).toFixed(3),\n          expectedAspectRatio: (9 / 16).toFixed(3),\n          valid: outputValid\n        },\n        videos: {\n          original: `/test-results/original_${testId}.mp4`,\n          framed: `/test-results/framed_${testId}.mp4`\n        }\n      }\n    })\n  }\n  catch (error: any) {\n    console.error(\"[Framing Test] Error:\", error)\n    \n    return NextResponse.json({\n      success: false,\n      error: error.message || \"Test failed\",\n      testId\n    }, { status: 500 })\n  }\n  finally {\n    if (fs.existsSync(tempDir)) {\n      fs.rmSync(tempDir, { recursive: true, force: true })\n      console.log(`[Framing Test] Cleaned up temp directory: ${tempDir}`)\n    }\n  }\n}\n","size_bytes":11774},"src/test-framing.ts":{"content":"import path from \"path\"\nimport fs from \"fs\"\nimport { promisify } from \"util\"\nimport { execFile } from \"child_process\"\nimport { computeCropMapPerson, buildFFmpegFilter, type ComputeInput, type Constraints, type TranscriptWord } from \"./services/framingService\"\nimport { renderSmartFramedClip } from \"./services/ffmpeg\"\n\nconst execFileAsync = promisify(execFile)\n\nconst TEST_DURATION = 10\n\nasync function createTestVideo(): Promise<string> {\n  const tempDir = path.join(process.cwd(), \"tmp\", `framing_test_${Date.now()}`)\n  fs.mkdirSync(tempDir, { recursive: true })\n  const outputPath = path.join(tempDir, \"test.mp4\")\n  \n  console.log(`[Test] Creating synthetic test video (1920x1080, ${TEST_DURATION}s)...`)\n  console.log(`[Test] Simulating two people standing far apart with moving colored boxes...`)\n  \n  const ffmpegPath = require(\"ffmpeg-static\")\n  \n  try {\n    await execFileAsync(ffmpegPath, [\n      \"-f\", \"lavfi\",\n      \"-i\", `color=c=black:s=1920x1080:d=${TEST_DURATION}:r=25`,\n      \"-vf\", `drawbox=x=200:y=200:w=300:h=600:color=blue:t=fill,drawbox=x=1400:y=200:w=300:h=600:color=red:t=fill,drawtext=text='Person 1':fontsize=40:fontcolor=white:x=250:y=400,drawtext=text='Person 2':fontsize=40:fontcolor=white:x=1450:y=400`,\n      \"-pix_fmt\", \"yuv420p\",\n      \"-c:v\", \"libx264\",\n      \"-preset\", \"ultrafast\",\n      \"-y\",\n      outputPath\n    ], { timeout: 60000, maxBuffer: 10 * 1024 * 1024 })\n    \n    if (!fs.existsSync(outputPath)) {\n      throw new Error(\"Video creation failed: output file not found\")\n    }\n    \n    console.log(`[Test] ✓ Created test video at ${outputPath}`)\n    return outputPath\n  } catch (error) {\n    console.error(`[Test] Video creation failed:`, error)\n    throw error\n  }\n}\n\nasync function getVideoInfo(videoPath: string): Promise<{ width: number; height: number; duration: number }> {\n  const ffprobePath = require(\"ffprobe-static\").path\n  \n  const { stdout } = await execFileAsync(ffprobePath, [\n    \"-v\", \"error\",\n    \"-select_streams\", \"v:0\",\n    \"-show_entries\", \"stream=width,height,duration\",\n    \"-of\", \"json\",\n    videoPath\n  ], { maxBuffer: 10 * 1024 * 1024 })\n  \n  const data = JSON.parse(stdout)\n  const stream = data.streams[0]\n  \n  return {\n    width: parseInt(stream.width),\n    height: parseInt(stream.height),\n    duration: parseFloat(stream.duration || \"0\")\n  }\n}\n\nasync function testFraming() {\n  console.log(\"=\".repeat(80))\n  console.log(\"FRAMING SYSTEM TEST\")\n  console.log(\"=\".repeat(80))\n  console.log()\n  console.log(\"Test scenario: Two people standing far apart\")\n  console.log(\"Expected behavior: System should use z_min >= 1.0 to prevent impossible crops\")\n  console.log()\n  \n  let videoPath: string\n  let videoInfo: { width: number; height: number; duration: number }\n  \n  try {\n    videoPath = await createTestVideo()\n    videoInfo = await getVideoInfo(videoPath)\n    \n    console.log(`[Test] Video info: ${videoInfo.width}x${videoInfo.height}, ${videoInfo.duration.toFixed(1)}s`)\n    console.log()\n    \n    const targetW = Math.floor((videoInfo.height * 9) / 16)\n    const zMinWidth = targetW / videoInfo.width\n    const zMinHeight = 1.0\n    const zMinExpected = Math.max(zMinWidth, zMinHeight, 0.88)\n    \n    console.log(`[Test] Expected z_min calculation:`)\n    console.log(`  - targetW: ${targetW}px (9:16 from ${videoInfo.height}px height)`)\n    console.log(`  - z_min_width: ${zMinWidth.toFixed(3)} (${targetW} / ${videoInfo.width})`)\n    console.log(`  - z_min_height: ${zMinHeight.toFixed(3)} (always 1.0 for full height)`)\n    console.log(`  - z_min_expected: ${zMinExpected.toFixed(3)}`)\n    console.log()\n    \n    const input: ComputeInput = {\n      videoPath,\n      segStart: 0,\n      segEnd: Math.min(videoInfo.duration, TEST_DURATION),\n      baseW: videoInfo.width,\n      baseH: videoInfo.height,\n      transcript: [] as TranscriptWord[]\n    }\n    \n    const constraints: Constraints = {\n      margin: 0.05,\n      safeTop: 0.15,\n      safeBottom: 0.1,\n      maxPan: 150,\n      easeMs: 500,\n      centerBiasX: 0.5,\n      centerBiasY: 0.4\n    }\n    \n    console.log(`[Test] Running person detection and framing calculation...`)\n    console.log()\n    \n    const cropMap = await computeCropMapPerson(input, constraints)\n    \n    if (!cropMap || cropMap.length === 0) {\n      console.error(`[Test] ❌ FAILED: No crop map generated (no people detected?)`)\n      process.exit(1)\n    }\n    \n    console.log(`[Test] ✓ Generated ${cropMap.length} framing keyframes`)\n    console.log()\n    \n    const zoomValues = cropMap.map(k => k.z)\n    const zMin = Math.min(...zoomValues)\n    const zMax = Math.max(...zoomValues)\n    const zAvg = zoomValues.reduce((a, b) => a + b, 0) / zoomValues.length\n    \n    console.log(`[Test] Zoom statistics:`)\n    console.log(`  - z_min: ${zMin.toFixed(3)}`)\n    console.log(`  - z_max: ${zMax.toFixed(3)}`)\n    console.log(`  - z_avg: ${zAvg.toFixed(3)}`)\n    console.log()\n    \n    if (zMin < zMinExpected - 0.001) {\n      console.error(`[Test] ❌ FAILED: Zoom below expected minimum!`)\n      console.error(`  Expected z >= ${zMinExpected.toFixed(3)}, but got z_min = ${zMin.toFixed(3)}`)\n      process.exit(1)\n    }\n    \n    console.log(`[Test] ✓ All zoom values >= ${zMinExpected.toFixed(3)}`)\n    console.log()\n    \n    console.log(`[Test] Validating crop dimensions...`)\n    let allValid = true\n    const samplesToCheck = Math.min(10, cropMap.length)\n    \n    for (let i = 0; i < samplesToCheck; i++) {\n      const idx = Math.floor((i * cropMap.length) / samplesToCheck)\n      const k = cropMap[idx]\n      const cropW = Math.round(targetW / k.z)\n      const cropH = Math.round(videoInfo.height / k.z)\n      \n      if (cropW > videoInfo.width || cropH > videoInfo.height) {\n        console.error(`[Test] ❌ Invalid crop at t=${k.t.toFixed(2)}s: crop=(${cropW}x${cropH}) > video=(${videoInfo.width}x${videoInfo.height})`)\n        allValid = false\n      }\n      \n      if (k.x < 0 || k.y < 0 || k.x + cropW > videoInfo.width || k.y + cropH > videoInfo.height) {\n        console.error(`[Test] ❌ Invalid crop position at t=${k.t.toFixed(2)}s: pos=(${k.x},${k.y}) size=(${cropW}x${cropH})`)\n        allValid = false\n      }\n    }\n    \n    if (!allValid) {\n      console.error(`[Test] ❌ FAILED: Invalid crop dimensions detected`)\n      process.exit(1)\n    }\n    \n    console.log(`[Test] ✓ All ${samplesToCheck} sampled crops are valid`)\n    console.log()\n    \n    console.log(`[Test] Generating FFmpeg filter...`)\n    try {\n      const filter = buildFFmpegFilter(videoInfo.width, videoInfo.height, cropMap)\n      console.log(`[Test] ✓ FFmpeg filter generated successfully`)\n      console.log(`[Test] Filter length: ${filter.length} characters`)\n      console.log()\n    } catch (error) {\n      console.error(`[Test] ❌ FAILED: FFmpeg filter generation failed:`, error)\n      process.exit(1)\n    }\n    \n    console.log(`[Test] Rendering test clip...`)\n    const outputDir = path.join(path.dirname(videoPath), \"test_output\")\n    fs.mkdirSync(outputDir, { recursive: true })\n    const outputPath = path.join(outputDir, \"framing_test.mp4\")\n    \n    try {\n      const filterExpr = buildFFmpegFilter(videoInfo.width, videoInfo.height, cropMap)\n      \n      await renderSmartFramedClip({\n        inputPath: videoPath,\n        outputPath,\n        startTime: 0,\n        duration: Math.min(videoInfo.duration, TEST_DURATION),\n        srtPath: \"\",\n        filterExpr\n      })\n      \n      if (fs.existsSync(outputPath)) {\n        const stats = fs.statSync(outputPath)\n        console.log(`[Test] ✓ Test clip rendered successfully`)\n        console.log(`[Test] Output: ${outputPath}`)\n        console.log(`[Test] Size: ${(stats.size / 1024 / 1024).toFixed(2)} MB`)\n      } else {\n        throw new Error(\"Output file not found\")\n      }\n    } catch (error) {\n      console.error(`[Test] ❌ Rendering failed:`, error)\n      process.exit(1)\n    }\n    \n    console.log()\n    console.log(\"=\".repeat(80))\n    console.log(\"✓ ALL TESTS PASSED\")\n    console.log(\"=\".repeat(80))\n    console.log()\n    console.log(\"Summary:\")\n    console.log(`  - Dimension-aware z_min working correctly (${zMinExpected.toFixed(3)})`)\n    console.log(`  - All ${cropMap.length} keyframes have valid zoom values`)\n    console.log(`  - All crop dimensions within video bounds`)\n    console.log(`  - FFmpeg filter generated successfully`)\n    console.log(`  - Test clip rendered successfully`)\n    console.log()\n    console.log(`Visual verification: ${outputPath}`)\n    \n  } catch (error) {\n    console.error(`[Test] ❌ Test failed:`, error)\n    process.exit(1)\n  }\n}\n\ntestFraming().catch((error) => {\n  console.error(\"Fatal error:\", error)\n  process.exit(1)\n})\n","size_bytes":8643},"src/services/detectors/poseDetector.ts":{"content":"import * as poseDetection from \"@tensorflow-models/pose-detection\"\nimport * as tf from \"@tensorflow/tfjs-node\"\n\nexport type Detection = {\n  x: number\n  y: number\n  w: number\n  h: number\n  score: number\n}\n\nlet poseDetector: poseDetection.PoseDetector | null = null\n\nconst POSE_CONF = Math.max(0, Math.min(1, Number(process.env.POSE_CONF || 0.3)))\nconst POSE_MIN_SIZE = Math.max(20, Number(process.env.POSE_MIN_SIZE || 100))\n\nasync function loadPoseModel(): Promise<void> {\n  if (poseDetector) {\n    return\n  }\n  const model = poseDetection.SupportedModels.MoveNet\n  poseDetector = await poseDetection.createDetector(model, {\n    modelType: poseDetection.movenet.modelType.SINGLEPOSE_LIGHTNING\n  })\n  console.log(`[Pose Detection] Successfully loaded MoveNet SINGLEPOSE_LIGHTNING model`)\n}\n\nexport async function detectPoses(\n  img: tf.Tensor3D,\n  baseW: number,\n  baseH: number\n): Promise<Detection[]> {\n  await loadPoseModel()\n  \n  const srcH = img.shape[0]\n  const srcW = img.shape[1]\n  \n  const poses = await poseDetector!.estimatePoses(img)\n  \n  const out: Detection[] = []\n  for (const pose of poses) {\n    if (!pose.keypoints || pose.keypoints.length === 0) {\n      continue\n    }\n    \n    const validPoints = pose.keypoints.filter((kp) => {\n      return kp.score && kp.score >= POSE_CONF\n    })\n    \n    if (validPoints.length < 3) {\n      continue\n    }\n    \n    let minX = srcW\n    let minY = srcH\n    let maxX = 0\n    let maxY = 0\n    let scoreSum = 0\n    \n    for (const kp of validPoints) {\n      minX = Math.min(minX, kp.x)\n      minY = Math.min(minY, kp.y)\n      maxX = Math.max(maxX, kp.x)\n      maxY = Math.max(maxY, kp.y)\n      scoreSum += kp.score || 0\n    }\n    \n    const avgScore = scoreSum / validPoints.length\n    const w = maxX - minX\n    const h = maxY - minY\n    \n    if (w < POSE_MIN_SIZE || h < POSE_MIN_SIZE) {\n      continue\n    }\n    \n    const bx = Math.round(minX * (baseW / srcW))\n    const by = Math.round(minY * (baseH / srcH))\n    const bw = Math.round(w * (baseW / srcW))\n    const bh = Math.round(h * (baseH / srcH))\n    \n    out.push({\n      x: bx,\n      y: by,\n      w: bw,\n      h: bh,\n      score: avgScore\n    })\n  }\n  \n  return out\n}\n","size_bytes":2178},"src/services/detectors/faceDetector.ts":{"content":"import * as faceapi from \"@vladmandic/face-api\"\nimport * as tf from \"@tensorflow/tfjs-node\"\nimport * as path from \"path\"\n\nexport type Detection = {\n  x: number\n  y: number\n  w: number\n  h: number\n  score: number\n}\n\nlet faceModelsLoaded = false\n\nconst FACE_CONF = Math.max(0, Math.min(1, Number(process.env.FACE_CONF || 0.5)))\nconst FACE_MIN_SIZE = Math.max(20, Number(process.env.FACE_MIN_SIZE || 80))\n\nasync function loadFaceModels(): Promise<void> {\n  if (faceModelsLoaded) {\n    return\n  }\n  const modelsPath = path.join(process.cwd(), \"models\")\n  await faceapi.nets.ssdMobilenetv1.loadFromDisk(modelsPath)\n  faceModelsLoaded = true\n  console.log(`[Face Detection] Successfully loaded SSD MobileNet v1 model from ${modelsPath}`)\n}\n\nexport async function detectFaces(\n  img: tf.Tensor3D,\n  baseW: number,\n  baseH: number\n): Promise<Detection[]> {\n  await loadFaceModels()\n  \n  const srcH = img.shape[0]\n  const srcW = img.shape[1]\n  \n  const detections = await faceapi.detectAllFaces(\n    img as any,\n    new faceapi.SsdMobilenetv1Options({ minConfidence: FACE_CONF })\n  )\n  \n  const out: Detection[] = []\n  for (const det of detections) {\n    const box = det.box\n    const x1 = Math.max(0, box.x)\n    const y1 = Math.max(0, box.y)\n    const x2 = Math.min(srcW, box.x + box.width)\n    const y2 = Math.min(srcH, box.y + box.height)\n    const w = x2 - x1\n    const h = y2 - y1\n    \n    if (w < FACE_MIN_SIZE || h < FACE_MIN_SIZE) {\n      continue\n    }\n    \n    const bx = Math.round(x1 * (baseW / srcW))\n    const by = Math.round(y1 * (baseH / srcH))\n    const bw = Math.round(w * (baseW / srcW))\n    const bh = Math.round(h * (baseH / srcH))\n    \n    out.push({\n      x: bx,\n      y: by,\n      w: bw,\n      h: bh,\n      score: det.score\n    })\n  }\n  \n  return out\n}\n","size_bytes":1768},"src/services/detectors/index.ts":{"content":"import * as tf from \"@tensorflow/tfjs-node\"\nimport { detectFaces } from \"./faceDetector\"\nimport { detectPoses } from \"./poseDetector\"\n\nexport type PersonDet = {\n  x: number\n  y: number\n  w: number\n  h: number\n  score: number\n  detectorType?: 'face' | 'pose'\n}\n\nexport async function detectPersons(\n  img: tf.Tensor3D,\n  baseW: number,\n  baseH: number\n): Promise<PersonDet[]> {\n  const faces = await detectFaces(img, baseW, baseH)\n  \n  if (faces.length > 0) {\n    return faces.map(f => ({ ...f, detectorType: 'face' as const }))\n  }\n  \n  const poses = await detectPoses(img, baseW, baseH)\n  return poses.map(p => ({ ...p, detectorType: 'pose' as const }))\n}\n","size_bytes":657},"app/test/framing/page.tsx":{"content":"\"use client\"\n\nimport { useState } from \"react\"\nimport { Box, Button, Typography, Paper, LinearProgress, Alert, Chip, TextField, Select, MenuItem, FormControl, InputLabel } from \"@mui/material\"\nimport CheckCircleIcon from \"@mui/icons-material/CheckCircle\"\nimport CancelIcon from \"@mui/icons-material/Cancel\"\nimport PlayArrowIcon from \"@mui/icons-material/PlayArrow\"\n\ntype ValidationResult = {\n  test: string\n  passed: boolean\n  details: any\n}\n\ntype TestResults = {\n  video: {\n    width: number\n    height: number\n    duration: number\n  }\n  globalCrop: {\n    cropX: number\n    cropY: number\n    cropW: number\n    cropH: number\n    zMin: number\n  } | null\n  detection: {\n    keyframes: number\n    requestedDuration: number\n    actualDuration: number\n  }\n  validations: ValidationResult[]\n  output: {\n    width: number\n    height: number\n    aspectRatio: string\n    expectedAspectRatio: string\n    valid: boolean\n  }\n  videos: {\n    original: string\n    framed: string\n  }\n}\n\nexport default function FramingTestPage() {\n  const [testing, setTesting] = useState(false)\n  const [results, setResults] = useState<TestResults | null>(null)\n  const [error, setError] = useState<string | null>(null)\n  const [url, setUrl] = useState(\"https://www.youtube.com/watch?v=EngW7tLk6R8\")\n  const [durationMinutes, setDurationMinutes] = useState(3)\n  const [progress, setProgress] = useState(\"\")\n\n  const runTest = async () => {\n    setTesting(true)\n    setError(null)\n    setResults(null)\n    setProgress(\"Step 1/3: Downloading video from YouTube...\")\n\n    const timer1 = setTimeout(() => {\n      setProgress(\"Step 2/3: Running TinyFaceDetector on sampled frames...\")\n    }, 5000)\n\n    const timer2 = setTimeout(() => {\n      setProgress(\"Step 3/3: Computing two-speaker-aware crop and rendering output...\")\n    }, 15000)\n\n    try {\n      const response = await fetch(\"/api/test/framing\", {\n        method: \"POST\",\n        headers: {\n          \"Content-Type\": \"application/json\"\n        },\n        body: JSON.stringify({\n          url,\n          durationMinutes\n        })\n      })\n      \n      const data = await response.json()\n      \n      if (!data.success) {\n        clearTimeout(timer1)\n        clearTimeout(timer2)\n        setError(data.error || \"Test failed\")\n        setProgress(\"\")\n        return\n      }\n\n      clearTimeout(timer1)\n      clearTimeout(timer2)\n      setProgress(\"✓ Test completed successfully!\")\n      setResults(data.results)\n    }\n    catch (err: any) {\n      clearTimeout(timer1)\n      clearTimeout(timer2)\n      setError(err.message || \"Network error\")\n      setProgress(\"\")\n    }\n    finally {\n      setTesting(false)\n    }\n  }\n\n  return (\n    <Box sx={{ p: 4, maxWidth: 1400, mx: \"auto\" }}>\n      <Typography variant=\"h4\" gutterBottom>\n        Smart Framing Test\n      </Typography>\n      \n      <Typography variant=\"body1\" color=\"text.secondary\" sx={{ mb: 4 }}>\n        Test the TinyFaceDetector + two-speaker-aware global crop with any YouTube video. \n        The system computes ONE static crop per video using face detection only and visualizes the 9:16 crop rectangle.\n      </Typography>\n\n      <Paper sx={{ p: 3, mb: 4 }}>\n        <Typography variant=\"h6\" gutterBottom>\n          Test Configuration\n        </Typography>\n        \n        <Box sx={{ display: \"flex\", flexDirection: \"column\", gap: 3 }}>\n          <TextField\n            label=\"YouTube URL\"\n            value={url}\n            onChange={(e) => setUrl(e.target.value)}\n            fullWidth\n            disabled={testing}\n            placeholder=\"https://www.youtube.com/watch?v=...\"\n            helperText=\"Enter any YouTube video URL (interviews and talking heads work best)\"\n          />\n          \n          <FormControl fullWidth disabled={testing}>\n            <InputLabel>Duration to Process</InputLabel>\n            <Select\n              value={durationMinutes}\n              onChange={(e) => setDurationMinutes(e.target.value as number)}\n              label=\"Duration to Process\"\n            >\n              <MenuItem value={1}>1 minute</MenuItem>\n              <MenuItem value={2}>2 minutes</MenuItem>\n              <MenuItem value={3}>3 minutes (recommended)</MenuItem>\n              <MenuItem value={4}>4 minutes</MenuItem>\n              <MenuItem value={5}>5 minutes (max)</MenuItem>\n            </Select>\n          </FormControl>\n\n          <Button\n            variant=\"contained\"\n            size=\"large\"\n            onClick={runTest}\n            disabled={testing || !url}\n            startIcon={<PlayArrowIcon />}\n            fullWidth\n          >\n            {testing ? \"Testing...\" : \"Center Faces\"}\n          </Button>\n        </Box>\n      </Paper>\n\n      {(testing || progress) && (\n        <Paper sx={{ p: 3, mb: 4 }}>\n          <Typography variant=\"h6\" gutterBottom>\n            {testing ? \"Running Test...\" : \"Test Status\"}\n          </Typography>\n          {testing && <LinearProgress sx={{ mb: 2 }} />}\n          {progress && (\n            <Typography variant=\"body2\" color={progress.startsWith(\"✓\") ? \"success.main\" : \"text.secondary\"}>\n              {progress}\n            </Typography>\n          )}\n          {testing && (\n            <Typography variant=\"caption\" color=\"text.secondary\" sx={{ mt: 1, display: \"block\" }}>\n              This may take 30-120 seconds depending on video length and detection complexity\n            </Typography>\n          )}\n        </Paper>\n      )}\n\n      {error && (\n        <Alert severity=\"error\" sx={{ mb: 4 }}>\n          <Typography variant=\"h6\">Test Failed</Typography>\n          <Typography variant=\"body2\">{error}</Typography>\n        </Alert>\n      )}\n\n      {results && (\n        <>\n          {results.globalCrop && (\n            <Paper sx={{ p: 3, mb: 4 }}>\n              <Typography variant=\"h6\" gutterBottom>\n                Global Crop (TinyFaceDetector + Two-Speaker-Aware)\n              </Typography>\n              <Box sx={{ display: \"grid\", gridTemplateColumns: \"repeat(auto-fit, minmax(150px, 1fr))\", gap: 2, mb: 3 }}>\n                <Box>\n                  <Typography variant=\"body2\" color=\"text.secondary\">Video Size</Typography>\n                  <Typography variant=\"body1\">{results.video.width}x{results.video.height}</Typography>\n                </Box>\n                <Box>\n                  <Typography variant=\"body2\" color=\"text.secondary\">Crop Position</Typography>\n                  <Typography variant=\"body1\">({results.globalCrop.cropX}, {results.globalCrop.cropY})</Typography>\n                </Box>\n                <Box>\n                  <Typography variant=\"body2\" color=\"text.secondary\">Crop Size</Typography>\n                  <Typography variant=\"body1\">{results.globalCrop.cropW}x{results.globalCrop.cropH}</Typography>\n                </Box>\n                <Box>\n                  <Typography variant=\"body2\" color=\"text.secondary\">Zoom Factor</Typography>\n                  <Typography variant=\"body1\">{results.globalCrop.zMin.toFixed(2)}</Typography>\n                </Box>\n              </Box>\n              \n              <Typography variant=\"subtitle2\" gutterBottom sx={{ mt: 2 }}>\n                Crop Visualization\n              </Typography>\n              <Box sx={{ position: \"relative\", display: \"inline-block\", maxWidth: \"100%\" }}>\n                <Box\n                  component=\"video\"\n                  src={results.videos.original}\n                  sx={{\n                    width: \"100%\",\n                    maxWidth: 800,\n                    display: \"block\",\n                    bgcolor: \"black\",\n                    borderRadius: 1\n                  }}\n                />\n                <Box\n                  sx={{\n                    position: \"absolute\",\n                    left: `${(results.globalCrop.cropX / results.video.width) * 100}%`,\n                    top: `${(results.globalCrop.cropY / results.video.height) * 100}%`,\n                    width: `${(results.globalCrop.cropW / results.video.width) * 100}%`,\n                    height: `${(results.globalCrop.cropH / results.video.height) * 100}%`,\n                    border: \"3px solid #ff0000\",\n                    pointerEvents: \"none\",\n                    boxShadow: \"0 0 0 9999px rgba(0,0,0,0.5)\"\n                  }}\n                />\n              </Box>\n              <Typography variant=\"caption\" color=\"text.secondary\" display=\"block\" sx={{ mt: 1 }}>\n                Red rectangle shows the 9:16 crop area. This exact crop is used for all clips from this video.\n              </Typography>\n            </Paper>\n          )}\n\n          <Paper sx={{ p: 3, mb: 4 }}>\n            <Typography variant=\"h6\" gutterBottom>\n              Detection Results\n            </Typography>\n            <Box sx={{ display: \"flex\", gap: 4 }}>\n              <Box sx={{ flex: 1 }}>\n                <Typography variant=\"body2\" color=\"text.secondary\">\n                  Keyframes Generated\n                </Typography>\n                <Typography variant=\"h5\">{results.detection.keyframes}</Typography>\n              </Box>\n              <Box sx={{ flex: 1 }}>\n                <Typography variant=\"body2\" color=\"text.secondary\">\n                  Duration\n                </Typography>\n                <Typography variant=\"h5\">{results.detection.actualDuration.toFixed(1)}s</Typography>\n              </Box>\n            </Box>\n          </Paper>\n\n          <Paper sx={{ p: 3, mb: 4 }}>\n            <Typography variant=\"h6\" gutterBottom>\n              Validation Results\n            </Typography>\n            <Box sx={{ display: \"flex\", flexDirection: \"column\", gap: 2 }}>\n              {results.validations.map((validation, idx) => (\n                <Box key={idx} sx={{ display: \"flex\", alignItems: \"center\", gap: 1 }}>\n                  {validation.passed ? (\n                    <CheckCircleIcon color=\"success\" />\n                  ) : (\n                    <CancelIcon color=\"error\" />\n                  )}\n                  <Typography variant=\"body1\" sx={{ flex: 1 }}>\n                    {validation.test}\n                  </Typography>\n                  {validation.details && (\n                    <Box sx={{ display: \"flex\", gap: 1, flexWrap: \"wrap\" }}>\n                      {Object.entries(validation.details).map(([key, value]) => (\n                        <Chip\n                          key={key}\n                          label={`${key}: ${value}`}\n                          size=\"small\"\n                          variant=\"outlined\"\n                        />\n                      ))}\n                    </Box>\n                  )}\n                </Box>\n              ))}\n              \n              <Box sx={{ display: \"flex\", alignItems: \"center\", gap: 1 }}>\n                {results.output.valid ? (\n                  <CheckCircleIcon color=\"success\" />\n                ) : (\n                  <CancelIcon color=\"error\" />\n                )}\n                <Typography variant=\"body1\" sx={{ flex: 1 }}>\n                  Output aspect ratio validation\n                </Typography>\n                <Chip\n                  label={`${results.output.width}x${results.output.height}`}\n                  size=\"small\"\n                  variant=\"outlined\"\n                />\n                <Chip\n                  label={`Ratio: ${results.output.aspectRatio}`}\n                  size=\"small\"\n                  variant=\"outlined\"\n                />\n              </Box>\n            </Box>\n          </Paper>\n\n          <Paper sx={{ p: 3 }}>\n            <Typography variant=\"h6\" gutterBottom>\n              Video Comparison\n            </Typography>\n            <Box sx={{ display: \"flex\", flexDirection: { xs: \"column\", md: \"row\" }, gap: 3 }}>\n              <Box sx={{ flex: 1 }}>\n                <Typography variant=\"subtitle1\" gutterBottom>\n                  Original 16:9\n                </Typography>\n                <Box\n                  component=\"video\"\n                  src={results.videos.original}\n                  controls\n                  sx={{\n                    width: \"100%\",\n                    maxHeight: 400,\n                    bgcolor: \"black\",\n                    borderRadius: 1\n                  }}\n                />\n              </Box>\n              <Box sx={{ flex: 1 }}>\n                <Typography variant=\"subtitle1\" gutterBottom>\n                  Framed 9:16\n                </Typography>\n                <Box\n                  component=\"video\"\n                  src={results.videos.framed}\n                  controls\n                  sx={{\n                    width: \"100%\",\n                    maxHeight: 400,\n                    bgcolor: \"black\",\n                    borderRadius: 1\n                  }}\n                />\n              </Box>\n            </Box>\n          </Paper>\n        </>\n      )}\n    </Box>\n  )\n}\n","size_bytes":12753},"attached_assets/processor_1764019232735.ts":{"content":"import { jobManager } from \"./jobs\";\nimport { getVideoInfo, downloadVideo, YouTubeAuthError } from \"./youtube\";\nimport {\n  extractFrames,\n  getVideoResolution,\n  renderVerticalVideo,\n} from \"./ffmpeg\";\nimport {\n  detectFacesInImage,\n  drawDebugFrame,\n  type FaceDetection,\n} from \"./faceDetector\";\nimport type { ProcessMode, CropInfo } from \"@shared/schema\";\nimport { promises as fs } from \"fs\";\n\nexport async function processVideo(jobId: string): Promise<void> {\n  const job = await jobManager.getJob(jobId);\n  if (!job) {\n    throw new Error(\"Job not found\");\n  }\n\n  try {\n    await jobManager.setStatus(jobId, \"processing\");\n    await jobManager.updateProgress(jobId, 0);\n\n    const { url, startSeconds, mode } = job;\n    if (!url || startSeconds === undefined || !mode) {\n      throw new Error(\"Missing job parameters\");\n    }\n\n    const durationSeconds = getModeDuration(mode, startSeconds);\n\n    if (durationSeconds > 7200) {\n      throw new Error(\n        \"Duração excede o limite máximo de 2 horas. Por favor, escolha um intervalo menor.\",\n      );\n    }\n\n    if (await jobManager.isJobCancelled(jobId)) {\n      await jobManager.cleanupJob(jobId);\n      return;\n    }\n\n    await jobManager.updateProgress(jobId, 5);\n    console.log(`[Job ${jobId}] Downloading video...`);\n    const inputPath = jobManager.getInputPath(jobId);\n    await downloadVideo(url, inputPath);\n\n    if (await jobManager.isJobCancelled(jobId)) {\n      await jobManager.cleanupJob(jobId);\n      return;\n    }\n\n    await jobManager.updateProgress(jobId, 20);\n    console.log(`[Job ${jobId}] Getting video resolution...`);\n    const resolution = await getVideoResolution(inputPath);\n\n    if (await jobManager.isJobCancelled(jobId)) {\n      await jobManager.cleanupJob(jobId);\n      return;\n    }\n\n    await jobManager.updateProgress(jobId, 25);\n    console.log(`[Job ${jobId}] Extracting frames...`);\n\n    const fps = getAdaptiveFps(durationSeconds);\n    const framesDir = jobManager.getFramesDir();\n\n    const frames = await extractFrames(\n      inputPath,\n      framesDir,\n      startSeconds,\n      durationSeconds,\n      fps,\n      jobId,\n      async (percent) => {\n        if (!(await jobManager.isJobCancelled(jobId))) {\n          await jobManager.updateProgress(jobId, 25 + (percent / 100) * 15);\n        }\n      },\n    );\n\n    if (await jobManager.isJobCancelled(jobId)) {\n      await jobManager.cleanupJob(jobId);\n      return;\n    }\n\n    if (frames.length === 0) {\n      throw new Error(\n        \"Não foi possível extrair frames válidos deste vídeo. Verifique se o vídeo está corrompido ou tente outro intervalo.\",\n      );\n    }\n\n    await jobManager.updateProgress(jobId, 40);\n    console.log(`[Job ${jobId}] Detecting faces in ${frames.length} frames...`);\n\n    const allDetections: FaceDetection[] = [];\n    const framesDetections: FrameDetections[] = [];\n    let framesWithFaces = 0;\n\n    // passo entre frames em segundos (aprox)\n    const frameStepSeconds =\n      frames.length > 1\n        ? durationSeconds / (frames.length - 1)\n        : durationSeconds;\n\n    for (let i = 0; i < frames.length; i++) {\n      if (await jobManager.isJobCancelled(jobId)) {\n        await jobManager.cleanupJob(jobId);\n        return;\n      }\n\n      const framePath = frames[i];\n      const currentTime = startSeconds + i * frameStepSeconds;\n\n      const detections = await detectFacesInImage(\n        framePath,\n        resolution.width,\n        resolution.height,\n      );\n\n      if (detections.length > 0) {\n        const sortedDetections = detections\n          // ligeiramente mais tolerante para apanhar o segundo convidado\n          .filter((d) => d.score >= 0.3)\n          // caras maiores primeiro\n          .sort((a, b) => b.width * b.height - a.width * a.height)\n          // até 3 caras por frame (host + convidado + eventual extra)\n          .slice(0, 3);\n\n        allDetections.push(...sortedDetections);\n        framesWithFaces++;\n\n        framesDetections.push({\n          index: i,\n          time: currentTime,\n          detections: sortedDetections,\n        });\n      } else {\n        // guardar frame sem caras – útil para a timeline\n        framesDetections.push({\n          index: i,\n          time: currentTime,\n          detections: [],\n        });\n      }\n\n      const progress = 40 + ((i + 1) / frames.length) * 35;\n      await jobManager.updateProgress(jobId, progress);\n    }\n\n    await jobManager.setFramesAnalyzed(jobId, frames.length);\n\n    // timeline de centros (Auto Reframe base)\n    const centerTimeline = buildCenterTimeline(\n      resolution,\n      framesDetections,\n      startSeconds,\n      durationSeconds,\n    );\n    console.log(\"[centerTimeline] primeiros 10 pontos:\", centerTimeline.slice(0, 10));\n\n\n    // por enquanto, ainda usamos crop fixo global (calculateCrop antigo)\n\n    if (await jobManager.isJobCancelled(jobId)) {\n      await jobManager.cleanupJob(jobId);\n      return;\n    }\n\n    await jobManager.updateProgress(jobId, 75);\n    console.log(\n      `[Job ${jobId}] Calculating crop (${allDetections.length} faces detected)...`,\n    );\n\n    const crop = calculateCrop(resolution, allDetections);\n    await jobManager.setCrop(jobId, crop);\n\n    const debugPath = jobManager.getDebugPath(jobId);\n    const debugFramePath =\n      frames.length > 0 ? frames[Math.floor(frames.length / 2)] : frames[0];\n\n    if (debugFramePath) {\n      const debugFrameDetections = await detectFacesInImage(\n        debugFramePath,\n        resolution.width,\n        resolution.height,\n      );\n      const debugSuccess = await drawDebugFrame(\n        debugFramePath,\n        debugPath,\n        debugFrameDetections,\n        crop.cropX,\n        crop.cropY,\n        crop.cropWidth,\n        crop.cropHeight,\n      );\n\n      if (debugSuccess) {\n        await jobManager.setDebugImagePath(jobId, debugPath);\n      } else {\n        console.warn(\n          `[Job ${jobId}] Debug frame generation failed (non-critical), skipping...`,\n        );\n      }\n    }\n\n    if (await jobManager.isJobCancelled(jobId)) {\n      await jobManager.cleanupJob(jobId);\n      return;\n    }\n\n    await jobManager.updateProgress(jobId, 80);\n    console.log(`[Job ${jobId}] Rendering vertical video...`);\n\n    const outputPath = jobManager.getOutputPath(jobId);\n    await renderVerticalVideo(\n      inputPath,\n      outputPath,\n      startSeconds,\n      durationSeconds,\n      crop.cropX,\n      crop.cropY,\n      crop.cropWidth,\n      crop.cropHeight,\n      async (percent) => {\n        if (!(await jobManager.isJobCancelled(jobId))) {\n          await jobManager.updateProgress(jobId, 80 + (percent / 100) * 19);\n        }\n      },\n    );\n\n    if (await jobManager.isJobCancelled(jobId)) {\n      await jobManager.cleanupJob(jobId);\n      return;\n    }\n\n    await jobManager.setDownloadPath(jobId, outputPath);\n    await jobManager.updateProgress(jobId, 100);\n    await jobManager.setStatus(jobId, \"done\");\n\n    console.log(`[Job ${jobId}] Processing completed successfully`);\n  } catch (error: any) {\n    console.error(`[Job ${jobId}] Processing error:`, error);\n\n    let errorMessage = error.message || \"Erro desconhecido\";\n\n    if (error instanceof YouTubeAuthError) {\n      errorMessage = error.message;\n    }\n\n    await jobManager.setStatus(jobId, \"error\", errorMessage);\n    await jobManager.cleanupJob(jobId);\n  }\n}\n\nfunction getModeDuration(mode: ProcessMode, startSeconds: number): number {\n  switch (mode) {\n    case \"3m\":\n      return 3 * 60;\n    case \"5m\":\n      return 5 * 60;\n    case \"full\":\n      return 2 * 60 * 60;\n    default:\n      return 3 * 60;\n  }\n}\n\nfunction getAdaptiveFps(durationSeconds: number): number {\n  if (durationSeconds <= 600) {\n    return 2;\n  } else if (durationSeconds <= 3600) {\n    return 5;\n  } else {\n    return 12;\n  }\n}\n\ninterface WeightedValue {\n  value: number;\n  weight: number;\n}\n\ninterface FrameDetections {\n  index: number; // índice do frame\n  time: number; // timestamp em segundos (desde o início do vídeo original)\n  detections: FaceDetection[]; // caras nesse frame\n}\n\nfunction weightedMedian(values: WeightedValue[]): number {\n  if (values.length === 0) {\n    return 0;\n  }\n\n  const sorted = [...values].sort((a, b) => {\n    if (a.value < b.value) {\n      return -1;\n    }\n    if (a.value > b.value) {\n      return 1;\n    }\n    return 0;\n  });\n\n  const totalWeight = sorted.reduce((sum, item) => sum + item.weight, 0);\n  const half = totalWeight / 2;\n\n  let acc = 0;\n  for (const item of sorted) {\n    acc += item.weight;\n    if (acc >= half) {\n      return item.value;\n    }\n  }\n\n  return sorted[sorted.length - 1].value;\n}\n\nfunction getFrameCenterX(\n  detections: FaceDetection[],\n  videoWidth: number,\n): number {\n  const videoCenterX = videoWidth / 2;\n\n  if (detections.length === 0) {\n    return videoCenterX;\n  }\n\n  const sorted = [...detections].sort((a, b) => b.area - a.area);\n\n  const top1 = sorted[0];\n  const top2 = sorted[1];\n\n  // Se só temos uma cara forte → focamos nessa pessoa\n  if (!top2 || top2.area < top1.area * 0.55) {\n    return top1.centerX;\n  }\n\n  const distance = Math.abs(top2.centerX - top1.centerX);\n\n  // Se as duas caras estão bem separadas → two-shot, centro no meio\n  if (distance > videoWidth * 0.25) {\n    return (top1.centerX + top2.centerX) / 2;\n  }\n\n  // Caso intermédio → focamos na maior\n  return top1.centerX;\n}\n\ninterface CenterKeyframe {\n  time: number; // segundos\n  centerX: number;\n}\n\nfunction buildCenterTimeline(\n  resolution: { width: number; height: number },\n  frames: FrameDetections[],\n  startSeconds: number,\n  durationSeconds: number,\n): CenterKeyframe[] {\n  const { width: videoWidth } = resolution;\n  const videoCenterX = videoWidth / 2;\n\n  if (frames.length === 0) {\n    return [\n      {\n        time: startSeconds,\n        centerX: videoCenterX,\n      },\n    ];\n  }\n\n  const raw: CenterKeyframe[] = frames.map((f) => ({\n    time: f.time,\n    centerX: getFrameCenterX(f.detections, videoWidth),\n  }));\n\n  // Pequeno smoothing temporal (janela 3 frames) para evitar saltos bruscos\n  const smoothed: CenterKeyframe[] = [];\n\n  for (let i = 0; i < raw.length; i++) {\n    let sum = 0;\n    let weight = 0;\n\n    for (let j = i - 1; j <= i + 1; j++) {\n      if (j < 0 || j >= raw.length) continue;\n      const w = j === i ? 2 : 1; // frame atual pesa mais\n      sum += raw[j].centerX * w;\n      weight += w;\n    }\n\n    smoothed.push({\n      time: raw[i].time,\n      centerX: sum / weight,\n    });\n  }\n\n  return smoothed;\n}\n\nfunction calculateCrop(\n  resolution: { width: number; height: number },\n  detections: FaceDetection[],\n): CropInfo {\n  const { width: videoWidth, height: videoHeight } = resolution;\n\n  // ---- CONFIGURAÇÃO DE FRAMING (fácil de afinar) ----\n  const STRONG_MIN_AREA_RATIO = 0.008; // 0.8% da área total do vídeo\n  const STRONG_MIN_SCORE = 0.25; // score mínimo para \"strong detections\"\n  const TWO_SPEAKERS_MIN_SIDE_RATIO = 0.18; // min. 18% do peso em cada lado p/ considerar 2 oradores\n  const VIDEO_CENTER_BLEND = 0.1; // 10% de mistura com o centro global do vídeo\n  const MAX_SHIFT_FROM_CENTER_RATIO = 0.22; // não deixar o crop afastar-se mais que ~22% da largura\n  // ---------------------------------------------------\n\n  // 9:16 máximo possível dentro da imagem original\n  const maxCropHeight = videoHeight;\n  let maxCropWidth = Math.round((maxCropHeight * 9) / 16);\n\n  // Se por algum motivo o vídeo for \"estreito\", ajustamos a largura\n  if (maxCropWidth > videoWidth) {\n    maxCropWidth = videoWidth;\n  }\n\n  // Se não há deteções, usa crop centrado padrão\n  if (detections.length === 0) {\n    const centerCropX = Math.max(\n      0,\n      Math.floor((videoWidth - maxCropWidth) / 2),\n    );\n    return {\n      cropX: centerCropX,\n      cropY: 0,\n      cropWidth: maxCropWidth,\n      cropHeight: maxCropHeight,\n      videoWidth,\n      videoHeight,\n    };\n  }\n\n  // 1) Filtrar detecções mais fortes (dar prioridade a planos médios/close-ups)\n  const frameArea = videoWidth * videoHeight;\n  const strongDetections = detections.filter((d) => {\n    return (\n      d.score >= STRONG_MIN_SCORE && d.area >= STRONG_MIN_AREA_RATIO * frameArea\n    );\n  });\n\n  const usedDetections =\n    strongDetections.length >= 8 ? strongDetections : detections;\n\n  // 2) Bounding box global do grupo de caras\n  let facesLeft = Infinity;\n  let facesRight = -Infinity;\n  let facesTop = Infinity;\n  let facesBottom = -Infinity;\n\n  const centerXValues: WeightedValue[] = [];\n  const centerYValues: WeightedValue[] = [];\n\n  for (const d of usedDetections) {\n    const right = d.x + d.width;\n    const bottom = d.y + d.height;\n\n    if (d.x < facesLeft) facesLeft = d.x;\n    if (right > facesRight) facesRight = right;\n    if (d.y < facesTop) facesTop = d.y;\n    if (bottom > facesBottom) facesBottom = bottom;\n\n    const w = Math.max(d.area, 1);\n    centerXValues.push({ value: d.centerX, weight: w });\n    centerYValues.push({ value: d.centerY, weight: w });\n  }\n\n  const facesWidth = facesRight - facesLeft;\n  const facesHeight = facesBottom - facesTop;\n\n  // 3) Detectar se temos 2 oradores bem definidos (esquerda/direita)\n  const totalWeight = centerXValues.reduce((sum, v) => sum + v.weight, 0);\n  const groupCenterX = (facesLeft + facesRight) / 2;\n  const medianCenterX = weightedMedian(centerXValues);\n\n  const leftCluster = centerXValues.filter((v) => v.value <= groupCenterX);\n  const rightCluster = centerXValues.filter((v) => v.value > groupCenterX);\n\n  const leftWeight = leftCluster.reduce((sum, v) => sum + v.weight, 0);\n  const rightWeight = rightCluster.reduce((sum, v) => sum + v.weight, 0);\n\n  const hasTwoSides =\n    totalWeight > 0 &&\n    leftWeight / totalWeight >= TWO_SPEAKERS_MIN_SIDE_RATIO &&\n    rightWeight / totalWeight >= TWO_SPEAKERS_MIN_SIDE_RATIO;\n\n  let centerFromFaces = medianCenterX;\n  let anchorsCenterX = groupCenterX;\n\n  if (hasTwoSides && leftCluster.length > 0 && rightCluster.length > 0) {\n    const leftMedian = weightedMedian(leftCluster);\n    const rightMedian = weightedMedian(rightCluster);\n    anchorsCenterX = (leftMedian + rightMedian) / 2;\n    // Centro vindo das caras (entre os dois oradores, mas puxado para onde há mais frames)\n    centerFromFaces = anchorsCenterX * 0.8 + medianCenterX * 0.2;\n  } else {\n    // Um só orador predominante / setup assimétrico\n    centerFromFaces = groupCenterX * 0.6 + medianCenterX * 0.4;\n  }\n\n  // 4) Misturar um pouco com o centro global do vídeo e limitar o desvio máximo\n  const videoCenterX = videoWidth / 2;\n  let centerX =\n    centerFromFaces * (1 - VIDEO_CENTER_BLEND) +\n    videoCenterX * VIDEO_CENTER_BLEND;\n\n  const maxShift = videoWidth * MAX_SHIFT_FROM_CENTER_RATIO;\n  const minCenter = videoCenterX - maxShift;\n  const maxCenter = videoCenterX + maxShift;\n\n  if (centerX < minCenter) centerX = minCenter;\n  if (centerX > maxCenter) centerX = maxCenter;\n\n  // Usamos sempre a altura total -> MENOS zoom possível\n  const cropHeight = maxCropHeight;\n  const cropWidth = maxCropWidth;\n\n  // Mantemos Y = 0 para não cortar cabeças\n  let cropX = Math.round(centerX - cropWidth / 2);\n  let cropY = 0;\n\n  // Clamps finais\n  if (cropX < 0) cropX = 0;\n  if (cropX + cropWidth > videoWidth) cropX = videoWidth - cropWidth;\n  if (cropY < 0) cropY = 0;\n  if (cropY + cropHeight > videoHeight) cropY = videoHeight - cropHeight;\n\n  console.log(\"[framing]\", {\n    totalDetections: detections.length,\n    usedDetections: usedDetections.length,\n    facesLeft,\n    facesRight,\n    facesTop,\n    facesBottom,\n    facesWidth,\n    facesHeight,\n    totalWeight,\n    leftWeight,\n    rightWeight,\n    hasTwoSides,\n    groupCenterX,\n    medianCenterX,\n    anchorsCenterX,\n    centerFromFaces,\n    videoCenterX,\n    centerX,\n    cropX,\n    cropY,\n    cropWidth,\n    cropHeight,\n    videoWidth,\n    videoHeight,\n  });\n\n  return {\n    cropX,\n    cropY,\n    cropWidth,\n    cropHeight,\n    videoWidth,\n    videoHeight,\n  };\n}\n","size_bytes":15835},"attached_assets/jobs_1764019232734.ts":{"content":"import type { JobState, JobStatus, ProcessMode, CropInfo, Job, InsertJob } from \"@shared/schema\";\nimport { jobs } from \"@shared/schema\";\nimport { db } from \"../db\";\nimport { eq, and, lt } from \"drizzle-orm\";\nimport { randomUUID } from \"crypto\";\nimport { promises as fs } from \"fs\";\nimport path from \"path\";\n\nexport class JobManager {\n  private readonly TMP_DIR = path.join(process.cwd(), \"tmp\");\n  private readonly FRAMES_DIR = path.join(process.cwd(), \"tmp/frames\");\n\n  async initialize() {\n    await fs.mkdir(this.TMP_DIR, { recursive: true });\n    await fs.mkdir(this.FRAMES_DIR, { recursive: true });\n  }\n\n  private dbJobToState(job: Job): JobState {\n    return {\n      id: job.id,\n      status: job.status,\n      progress: job.progress,\n      url: job.url,\n      startSeconds: job.startSeconds,\n      mode: job.mode,\n      errorMessage: job.errorMessage || undefined,\n      downloadPath: job.downloadPath || undefined,\n      debugImagePath: job.debugImagePath || undefined,\n      framesAnalyzed: job.framesAnalyzed || undefined,\n      crop: job.crop || undefined,\n      createdAt: job.createdAt.getTime(),\n    };\n  }\n\n  async createJob(url: string, startSeconds: number, mode: ProcessMode): Promise<JobState> {\n    const id = randomUUID();\n    const [job] = await db\n      .insert(jobs)\n      .values({\n        id,\n        url,\n        startSeconds,\n        mode,\n        status: \"pending\",\n        progress: 0,\n      })\n      .returning();\n    return this.dbJobToState(job);\n  }\n\n  async getJob(id: string): Promise<JobState | undefined> {\n    const [job] = await db.select().from(jobs).where(eq(jobs.id, id));\n    return job ? this.dbJobToState(job) : undefined;\n  }\n\n  async updateJob(id: string, updates: Partial<InsertJob>): Promise<void> {\n    await db\n      .update(jobs)\n      .set({ ...updates, updatedAt: new Date() })\n      .where(eq(jobs.id, id));\n  }\n\n  async updateProgress(id: string, progress: number): Promise<void> {\n    await this.updateJob(id, { progress: Math.floor(Math.min(100, Math.max(0, progress))) });\n  }\n\n  async setStatus(id: string, status: JobStatus, errorMessage?: string): Promise<void> {\n    await this.updateJob(id, { status, errorMessage });\n  }\n\n  async setCrop(id: string, crop: CropInfo): Promise<void> {\n    await this.updateJob(id, { crop });\n  }\n\n  async setFramesAnalyzed(id: string, count: number): Promise<void> {\n    await this.updateJob(id, { framesAnalyzed: count });\n  }\n\n  async setDownloadPath(id: string, downloadPath: string): Promise<void> {\n    await this.updateJob(id, { downloadPath });\n  }\n\n  async setDebugImagePath(id: string, debugImagePath: string): Promise<void> {\n    await this.updateJob(id, { debugImagePath });\n  }\n\n  async isJobCancelled(id: string): Promise<boolean> {\n    const job = await this.getJob(id);\n    return job?.status === \"cancelled\";\n  }\n\n  async cleanupJob(id: string): Promise<void> {\n    try {\n      const inputPath = path.join(this.TMP_DIR, `input-${id}.mp4`);\n      const outputPath = path.join(this.TMP_DIR, `output-${id}.mp4`);\n      const debugPath = path.join(this.TMP_DIR, `debug-${id}.png`);\n\n      const removeIfExists = async (filePath: string) => {\n        try {\n          await fs.unlink(filePath);\n        } catch (err: any) {\n          if (err.code !== \"ENOENT\") {\n            console.error(`Error removing ${filePath}:`, err);\n          }\n        }\n      };\n\n      await removeIfExists(inputPath);\n      await removeIfExists(outputPath);\n      await removeIfExists(debugPath);\n\n      const files = await fs.readdir(this.FRAMES_DIR);\n      const jobFrames = files.filter(f => f.startsWith(`${id}-frame-`));\n      await Promise.all(\n        jobFrames.map(f => removeIfExists(path.join(this.FRAMES_DIR, f)))\n      );\n    } catch (err) {\n      console.error(`Error cleaning up job ${id}:`, err);\n    }\n  }\n\n  async cleanupOldJobs(maxAgeMs: number = 30 * 60 * 1000): Promise<void> {\n    const cutoffDate = new Date(Date.now() - maxAgeMs);\n    const oldJobs = await db\n      .select()\n      .from(jobs)\n      .where(\n        and(\n          lt(jobs.createdAt, cutoffDate),\n          eq(jobs.status, \"done\" as JobStatus)\n        )\n      );\n\n    for (const job of oldJobs) {\n      await this.cleanupJob(job.id);\n      await db.delete(jobs).where(eq(jobs.id, job.id));\n    }\n  }\n\n  getInputPath(jobId: string): string {\n    return path.join(this.TMP_DIR, `input-${jobId}.mp4`);\n  }\n\n  getOutputPath(jobId: string): string {\n    return path.join(this.TMP_DIR, `output-${jobId}.mp4`);\n  }\n\n  getDebugPath(jobId: string): string {\n    return path.join(this.TMP_DIR, `debug-${jobId}.png`);\n  }\n\n  getFramesDir(): string {\n    return this.FRAMES_DIR;\n  }\n\n  getFramePath(jobId: string, frameNumber: number): string {\n    return path.join(this.FRAMES_DIR, `${jobId}-frame-${frameNumber.toString().padStart(4, '0')}.png`);\n  }\n}\n\nexport const jobManager = new JobManager();\n","size_bytes":4864},"attached_assets/modelDownloader_1764019232735.ts":{"content":"import { promises as fs } from \"fs\";\nimport path from \"path\";\nimport https from \"https\";\n\nconst MODEL_FILES = [\n  \"tiny_face_detector_model-weights_manifest.json\",\n  \"tiny_face_detector_model.bin\",\n];\n\nconst BASE_URL = \"https://vladmandic.github.io/face-api/model/\";\n\nasync function downloadFile(url: string, dest: string): Promise<void> {\n  return new Promise((resolve, reject) => {\n    const file = fs.open(dest, 'w');\n    \n    https.get(url, (response) => {\n      if (response.statusCode !== 200) {\n        reject(new Error(`Failed to download ${url}: ${response.statusCode}`));\n        return;\n      }\n\n      const chunks: Buffer[] = [];\n      response.on('data', (chunk) => chunks.push(chunk));\n      response.on('end', async () => {\n        try {\n          await fs.writeFile(dest, Buffer.concat(chunks));\n          resolve();\n        } catch (err) {\n          reject(err);\n        }\n      });\n    }).on('error', reject);\n  });\n}\n\nexport async function ensureModelsDownloaded(): Promise<void> {\n  const modelsPath = path.join(process.cwd(), \"models\");\n\n  await fs.mkdir(modelsPath, { recursive: true });\n\n  const modelFiles = await fs.readdir(modelsPath).catch(() => []);\n  const allExist = MODEL_FILES.every(file => modelFiles.includes(file));\n\n  if (allExist) {\n    console.log(\"Face detection models already exist\");\n    return;\n  }\n\n  console.log(\"Downloading face detection models...\");\n\n  for (const file of MODEL_FILES) {\n    const url = BASE_URL + file;\n    const dest = path.join(modelsPath, file);\n\n    try {\n      console.log(`Downloading ${file}...`);\n      await downloadFile(url, dest);\n      console.log(`Downloaded ${file}`);\n    } catch (error) {\n      console.error(`Error downloading ${file}:`, error);\n      throw error;\n    }\n  }\n\n  console.log(\"All models downloaded successfully\");\n}\n","size_bytes":1813},"attached_assets/ffmpeg_1764019232735.ts":{"content":"import ffmpeg from \"fluent-ffmpeg\";\nimport ffmpegStatic from \"ffmpeg-static\";\nimport ffprobeInstaller from \"@ffprobe-installer/ffprobe\";\nimport { promises as fs } from \"fs\";\nimport path from \"path\";\n\nif (ffmpegStatic) {\n  ffmpeg.setFfmpegPath(ffmpegStatic);\n}\n\nif (ffprobeInstaller.path) {\n  ffmpeg.setFfprobePath(ffprobeInstaller.path);\n}\n\nexport interface VideoResolution {\n  width: number;\n  height: number;\n}\n\nexport async function getVideoResolution(videoPath: string): Promise<VideoResolution> {\n  return new Promise((resolve, reject) => {\n    ffmpeg.ffprobe(videoPath, (err, metadata) => {\n      if (err) {\n        reject(err);\n        return;\n      }\n\n      const videoStream = metadata.streams.find(s => s.codec_type === \"video\");\n      if (!videoStream || !videoStream.width || !videoStream.height) {\n        reject(new Error(\"Could not find video stream dimensions\"));\n        return;\n      }\n\n      resolve({\n        width: videoStream.width,\n        height: videoStream.height,\n      });\n    });\n  });\n}\n\nexport async function extractFrames(\n  videoPath: string,\n  outputDir: string,\n  startSeconds: number,\n  durationSeconds: number,\n  fps: number,\n  jobId: string,\n  onProgress?: (percent: number) => void\n): Promise<string[]> {\n  await fs.mkdir(outputDir, { recursive: true });\n\n  const pattern = path.join(outputDir, `${jobId}-frame-%04d.png`);\n\n  return new Promise((resolve, reject) => {\n    let extractedFrames: string[] = [];\n\n    ffmpeg(videoPath)\n      .setStartTime(startSeconds)\n      .duration(durationSeconds)\n      .outputOptions([\n        `-vf fps=1/${fps},scale=640:-1`,\n      ])\n      .output(pattern)\n      .on(\"end\", async () => {\n        try {\n          const files = await fs.readdir(outputDir);\n          extractedFrames = files\n            .filter(f => f.startsWith(`${jobId}-frame-`) && f.endsWith(\".png\"))\n            .map(f => path.join(outputDir, f))\n            .sort();\n          resolve(extractedFrames);\n        } catch (err) {\n          reject(err);\n        }\n      })\n      .on(\"error\", (err) => {\n        reject(err);\n      })\n      .on(\"progress\", (progress) => {\n        if (onProgress && progress.percent) {\n          onProgress(progress.percent);\n        }\n      })\n      .run();\n  });\n}\n\nexport async function renderVerticalVideo(\n  inputPath: string,\n  outputPath: string,\n  startSeconds: number,\n  durationSeconds: number,\n  cropX: number,\n  cropY: number,\n  cropWidth: number,\n  cropHeight: number,\n  onProgress?: (percent: number) => void\n): Promise<void> {\n  return new Promise((resolve, reject) => {\n    ffmpeg(inputPath)\n      .setStartTime(startSeconds)\n      .duration(durationSeconds)\n      .outputOptions([\n        `-vf crop=${cropWidth}:${cropHeight}:${cropX}:${cropY},scale=1080:1920`,\n        \"-c:v libx264\",\n        \"-preset veryfast\",\n        \"-movflags +faststart\",\n        \"-c:a copy\",\n      ])\n      .output(outputPath)\n      .on(\"end\", () => resolve())\n      .on(\"error\", (err) => reject(err))\n      .on(\"progress\", (progress) => {\n        if (onProgress && progress.percent) {\n          onProgress(progress.percent);\n        }\n      })\n      .run();\n  });\n}\n","size_bytes":3132},"src/services/modelDownloader.ts":{"content":"import { promises as fs } from \"fs\";\nimport path from \"path\";\nimport https from \"https\";\n\nconst MODEL_FILES = [\n  \"tiny_face_detector_model-weights_manifest.json\",\n  \"tiny_face_detector_model.bin\",\n];\n\nconst BASE_URL = \"https://vladmandic.github.io/face-api/model/\";\n\nasync function downloadFile(url: string, dest: string): Promise<void> {\n  return new Promise((resolve, reject) => {\n    https.get(url, (response) => {\n      if (response.statusCode !== 200) {\n        reject(new Error(`Failed to download ${url}: ${response.statusCode}`));\n        return;\n      }\n\n      const chunks: Buffer[] = [];\n      response.on('data', (chunk) => chunks.push(chunk));\n      response.on('end', async () => {\n        try {\n          await fs.writeFile(dest, Buffer.concat(chunks));\n          resolve();\n        }\n        catch (err) {\n          reject(err);\n        }\n      });\n    }).on('error', reject);\n  });\n}\n\nexport async function ensureModelsDownloaded(): Promise<void> {\n  const modelsPath = path.join(process.cwd(), \"models\");\n\n  await fs.mkdir(modelsPath, { recursive: true });\n\n  const modelFiles: string[] = await fs.readdir(modelsPath).catch(() => []);\n  const allExist = MODEL_FILES.every(file => modelFiles.includes(file));\n\n  if (allExist) {\n    console.log(\"Face detection models already exist\");\n    return;\n  }\n\n  console.log(\"Downloading face detection models...\");\n\n  for (const file of MODEL_FILES) {\n    const url = BASE_URL + file;\n    const dest = path.join(modelsPath, file);\n\n    try {\n      console.log(`Downloading ${file}...`);\n      await downloadFile(url, dest);\n      console.log(`Downloaded ${file}`);\n    }\n    catch (error) {\n      console.error(`Error downloading ${file}:`, error);\n      throw error;\n    }\n  }\n\n  console.log(\"All models downloaded successfully\");\n}\n","size_bytes":1793},"attached_assets/faceDetector_1764019232736.ts":{"content":"import * as faceapi from \"@vladmandic/face-api\";\nimport * as tf from \"@tensorflow/tfjs-node\";\nimport { Canvas, Image, ImageData, loadImage } from \"canvas\";\nimport { promises as fs } from \"fs\";\nimport path from \"path\";\n\n// @ts-ignore\nfaceapi.env.monkeyPatch({ Canvas, Image, ImageData });\n\nlet modelsLoaded = false;\n\nexport async function initializeFaceDetection(): Promise<void> {\n  if (modelsLoaded) {\n    return;\n  }\n\n  try {\n    const modelsPath = path.join(process.cwd(), \"models\");\n\n    await fs.mkdir(modelsPath, { recursive: true });\n\n    await faceapi.nets.tinyFaceDetector.loadFromDisk(modelsPath);\n\n    modelsLoaded = true;\n    console.log(\"Face detection models loaded successfully\");\n  } catch (error) {\n    console.error(\"Error loading face detection models:\", error);\n    throw new Error(\"Failed to initialize face detection models\");\n  }\n}\n\nexport interface FaceDetection {\n  x: number;\n  y: number;\n  width: number;\n  height: number;\n  centerX: number;\n  centerY: number;\n  score: number;\n  area: number;\n}\n\nexport async function detectFacesInImage(\n  imagePath: string,\n  videoWidth?: number,\n  videoHeight?: number,\n): Promise<FaceDetection[]> {\n  if (!modelsLoaded) {\n    throw new Error(\"Face detection models not loaded\");\n  }\n\n  try {\n    const img = await loadImage(imagePath);\n\n    const frameWidth = img.width;\n    const frameHeight = img.height;\n\n    // Se nos derem a resolução do vídeo, usamos para escalar as boxes.\n    const targetWidth = videoWidth ?? frameWidth;\n    const targetHeight = videoHeight ?? frameHeight;\n\n    const scaleX = targetWidth / frameWidth;\n    const scaleY = targetHeight / frameHeight;\n\n    const detections = await faceapi.detectAllFaces(\n      img as any,\n      new faceapi.TinyFaceDetectorOptions({\n        inputSize: 512,\n        scoreThreshold: 0.2,\n      }),\n    );\n\n    const minWidth = targetWidth * 0.035; // 3.5% em vez de 4%\n    const minHeight = targetHeight * 0.035;\n\n    const result: FaceDetection[] = [];\n\n    for (const detection of detections) {\n      const box = detection.box;\n\n      // Coordenadas na resolução ORIGINAL do frame (640 etc.)\n      const rawX = box.x;\n      const rawY = box.y;\n      const rawW = box.width;\n      const rawH = box.height;\n\n      // Escalar para a resolução DO VÍDEO (1920x1080)\n      const x = rawX * scaleX;\n      const y = rawY * scaleY;\n      const width = rawW * scaleX;\n      const height = rawH * scaleY;\n\n      const centerX = x + width / 2;\n      const centerY = y + height / 2;\n      const area = width * height;\n\n      // Filtros em coordenadas do VÍDEO\n      if (width < minWidth || height < minHeight) {\n        continue;\n      }\n\n      if (centerY < targetHeight * 0.15 || centerY > targetHeight * 0.95) {\n        continue;\n      }\n\n      result.push({\n        x,\n        y,\n        width,\n        height,\n        centerX,\n        centerY,\n        score: detection.score,\n        area,\n      });\n    }\n\n    return result;\n  } catch (error: any) {\n    const fileName = path.basename(imagePath);\n    console.warn(\n      `Failed to load or detect faces in frame ${fileName}: ${error.message}`,\n    );\n    return [];\n  }\n}\n\nexport async function drawDebugFrame(\n  imagePath: string,\n  outputPath: string,\n  faces: FaceDetection[],\n  cropX: number,\n  cropY: number,\n  cropWidth: number,\n  cropHeight: number,\n): Promise<boolean> {\n  try {\n    const img = await loadImage(imagePath);\n\n    const canvas = faceapi.createCanvas(img.width, img.height);\n    const ctx = canvas.getContext(\"2d\");\n\n    ctx.drawImage(img as any, 0, 0);\n\n    faces.forEach((face) => {\n      ctx.strokeStyle = \"#00ff00\";\n      ctx.lineWidth = 3;\n      ctx.strokeRect(face.x, face.y, face.width, face.height);\n    });\n\n    ctx.strokeStyle = \"#ff0000\";\n    ctx.lineWidth = 4;\n    ctx.strokeRect(cropX, cropY, cropWidth, cropHeight);\n\n    ctx.fillStyle = \"#ff0000\";\n    ctx.font = \"24px Arial\";\n    ctx.fillText(\"Crop 9:16\", cropX + 10, cropY + 30);\n\n    const outBuffer = canvas.toBuffer(\"image/png\");\n    await fs.writeFile(outputPath, outBuffer);\n    return true;\n  } catch (error: any) {\n    const fileName = path.basename(imagePath);\n    console.warn(\n      `Failed to generate debug frame from ${fileName}: ${error.message} (non-critical, continuing...)`,\n    );\n    return false;\n  }\n}\n","size_bytes":4286}},"version":2}